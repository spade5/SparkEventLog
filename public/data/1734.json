{
    "Event": "SparkListenerLogStart",
    "Spark Version": "3.3.2"
}
{
    "Event": "SparkListenerResourceProfileAdded",
    "Resource Profile Id": 0,
    "Executor Resource Requests": {
        "cores": {
            "Resource Name": "cores",
            "Amount": 1,
            "Discovery Script": "",
            "Vendor": ""
        },
        "memory": {
            "Resource Name": "memory",
            "Amount": 1024,
            "Discovery Script": "",
            "Vendor": ""
        },
        "offHeap": {
            "Resource Name": "offHeap",
            "Amount": 0,
            "Discovery Script": "",
            "Vendor": ""
        }
    },
    "Task Resource Requests": {
        "cpus": {
            "Resource Name": "cpus",
            "Amount": 1.0
        }
    }
}
{
    "Event": "SparkListenerExecutorAdded",
    "Timestamp": 1680515064693,
    "Executor ID": "driver",
    "Executor Info": {
        "Host": "node86",
        "Total Cores": 2,
        "Log Urls": {},
        "Attributes": {},
        "Resources": {},
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerBlockManagerAdded",
    "Block Manager ID": {
        "Executor ID": "driver",
        "Host": "node86",
        "Port": 43483
    },
    "Maximum Memory": 455501414,
    "Timestamp": 1680515064724,
    "Maximum Onheap Memory": 455501414,
    "Maximum Offheap Memory": 0
}
{
    "Event": "SparkListenerEnvironmentUpdate",
    "JVM Information": {
        "Java Home": "/home/chenhao/libs/jdk11",
        "Java Version": "11.0.16.1 (Oracle Corporation)",
        "Scala Version": "version 2.13.8"
    },
    "Spark Properties": {
        "spark.eventLog.enabled": "true",
        "spark.driver.port": "50612",
        "spark.jars": "file:/home/chenhao/workspace/SparkTemplate-1.3-jar-with-dependencies.jar",
        "spark.app.name": "WordCount",
        "spark.submit.pyFiles": "",
        "spark.app.submitTime": "1680515061595",
        "spark.submit.deployMode": "client",
        "spark.master": "local[2]",
        "spark.eventLog.dir": "/home/chenhao/logs/events",
        "spark.app.id": "local-1680515062694",
        "spark.executor.extraJavaOptions": "-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED",
        "spark.driver.host": "node86",
        "spark.scheduler.mode": "FIFO",
        "spark.streaming.kafka.maxRatePerPartition": "200000",
        "spark.app.startTime": "1680515061685",
        "spark.executor.id": "driver",
        "spark.driver.extraJavaOptions": "-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED",
        "spark.app.initial.jar.urls": "spark://node86:50612/jars/SparkTemplate-1.3-jar-with-dependencies.jar"
    },
    "Hadoop Properties": {
        "hadoop.service.shutdown.timeout": "30s",
        "yarn.resourcemanager.amlauncher.thread-count": "50",
        "yarn.nodemanager.numa-awareness.numactl.cmd": "/usr/bin/numactl",
        "fs.viewfs.overload.scheme.target.o3fs.impl": "org.apache.hadoop.fs.ozone.OzoneFileSystem",
        "yarn.timeline-service.timeline-client.number-of-async-entities-to-merge": "10",
        "hadoop.security.kms.client.timeout": "60",
        "yarn.resourcemanager.application-tag-based-placement.enable": "false",
        "yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds.min": "3600",
        "yarn.app.mapreduce.am.job.task.listener.thread-count": "30",
        "yarn.nodemanager.node-attributes.resync-interval-ms": "120000",
        "yarn.nodemanager.container-log-monitor.interval-ms": "60000",
        "fs.viewfs.overload.scheme.target.gs.impl": "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS",
        "fs.s3a.retry.limit": "7",
        "mapreduce.jobhistory.loadedjobs.cache.size": "5",
        "yarn.sharedcache.enabled": "false",
        "fs.s3a.connection.maximum": "96",
        "fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem",
        "yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms": "1000",
        "hadoop.http.authentication.kerberos.principal": "HTTP/_HOST@LOCALHOST",
        "mapreduce.jobhistory.loadedjob.tasks.max": "-1",
        "mapreduce.framework.name": "local",
        "yarn.sharedcache.uploader.server.thread-count": "50",
        "yarn.nodemanager.linux-container-executor.nonsecure-mode.user-pattern": "^[_.A-Za-z0-9][-@_.A-Za-z0-9]{0,255}?[$]?$",
        "tfile.fs.output.buffer.size": "262144",
        "hadoop.security.groups.cache.background.reload.threads": "3",
        "yarn.resourcemanager.webapp.cross-origin.enabled": "false",
        "fs.AbstractFileSystem.ftp.impl": "org.apache.hadoop.fs.ftp.FtpFs",
        "hadoop.registry.secure": "false",
        "hadoop.shell.safely.delete.limit.num.files": "100",
        "mapreduce.job.acl-view-job": " ",
        "fs.s3a.s3guard.ddb.background.sleep": "25ms",
        "fs.s3a.s3guard.ddb.table.create": "false",
        "fs.viewfs.overload.scheme.target.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem",
        "mapreduce.shuffle.pathcache.expire-after-access-minutes": "5",
        "mapreduce.input.fileinputformat.split.minsize": "0",
        "yarn.resourcemanager.container.liveness-monitor.interval-ms": "600000",
        "yarn.resourcemanager.client.thread-count": "50",
        "fs.viewfs.overload.scheme.target.http.impl": "org.apache.hadoop.fs.http.HttpFileSystem",
        "yarn.nodemanager.amrmproxy.interceptor-class.pipeline": "org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor",
        "yarn.timeline-service.entity-group-fs-store.leveldb-cache-read-cache-size": "10485760",
        "yarn.nodemanager.admin-env": "MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX",
        "yarn.resourcemanager.node-removal-untracked.timeout-ms": "60000",
        "mapreduce.am.max-attempts": "2",
        "hadoop.security.kms.client.failover.sleep.base.millis": "100",
        "yarn.nodemanager.amrmproxy.enabled": "false",
        "yarn.timeline-service.entity-group-fs-store.with-user-dir": "false",
        "io.seqfile.compress.blocksize": "1000000",
        "yarn.nodemanager.runtime.linux.docker.allowed-container-runtimes": "runc",
        "yarn.resourcemanager.nodemanagers.heartbeat-interval-slowdown-factor": "1.0",
        "yarn.sharedcache.checksum.algo.impl": "org.apache.hadoop.yarn.sharedcache.ChecksumSHA256Impl",
        "mapreduce.reduce.shuffle.fetch.retry.interval-ms": "1000",
        "mapreduce.task.profile.maps": "0-2",
        "yarn.scheduler.include-port-in-node-name": "false",
        "mapreduce.jobhistory.webapp.https.address": "0.0.0.0:19890",
        "yarn.node-labels.fs-store.impl.class": "org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore",
        "hadoop.http.authentication.signature.secret.file": "*********(redacted)",
        "hadoop.jetty.logs.serve.aliases": "true",
        "yarn.sharedcache.webapp.address": "0.0.0.0:8788",
        "fs.s3a.select.input.csv.quote.escape.character": "\\\\",
        "fs.viewfs.overload.scheme.target.swift.impl": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem",
        "hadoop.security.group.mapping.ldap.posix.attr.gid.name": "gidNumber",
        "ipc.client.fallback-to-simple-auth-allowed": "false",
        "yarn.nodemanager.resource.memory.enforced": "true",
        "yarn.resourcemanager.system-metrics-publisher.timeline-server-v1.enable-batch": "false",
        "yarn.client.failover-proxy-provider": "org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider",
        "yarn.nodemanager.collector-service.address": "${yarn.nodemanager.hostname}:8048",
        "fs.trash.checkpoint.interval": "0",
        "mapreduce.job.map.output.collector.class": "org.apache.hadoop.mapred.MapTask$MapOutputBuffer",
        "yarn.resourcemanager.node-ip-cache.expiry-interval-secs": "-1",
        "yarn.resourcemanager.placement-constraints.handler": "disabled",
        "yarn.timeline-service.handler-thread-count": "10",
        "yarn.resourcemanager.max-completed-applications": "1000",
        "yarn.nodemanager.aux-services.manifest.enabled": "false",
        "yarn.resourcemanager.system-metrics-publisher.enabled": "false",
        "yarn.resourcemanager.placement-constraints.algorithm.class": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.algorithm.DefaultPlacementAlgorithm",
        "yarn.resourcemanager.delegation.token.renew-interval": "*********(redacted)",
        "yarn.sharedcache.nm.uploader.replication.factor": "10",
        "hadoop.security.groups.negative-cache.secs": "30",
        "yarn.app.mapreduce.task.container.log.backups": "0",
        "mapreduce.reduce.skip.proc-count.auto-incr": "true",
        "yarn.timeline-service.http-authentication.simple.anonymous.allowed": "true",
        "ha.health-monitor.check-interval.ms": "1000",
        "yarn.nodemanager.runtime.linux.runc.host-pid-namespace.allowed": "false",
        "hadoop.metrics.jvm.use-thread-mxbean": "false",
        "ipc.[port_number].faircallqueue.multiplexer.weights": "8,4,2,1",
        "yarn.acl.reservation-enable": "false",
        "yarn.resourcemanager.store.class": "org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore",
        "yarn.app.mapreduce.am.hard-kill-timeout-ms": "10000",
        "yarn.resourcemanager.nodemanagers.heartbeat-interval-scaling-enable": "false",
        "yarn.resourcemanager.nodemanagers.heartbeat-interval-ms": "1000",
        "yarn.nodemanager.windows-container.cpu-limit.enabled": "false",
        "yarn.scheduler.configuration.leveldb-store.path": "${hadoop.tmp.dir}/yarn/system/confstore",
        "mapreduce.map.skip.proc-count.auto-incr": "true",
        "fs.s3a.committer.name": "file",
        "yarn.webapp.xfs-filter.enabled": "true",
        "yarn.resourcemanager.scheduler.address": "${yarn.resourcemanager.hostname}:8030",
        "yarn.node-labels.enabled": "false",
        "yarn.resourcemanager.webapp.ui-actions.enabled": "true",
        "fs.s3a.etag.checksum.enabled": "false",
        "yarn.nodemanager.container-metrics.enable": "true",
        "ha.health-monitor.rpc.connect.max.retries": "1",
        "yarn.timeline-service.client.fd-clean-interval-secs": "60",
        "hadoop.common.configuration.version": "3.0.0",
        "fs.s3a.s3guard.ddb.table.capacity.read": "0",
        "yarn.nodemanager.remote-app-log-dir-suffix": "logs",
        "yarn.nodemanager.container-log-monitor.dir-size-limit-bytes": "1000000000",
        "yarn.nodemanager.runtime.linux.docker.privileged-containers.allowed": "false",
        "file.blocksize": "67108864",
        "hadoop.http.idle_timeout.ms": "60000",
        "hadoop.registry.zk.retry.ceiling.ms": "60000",
        "yarn.sharedcache.store.in-memory.initial-delay-mins": "10",
        "mapreduce.jobhistory.principal": "jhs/_HOST@REALM.TLD",
        "mapreduce.task.profile.reduces": "0-2",
        "hadoop.zk.num-retries": "1000",
        "fs.viewfs.overload.scheme.target.hdfs.impl": "org.apache.hadoop.hdfs.DistributedFileSystem",
        "seq.io.sort.mb": "100",
        "yarn.scheduler.configuration.max.version": "100",
        "yarn.timeline-service.webapp.https.address": "${yarn.timeline-service.hostname}:8190",
        "mapreduce.task.timeout": "600000",
        "yarn.sharedcache.client-server.thread-count": "50",
        "hadoop.security.groups.shell.command.timeout": "0s",
        "hadoop.security.crypto.cipher.suite": "AES/CTR/NoPadding",
        "yarn.nodemanager.elastic-memory-control.oom-handler": "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DefaultOOMHandler",
        "yarn.resourcemanager.connect.max-wait.ms": "900000",
        "fs.defaultFS": "file:///",
        "yarn.minicluster.use-rpc": "false",
        "io.compression.codec.bzip2.library": "system-native",
        "yarn.webapp.filter-invalid-xml-chars": "false",
        "yarn.nodemanager.runtime.linux.runc.layer-mounts-interval-secs": "600",
        "fs.s3a.select.input.csv.record.delimiter": "\\n",
        "fs.s3a.change.detection.source": "etag",
        "ipc.[port_number].backoff.enable": "false",
        "yarn.nodemanager.distributed-scheduling.enabled": "false",
        "mapreduce.shuffle.connection-keep-alive.timeout": "5",
        "yarn.resourcemanager.webapp.https.address": "${yarn.resourcemanager.hostname}:8090",
        "yarn.webapp.enable-rest-app-submissions": "true",
        "fs.AbstractFileSystem.s3a.impl": "org.apache.hadoop.fs.s3a.S3A",
        "mapreduce.task.combine.progress.records": "10000",
        "yarn.resourcemanager.epoch.range": "0",
        "yarn.resourcemanager.am.max-attempts": "2",
        "yarn.nodemanager.runtime.linux.runc.image-toplevel-dir": "/runc-root",
        "yarn.nodemanager.linux-container-executor.cgroups.hierarchy": "/hadoop-yarn",
        "fs.AbstractFileSystem.wasbs.impl": "org.apache.hadoop.fs.azure.Wasbs",
        "yarn.timeline-service.entity-group-fs-store.cache-store-class": "org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore",
        "yarn.nodemanager.runtime.linux.runc.allowed-container-networks": "host,none,bridge",
        "fs.ftp.transfer.mode": "BLOCK_TRANSFER_MODE",
        "ipc.[port_number].decay-scheduler.decay-factor": "0.5",
        "fs.har.impl.disable.cache": "true",
        "yarn.webapp.ui2.enable": "false",
        "mapreduce.jobhistory.address": "0.0.0.0:10020",
        "yarn.resourcemanager.nm-tokens.master-key-rolling-interval-secs": "*********(redacted)",
        "yarn.is.minicluster": "false",
        "yarn.nodemanager.address": "${yarn.nodemanager.hostname}:0",
        "fs.abfss.impl": "org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem",
        "ipc.server.log.slow.rpc": "false",
        "ipc.server.reuseaddr": "true",
        "yarn.router.webapp.https.address": "0.0.0.0:8091",
        "yarn.nodemanager.webapp.cross-origin.enabled": "false",
        "fs.wasb.impl": "org.apache.hadoop.fs.azure.NativeAzureFileSystem",
        "fs.AbstractFileSystem.abfs.impl": "org.apache.hadoop.fs.azurebfs.Abfs",
        "hadoop.security.credential.clear-text-fallback": "true",
        "yarn.timeline-service.writer.async.queue.capacity": "100",
        "yarn.resourcemanager.fs.state-store.num-retries": "0",
        "yarn.resourcemanager.nodemanager-connect-retries": "10",
        "fs.ftp.timeout": "0",
        "yarn.resourcemanager.node-labels.provider.fetch-interval-ms": "1800000",
        "yarn.resourcemanager.auto-update.containers": "false",
        "yarn.app.mapreduce.am.job.committer.cancel-timeout": "60000",
        "yarn.scheduler.configuration.zk-store.parent-path": "/confstore",
        "yarn.nodemanager.default-container-executor.log-dirs.permissions": "710",
        "yarn.app.attempt.diagnostics.limit.kc": "64",
        "fs.viewfs.overload.scheme.target.swebhdfs.impl": "org.apache.hadoop.hdfs.web.SWebHdfsFileSystem",
        "yarn.client.failover-no-ha-proxy-provider": "org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider",
        "fs.s3a.change.detection.mode": "server",
        "ftp.bytes-per-checksum": "512",
        "yarn.nodemanager.resource.memory-mb": "-1",
        "yarn.timeline-service.writer.flush-interval-seconds": "60",
        "fs.s3a.fast.upload.active.blocks": "4",
        "yarn.resourcemanager.submission-preprocessor.enabled": "false",
        "yarn.nodemanager.collector-service.thread-count": "5",
        "ipc.[port_number].scheduler.impl": "org.apache.hadoop.ipc.DefaultRpcScheduler",
        "fs.azure.secure.mode": "false",
        "mapreduce.jobhistory.joblist.cache.size": "20000",
        "fs.ftp.host": "0.0.0.0",
        "yarn.nodemanager.log-aggregation.num-log-files-per-app": "30",
        "hadoop.security.kms.client.encrypted.key.cache.low-watermark": "0.3f",
        "fs.s3a.committer.magic.enabled": "true",
        "yarn.timeline-service.client.max-retries": "30",
        "dfs.ha.fencing.ssh.connect-timeout": "30000",
        "yarn.log-aggregation-enable": "false",
        "yarn.system-metrics-publisher.enabled": "false",
        "mapreduce.reduce.markreset.buffer.percent": "0.0",
        "fs.AbstractFileSystem.viewfs.impl": "org.apache.hadoop.fs.viewfs.ViewFs",
        "yarn.resourcemanager.nodemanagers.heartbeat-interval-speedup-factor": "1.0",
        "ha.failover-controller.new-active.rpc-timeout.ms": "60000",
        "yarn.nodemanager.container-localizer.java.opts": "-Xmx256m",
        "yarn.app.mapreduce.am.job.committer.commit-window": "10000",
        "hadoop.tags.system": "YARN,HDFS,NAMENODE,DATANODE,REQUIRED,SECURITY,KERBEROS,PERFORMANCE,CLIENT\n      ,SERVER,DEBUG,DEPRECATED,COMMON,OPTIONAL",
        "hadoop.caller.context.signature.max.size": "40",
        "yarn.scheduler.configuration.store.max-logs": "1000",
        "yarn.nodemanager.node-attributes.provider.fetch-interval-ms": "600000",
        "hadoop.http.cross-origin.enabled": "false",
        "hadoop.zk.acl": "world:anyone:rwcda",
        "mapreduce.task.io.sort.factor": "10",
        "yarn.nodemanager.amrmproxy.client.thread-count": "25",
        "mapreduce.jobhistory.datestring.cache.size": "200000",
        "mapreduce.job.acl-modify-job": " ",
        "yarn.nodemanager.windows-container.memory-limit.enabled": "false",
        "yarn.timeline-service.webapp.address": "${yarn.timeline-service.hostname}:8188",
        "yarn.nodemanager.container-manager.thread-count": "20",
        "yarn.minicluster.fixed.ports": "false",
        "yarn.cluster.max-application-priority": "0",
        "yarn.timeline-service.ttl-enable": "true",
        "mapreduce.jobhistory.recovery.store.fs.uri": "${hadoop.tmp.dir}/mapred/history/recoverystore",
        "ipc.[port_number].decay-scheduler.backoff.responsetime.enable": "false",
        "yarn.client.load.resource-types.from-server": "false",
        "ha.zookeeper.session-timeout.ms": "10000",
        "ipc.[port_number].decay-scheduler.metrics.top.user.count": "10",
        "tfile.io.chunk.size": "1048576",
        "fs.s3a.s3guard.ddb.table.capacity.write": "0",
        "yarn.dispatcher.print-events-info.threshold": "5000",
        "mapreduce.job.speculative.slowtaskthreshold": "1.0",
        "io.serializations": "org.apache.hadoop.io.serializer.WritableSerialization, org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization, org.apache.hadoop.io.serializer.avro.AvroReflectSerialization",
        "hadoop.security.kms.client.failover.sleep.max.millis": "2000",
        "hadoop.security.group.mapping.ldap.directory.search.timeout": "10000",
        "fs.swift.impl": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem",
        "yarn.nodemanager.local-cache.max-files-per-directory": "8192",
        "yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin.num-manifests-to-cache": "10",
        "mapreduce.map.sort.spill.percent": "0.80",
        "yarn.timeline-service.entity-group-fs-store.scan-interval-seconds": "60",
        "fs.s3a.select.enabled": "true",
        "mapreduce.ifile.readahead": "true",
        "yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms": "300000",
        "yarn.timeline-service.hbase.coprocessor.jar.hdfs.location": "/hbase/coprocessor/hadoop-yarn-server-timelineservice.jar",
        "hadoop.security.kms.client.encrypted.key.cache.num.refill.threads": "2",
        "yarn.resourcemanager.scheduler.class": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler",
        "hadoop.http.sni.host.check.enabled": "false",
        "fs.client.resolve.topology.enabled": "false",
        "yarn.nodemanager.runtime.linux.allowed-runtimes": "default",
        "io.skip.checksum.errors": "false",
        "yarn.timeline-service.client.best-effort": "false",
        "yarn.node-attribute.fs-store.impl.class": "org.apache.hadoop.yarn.server.resourcemanager.nodelabels.FileSystemNodeAttributeStore",
        "fs.s3a.retry.interval": "500ms",
        "yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled": "*********(redacted)",
        "hadoop.security.group.mapping.ldap.posix.attr.uid.name": "uidNumber",
        "fs.AbstractFileSystem.swebhdfs.impl": "org.apache.hadoop.fs.SWebHdfs",
        "yarn.nodemanager.elastic-memory-control.timeout-sec": "5",
        "yarn.timeline-service.reader.webapp.address": "${yarn.timeline-service.webapp.address}",
        "yarn.resourcemanager.placement-constraints.algorithm.pool-size": "1",
        "yarn.app.mapreduce.am.command-opts": "-Xmx1024m",
        "fs.s3a.metadatastore.fail.on.write.error": "true",
        "mapreduce.cluster.local.dir": "${hadoop.tmp.dir}/mapred/local",
        "io.mapfile.bloom.error.rate": "0.005",
        "yarn.sharedcache.store.class": "org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore",
        "ha.failover-controller.graceful-fence.rpc-timeout.ms": "5000",
        "ftp.replication": "3",
        "fs.getspaceused.jitterMillis": "60000",
        "hadoop.security.uid.cache.secs": "14400",
        "mapreduce.job.maxtaskfailures.per.tracker": "3",
        "fs.s3a.metadatastore.impl": "org.apache.hadoop.fs.s3a.s3guard.NullMetadataStore",
        "yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts": "3",
        "yarn.timeline-service.webapp.xfs-filter.xframe-options": "SAMEORIGIN",
        "fs.s3a.connection.timeout": "200000",
        "yarn.app.mapreduce.am.webapp.https.enabled": "false",
        "mapreduce.job.max.split.locations": "15",
        "yarn.resourcemanager.nm-container-queuing.max-queue-length": "15",
        "yarn.resourcemanager.delegation-token.always-cancel": "*********(redacted)",
        "hadoop.registry.zk.session.timeout.ms": "60000",
        "yarn.federation.cache-ttl.secs": "300",
        "mapreduce.jvm.system-properties-to-log": "os.name,os.version,java.home,java.runtime.version,java.vendor,java.version,java.vm.name,java.class.path,java.io.tmpdir,user.dir,user.name",
        "yarn.resourcemanager.opportunistic-container-allocation.nodes-used": "10",
        "yarn.minicluster.yarn.nodemanager.resource.memory-mb": "4096",
        "yarn.resourcemanager.admin.client.thread-count": "1",
        "yarn.dispatcher.drain-events.timeout": "300000",
        "yarn.timeline-service.entity-group-fs-store.active-dir": "/tmp/entity-file-history/active",
        "mapreduce.shuffle.transfer.buffer.size": "131072",
        "yarn.timeline-service.client.retry-interval-ms": "1000",
        "yarn.timeline-service.flowname.max-size": "0",
        "yarn.http.policy": "HTTP_ONLY",
        "fs.s3a.socket.send.buffer": "8192",
        "fs.AbstractFileSystem.abfss.impl": "org.apache.hadoop.fs.azurebfs.Abfss",
        "yarn.sharedcache.uploader.server.address": "0.0.0.0:8046",
        "yarn.resourcemanager.delegation-token.max-conf-size-bytes": "*********(redacted)",
        "hadoop.http.authentication.token.validity": "*********(redacted)",
        "mapreduce.shuffle.max.connections": "0",
        "mapreduce.job.emit-timeline-data": "false",
        "yarn.nodemanager.resource.system-reserved-memory-mb": "-1",
        "hadoop.kerberos.min.seconds.before.relogin": "60",
        "mapreduce.jobhistory.move.thread-count": "3",
        "ipc.[port_number].decay-scheduler.backoff.responsetime.thresholds": "10s,20s,30s,40s",
        "fs.s3a.buffer.dir": "${hadoop.tmp.dir}/s3a",
        "hadoop.ssl.enabled.protocols": "TLSv1.2",
        "mapreduce.jobhistory.admin.address": "0.0.0.0:10033",
        "yarn.log-aggregation-status.time-out.ms": "600000",
        "fs.s3a.accesspoint.required": "false",
        "yarn.nodemanager.health-checker.interval-ms": "600000",
        "yarn.router.clientrm.interceptor-class.pipeline": "org.apache.hadoop.yarn.server.router.clientrm.DefaultClientRequestInterceptor",
        "yarn.nodemanager.runtime.linux.sandbox-mode.local-dirs.permissions": "read",
        "yarn.resourcemanager.activities-manager.app-activities.max-queue-length": "100",
        "yarn.nodemanager.pmem-check-enabled": "true",
        "yarn.federation.enabled": "false",
        "yarn.resourcemanager.nm-container-queuing.load-comparator": "QUEUE_LENGTH",
        "mapreduce.job.complete.cancel.delegation.tokens": "*********(redacted)",
        "yarn.nodemanager.amrmproxy.ha.enable": "false",
        "fs.AbstractFileSystem.gs.impl": "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS",
        "mapreduce.shuffle.port": "13562",
        "yarn.resourcemanager.max-log-aggregation-diagnostics-in-memory": "10",
        "yarn.resourcemanager.zk-appid-node.split-index": "0",
        "ftp.blocksize": "67108864",
        "yarn.router.rmadmin.interceptor-class.pipeline": "org.apache.hadoop.yarn.server.router.rmadmin.DefaultRMAdminRequestInterceptor",
        "yarn.nodemanager.log-container-debug-info.enabled": "true",
        "yarn.resourcemanager.application-https.policy": "NONE",
        "yarn.client.max-cached-nodemanagers-proxies": "0",
        "yarn.nodemanager.linux-container-executor.cgroups.delete-delay-ms": "20",
        "yarn.nodemanager.delete.debug-delay-sec": "0",
        "yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage": "90.0",
        "mapreduce.app-submission.cross-platform": "false",
        "yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms": "10000",
        "yarn.nodemanager.container-retry-minimum-interval-ms": "1000",
        "hadoop.security.groups.cache.secs": "300",
        "yarn.workflow-id.tag-prefix": "workflowid:",
        "fs.azure.local.sas.key.mode": "false",
        "ipc.maximum.data.length": "134217728",
        "yarn.router.pipeline.cache-max-size": "25",
        "fs.s3a.endpoint": "s3.amazonaws.com",
        "mapreduce.shuffle.max.threads": "0",
        "yarn.resourcemanager.resource-tracker.nm.ip-hostname-check": "false",
        "hadoop.security.authorization": "false",
        "fs.s3a.paging.maximum": "5000",
        "nfs.exports.allowed.hosts": "* rw",
        "mapreduce.jobhistory.http.policy": "HTTP_ONLY",
        "yarn.sharedcache.store.in-memory.check-period-mins": "720",
        "hadoop.security.group.mapping.ldap.ssl": "false",
        "yarn.scheduler.configuration.leveldb-store.compaction-interval-secs": "86400",
        "yarn.timeline-service.writer.class": "org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl",
        "ha.zookeeper.parent-znode": "/hadoop-ha",
        "yarn.resourcemanager.submission-preprocessor.file-refresh-interval-ms": "60000",
        "hadoop.security.group.mapping.ldap.search.filter.group": "(objectClass=group)",
        "yarn.resourcemanager.placement-constraints.scheduler.pool-size": "1",
        "yarn.resourcemanager.activities-manager.cleanup-interval-ms": "5000",
        "yarn.admin.acl": "*",
        "yarn.sharedcache.admin.thread-count": "1",
        "mapreduce.task.local-fs.write-limit.bytes": "-1",
        "fs.adl.oauth2.access.token.provider.type": "*********(redacted)",
        "yarn.nodemanager.resource-plugins.gpu.docker-plugin": "nvidia-docker-v1",
        "fs.s3a.downgrade.syncable.exceptions": "true",
        "yarn.client.application-client-protocol.poll-interval-ms": "200",
        "yarn.nodemanager.log-aggregation.policy.class": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AllContainerLogAggregationPolicy",
        "mapreduce.reduce.shuffle.merge.percent": "0.66",
        "yarn.nodemanager.resourcemanager.minimum.version": "NONE",
        "mapreduce.job.speculative.speculative-cap-running-tasks": "0.1",
        "ipc.[port_number].identity-provider.impl": "org.apache.hadoop.ipc.UserIdentityProvider",
        "yarn.nodemanager.recovery.supervised": "false",
        "mapreduce.reduce.skip.maxgroups": "0",
        "yarn.resourcemanager.ha.automatic-failover.enabled": "true",
        "yarn.nodemanager.container-log-monitor.total-size-limit-bytes": "10000000000",
        "mapreduce.reduce.shuffle.connect.timeout": "180000",
        "yarn.nodemanager.health-checker.scripts": "script",
        "yarn.resourcemanager.address": "${yarn.resourcemanager.hostname}:8032",
        "ipc.client.ping": "true",
        "mapreduce.shuffle.ssl.file.buffer.size": "65536",
        "yarn.resourcemanager.ha.automatic-failover.embedded": "true",
        "fs.s3a.s3guard.consistency.retry.interval": "2s",
        "yarn.resourcemanager.nm-container-queuing.queue-limit-stdev": "1.0f",
        "mapreduce.job.end-notification.max.attempts": "5",
        "yarn.nodemanager.keytab": "/etc/krb5.keytab",
        "mapreduce.jobhistory.keytab": "/etc/security/keytab/jhs.service.keytab",
        "fs.s3a.threads.max": "64",
        "yarn.nodemanager.runtime.linux.docker.image-update": "false",
        "mapreduce.reduce.shuffle.input.buffer.percent": "0.70",
        "fs.viewfs.overload.scheme.target.abfss.impl": "org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem",
        "yarn.dispatcher.cpu-monitor.samples-per-min": "60",
        "hadoop.security.token.service.use_ip": "*********(redacted)",
        "yarn.nodemanager.runtime.linux.docker.allowed-container-networks": "host,none,bridge",
        "yarn.nodemanager.node-labels.resync-interval-ms": "120000",
        "mapreduce.job.end-notification.max.retry.interval": "5000",
        "yarn.nodemanager.containers-launcher.class": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher",
        "fs.s3a.multipart.purge": "false",
        "yarn.scheduler.configuration.store.class": "file",
        "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.DefaultCodec",
        "yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled": "false",
        "ipc.client.bind.wildcard.addr": "false",
        "yarn.resourcemanager.webapp.rest-csrf.enabled": "false",
        "ha.health-monitor.connect-retry-interval.ms": "1000",
        "hadoop.tmp.dir": "/tmp/hadoop-${user.name}",
        "mapreduce.job.maps": "2",
        "mapreduce.jobhistory.webapp.rest-csrf.custom-header": "X-XSRF-Header",
        "yarn.log-aggregation.retain-check-interval-seconds": "-1",
        "yarn.resourcemanager.resource-tracker.client.thread-count": "50",
        "yarn.resourcemanager.ha.automatic-failover.zk-base-path": "/yarn-leader-election",
        "fs.AbstractFileSystem.wasb.impl": "org.apache.hadoop.fs.azure.Wasb",
        "mapreduce.client.submit.file.replication": "10",
        "mapreduce.jobhistory.minicluster.fixed.ports": "false",
        "fs.s3a.multipart.threshold": "128M",
        "yarn.resourcemanager.webapp.xfs-filter.xframe-options": "SAMEORIGIN",
        "yarn.rm.system-metrics-publisher.emit-container-events": "false",
        "yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size": "10000",
        "io.seqfile.local.dir": "${hadoop.tmp.dir}/io/local",
        "fs.s3a.s3guard.ddb.throttle.retry.interval": "100ms",
        "mapreduce.jobhistory.done-dir": "${yarn.app.mapreduce.am.staging-dir}/history/done",
        "ipc.server.purge.interval": "15",
        "ipc.client.idlethreshold": "4000",
        "yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage": "false",
        "mapreduce.reduce.input.buffer.percent": "0.0",
        "yarn.nodemanager.runtime.linux.docker.userremapping-gid-threshold": "1",
        "yarn.nodemanager.webapp.rest-csrf.enabled": "false",
        "yarn.resourcemanager.history-writer.multi-threaded-dispatcher.pool-size": "10",
        "fs.ftp.host.port": "21",
        "ipc.ping.interval": "60000",
        "yarn.resourcemanager.admin.address": "${yarn.resourcemanager.hostname}:8033",
        "file.client-write-packet-size": "65536",
        "ipc.client.kill.max": "10",
        "mapreduce.reduce.speculative": "true",
        "ipc.client.connection.maxidletime": "10000",
        "mapreduce.task.io.sort.mb": "100",
        "yarn.nodemanager.localizer.client.thread-count": "5",
        "io.erasurecode.codec.rs.rawcoders": "rs_native,rs_java",
        "io.erasurecode.codec.rs-legacy.rawcoders": "rs-legacy_java",
        "yarn.nodemanager.localizer.cache.cleanup.interval-ms": "600000",
        "yarn.nodemanager.process-kill-wait.ms": "5000",
        "mapreduce.job.hdfs-servers": "${fs.defaultFS}",
        "fs.s3a.multiobjectdelete.enable": "true",
        "fs.viewfs.overload.scheme.target.wasb.impl": "org.apache.hadoop.fs.azure.NativeAzureFileSystem",
        "hadoop.security.group.mapping.ldap.search.attr.member": "member",
        "hadoop.security.random.device.file.path": "/dev/urandom",
        "hadoop.security.sensitive-config-keys": "*********(redacted)",
        "hadoop.rpc.socket.factory.class.default": "org.apache.hadoop.net.StandardSocketFactory",
        "yarn.intermediate-data-encryption.enable": "false",
        "hadoop.security.key.default.bitlength": "128",
        "mapreduce.job.reducer.unconditional-preempt.delay.sec": "300",
        "yarn.nodemanager.disk-health-checker.interval-ms": "120000",
        "yarn.nodemanager.log.deletion-threads-count": "4",
        "fs.s3a.committer.abort.pending.uploads": "true",
        "yarn.webapp.filter-entity-list-by-user": "false",
        "yarn.resourcemanager.activities-manager.app-activities.ttl-ms": "600000",
        "yarn.sharedcache.admin.address": "0.0.0.0:8047",
        "yarn.resourcemanager.placement-constraints.algorithm.iterator": "SERIAL",
        "hadoop.security.crypto.codec.classes.aes.ctr.nopadding": "org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec, org.apache.hadoop.crypto.JceAesCtrCryptoCodec",
        "mapreduce.job.cache.limit.max-resources-mb": "0",
        "fs.s3a.connection.ssl.enabled": "true",
        "yarn.app.mapreduce.am.webapp.https.client.auth": "false",
        "hadoop.workaround.non.threadsafe.getpwuid": "true",
        "fs.df.interval": "60000",
        "ipc.[port_number].decay-scheduler.thresholds": "13,25,50",
        "yarn.sharedcache.cleaner.resource-sleep-ms": "0",
        "yarn.nodemanager.disk-health-checker.min-healthy-disks": "0.25",
        "hadoop.shell.missing.defaultFs.warning": "false",
        "io.file.buffer.size": "65536",
        "fs.s3a.s3guard.ddb.max.retries": "9",
        "fs.viewfs.overload.scheme.target.file.impl": "org.apache.hadoop.fs.LocalFileSystem",
        "yarn.resourcemanager.connect.retry-interval.ms": "30000",
        "yarn.nodemanager.container.stderr.pattern": "{*stderr*,*STDERR*}",
        "yarn.scheduler.minimum-allocation-mb": "1024",
        "hadoop.http.cross-origin.max-age": "1800",
        "fs.s3a.connection.establish.timeout": "5000",
        "yarn.federation.state-store.class": "org.apache.hadoop.yarn.server.federation.store.impl.MemoryFederationStateStore",
        "mapreduce.reduce.log.level": "INFO",
        "yarn.resourcemanager.placement-constraints.retry-attempts": "3",
        "yarn.nodemanager.vmem-pmem-ratio": "2.1",
        "hadoop.rpc.protection": "authentication",
        "yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size": "10",
        "yarn.app.mapreduce.am.staging-dir": "/tmp/hadoop-yarn/staging",
        "mapreduce.reduce.shuffle.read.timeout": "180000",
        "io.erasurecode.codec.xor.rawcoders": "xor_native,xor_java",
        "fs.s3a.s3guard.consistency.retry.limit": "7",
        "mapreduce.job.running.map.limit": "0",
        "yarn.minicluster.control-resource-monitoring": "false",
        "hadoop.ssl.require.client.cert": "false",
        "hadoop.kerberos.kinit.command": "kinit",
        "adl.http.timeout": "-1",
        "hadoop.security.dns.log-slow-lookups.threshold.ms": "1000",
        "mapreduce.job.ubertask.enable": "false",
        "hadoop.caller.context.enabled": "false",
        "hadoop.security.group.mapping.ldap.num.attempts": "3",
        "ha.health-monitor.rpc-timeout.ms": "45000",
        "yarn.nodemanager.remote-app-log-dir": "/tmp/logs",
        "hadoop.zk.timeout-ms": "10000",
        "fs.s3a.s3guard.cli.prune.age": "86400000",
        "yarn.nodemanager.resource.pcores-vcores-multiplier": "1.0",
        "yarn.nodemanager.runtime.linux.sandbox-mode": "disabled",
        "fs.viewfs.overload.scheme.target.webhdfs.impl": "org.apache.hadoop.hdfs.web.WebHdfsFileSystem",
        "fs.s3a.committer.threads": "8",
        "yarn.nodemanager.delete.thread-count": "4",
        "mapreduce.job.finish-when-all-reducers-done": "true",
        "hadoop.registry.jaas.context": "Client",
        "yarn.timeline-service.leveldb-timeline-store.path": "${hadoop.tmp.dir}/yarn/timeline",
        "io.map.index.interval": "128",
        "mapreduce.jobhistory.webapp.rest-csrf.enabled": "false",
        "fs.s3a.change.detection.version.required": "true",
        "yarn.nodemanager.localizer.fetch.thread-count": "4",
        "yarn.resourcemanager.scheduler.client.thread-count": "50",
        "hadoop.ssl.hostname.verifier": "DEFAULT",
        "yarn.timeline-service.leveldb-state-store.path": "${hadoop.tmp.dir}/yarn/timeline",
        "hadoop.zk.retry-interval-ms": "1000",
        "hadoop.security.crypto.buffer.size": "8192",
        "yarn.nodemanager.node-labels.provider.fetch-interval-ms": "600000",
        "mapreduce.jobhistory.recovery.store.leveldb.path": "${hadoop.tmp.dir}/mapred/history/recoverystore",
        "yarn.client.failover-retries-on-socket-timeouts": "0",
        "fs.s3a.ssl.channel.mode": "default_jsse",
        "yarn.nodemanager.resource.memory.enabled": "false",
        "fs.azure.authorization.caching.enable": "true",
        "hadoop.security.instrumentation.requires.admin": "false",
        "yarn.resourcemanager.nm-container-queuing.max-queue-wait-time-ms": "100",
        "fs.abfs.impl": "org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem",
        "mapreduce.job.counters.max": "120",
        "yarn.timeline-service.store-class": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore",
        "mapreduce.jobhistory.move.interval-ms": "180000",
        "mapreduce.job.classloader": "false",
        "mapreduce.task.profile.map.params": "${mapreduce.task.profile.params}",
        "ipc.client.connect.timeout": "20000",
        "hadoop.security.auth_to_local.mechanism": "hadoop",
        "yarn.resourcemanager.reservation-system.planfollower.time-step": "1000",
        "yarn.resourcemanager.activities-manager.scheduler-activities.ttl-ms": "600000",
        "yarn.nodemanager.runtime.linux.docker.enable-userremapping.allowed": "true",
        "yarn.webapp.api-service.enable": "false",
        "yarn.nodemanager.container.stderr.tail.bytes": "4096",
        "yarn.nodemanager.disk-health-checker.disk-free-space-threshold.enabled": "true",
        "hadoop.security.group.mapping.ldap.read.timeout.ms": "60000",
        "hadoop.security.groups.cache.warn.after.ms": "5000",
        "file.bytes-per-checksum": "512",
        "mapreduce.outputcommitter.factory.scheme.s3a": "org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory",
        "yarn.timeline-service.app-collector.linger-period.ms": "60000",
        "yarn.nm.liveness-monitor.expiry-interval-ms": "600000",
        "yarn.nodemanager.recovery.enabled": "false",
        "mapreduce.job.end-notification.retry.interval": "1000",
        "fs.du.interval": "600000",
        "fs.ftp.impl": "org.apache.hadoop.fs.ftp.FTPFileSystem",
        "hadoop.security.groups.cache.background.reload": "false",
        "yarn.nodemanager.container-monitor.enabled": "true",
        "yarn.nodemanager.elastic-memory-control.enabled": "false",
        "net.topology.script.number.args": "100",
        "mapreduce.task.merge.progress.records": "10000",
        "yarn.nodemanager.container-executor.exit-code-file.timeout-ms": "2000",
        "mapreduce.fileoutputcommitter.algorithm.version": "1",
        "yarn.sharedcache.root-dir": "/sharedcache",
        "fs.s3a.retry.throttle.limit": "20",
        "hadoop.http.authentication.type": "simple",
        "fs.viewfs.overload.scheme.target.oss.impl": "org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem",
        "mapreduce.job.cache.limit.max-resources": "0",
        "mapreduce.task.userlog.limit.kb": "0",
        "ipc.[port_number].weighted-cost.handler": "1",
        "yarn.resourcemanager.scheduler.monitor.enable": "false",
        "ipc.client.connect.max.retries": "10",
        "hadoop.registry.zk.retry.times": "5",
        "yarn.nodemanager.localizer.address": "${yarn.nodemanager.hostname}:8040",
        "yarn.timeline-service.keytab": "/etc/krb5.keytab",
        "mapreduce.reduce.shuffle.fetch.retry.timeout-ms": "30000",
        "yarn.resourcemanager.rm.container-allocation.expiry-interval-ms": "600000",
        "yarn.resourcemanager.work-preserving-recovery.enabled": "true",
        "mapreduce.map.skip.maxrecords": "0",
        "yarn.nodemanager.resource-monitor.interval-ms": "3000",
        "yarn.nodemanager.resource-plugins.gpu.allowed-gpu-devices": "auto",
        "mapreduce.job.sharedcache.mode": "disabled",
        "mapreduce.map.cpu.vcores": "1",
        "mapreduce.job.reducer.preempt.delay.sec": "0",
        "hadoop.util.hash.type": "murmur",
        "yarn.nodemanager.webapp.rest-csrf.custom-header": "X-XSRF-Header",
        "mapreduce.shuffle.listen.queue.size": "128",
        "yarn.scheduler.configuration.mutation.acl-policy.class": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.DefaultConfigurationMutationACLPolicy",
        "yarn.log-aggregation.file-formats": "TFile",
        "yarn.timeline-service.client.fd-retain-secs": "300",
        "fs.s3a.select.output.csv.field.delimiter": ",",
        "yarn.nodemanager.health-checker.timeout-ms": "1200000",
        "hadoop.user.group.static.mapping.overrides": "dr.who=;",
        "fs.azure.sas.expiry.period": "90d",
        "fs.s3a.select.output.csv.record.delimiter": "\\n",
        "mapreduce.jobhistory.recovery.store.class": "org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService",
        "fs.viewfs.overload.scheme.target.https.impl": "org.apache.hadoop.fs.http.HttpsFileSystem",
        "fs.s3a.s3guard.ddb.table.sse.enabled": "false",
        "yarn.resourcemanager.fail-fast": "${yarn.fail-fast}",
        "yarn.resourcemanager.proxy-user-privileges.enabled": "false",
        "yarn.router.webapp.interceptor-class.pipeline": "org.apache.hadoop.yarn.server.router.webapp.DefaultRequestInterceptorREST",
        "yarn.nodemanager.resource.memory.cgroups.soft-limit-percentage": "90.0",
        "yarn.nodemanager.disk-validator": "basic",
        "yarn.nodemanager.linux-container-executor.cgroups.delete-timeout-ms": "1000",
        "fs.AbstractFileSystem.file.impl": "org.apache.hadoop.fs.local.LocalFs",
        "yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds": "-1",
        "mapreduce.jobhistory.cleaner.interval-ms": "86400000",
        "yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs": "*********(redacted)",
        "hadoop.ssl.server.conf": "ssl-server.xml",
        "fs.s3a.retry.throttle.interval": "100ms",
        "mapreduce.client.completion.pollinterval": "5000",
        "hadoop.ssl.keystores.factory.class": "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory",
        "yarn.timeline-service.entity-group-fs-store.done-dir": "/tmp/entity-file-history/done/",
        "yarn.resourcemanager.fs.state-store.uri": "${hadoop.tmp.dir}/yarn/system/rmstore",
        "mapreduce.jobhistory.always-scan-user-dir": "false",
        "yarn.app.mapreduce.client.job.max-retries": "3",
        "fs.viewfs.overload.scheme.target.ftp.impl": "org.apache.hadoop.fs.ftp.FTPFileSystem",
        "mapreduce.reduce.shuffle.retry-delay.max.ms": "60000",
        "hadoop.security.group.mapping.ldap.connection.timeout.ms": "60000",
        "mapreduce.task.profile.params": "-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s",
        "yarn.app.mapreduce.shuffle.log.backups": "0",
        "yarn.nodemanager.container-diagnostics-maximum-size": "10000",
        "hadoop.registry.zk.retry.interval.ms": "1000",
        "hadoop.registry.zk.quorum": "localhost:2181",
        "yarn.nodemanager.runtime.linux.runc.allowed-container-runtimes": "runc",
        "mapreduce.output.fileoutputformat.compress": "false",
        "fs.s3a.assumed.role.session.duration": "30m",
        "hadoop.security.group.mapping.ldap.conversion.rule": "none",
        "seq.io.sort.factor": "100",
        "fs.viewfs.overload.scheme.target.ofs.impl": "org.apache.hadoop.fs.ozone.RootedOzoneFileSystem",
        "yarn.sharedcache.cleaner.initial-delay-mins": "10",
        "yarn.app.mapreduce.am.resource.cpu-vcores": "1",
        "yarn.timeline-service.enabled": "false",
        "yarn.nodemanager.runtime.linux.docker.capabilities": "CHOWN,DAC_OVERRIDE,FSETID,FOWNER,MKNOD,NET_RAW,SETGID,SETUID,SETFCAP,SETPCAP,NET_BIND_SERVICE,SYS_CHROOT,KILL,AUDIT_WRITE",
        "yarn.acl.enable": "false",
        "hadoop.prometheus.endpoint.enabled": "false",
        "hadoop.security.group.mapping.ldap.num.attempts.before.failover": "3",
        "mapreduce.task.profile": "false",
        "fs.s3a.metadatastore.metadata.ttl": "15m",
        "yarn.nodemanager.opportunistic-containers-use-pause-for-preemption": "false",
        "yarn.nodemanager.resource.percentage-physical-cpu-limit": "100",
        "yarn.resourcemanager.opportunistic.max.container-allocation.per.am.heartbeat": "-1",
        "ipc.[port_number].cost-provider.impl": "org.apache.hadoop.ipc.DefaultCostProvider",
        "yarn.nodemanager.resource.memory.cgroups.swappiness": "0",
        "yarn.timeline-service.address": "${yarn.timeline-service.hostname}:10200",
        "ha.failover-controller.graceful-fence.connection.retries": "1",
        "yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user": "nobody",
        "yarn.timeline-service.reader.class": "org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineReaderImpl",
        "yarn.resourcemanager.configuration.provider-class": "org.apache.hadoop.yarn.LocalConfigurationProvider",
        "yarn.nodemanager.runtime.linux.docker.userremapping-uid-threshold": "1",
        "yarn.resourcemanager.configuration.file-system-based-store": "/yarn/conf",
        "mapreduce.job.cache.limit.max-single-resource-mb": "0",
        "yarn.nodemanager.runtime.linux.docker.stop.grace-period": "10",
        "yarn.resourcemanager.resource-profiles.source-file": "resource-profiles.json",
        "mapreduce.job.dfs.storage.capacity.kill-limit-exceed": "false",
        "mapreduce.jobhistory.client.thread-count": "10",
        "tfile.fs.input.buffer.size": "262144",
        "mapreduce.client.progressmonitor.pollinterval": "1000",
        "yarn.nodemanager.log-dirs": "${yarn.log.dir}/userlogs",
        "fs.automatic.close": "true",
        "yarn.resourcemanager.delegation-token-renewer.thread-retry-interval": "*********(redacted)",
        "fs.s3a.select.input.csv.quote.character": "\"",
        "yarn.nodemanager.hostname": "0.0.0.0",
        "yarn.nodemanager.runtime.linux.runc.manifest-to-resources-plugin": "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.HdfsManifestToResourcesPlugin",
        "yarn.nodemanager.remote-app-log-dir-include-older": "true",
        "ftp.stream-buffer-size": "4096",
        "yarn.fail-fast": "false",
        "yarn.nodemanager.runtime.linux.runc.layer-mounts-to-keep": "100",
        "yarn.timeline-service.app-aggregation-interval-secs": "15",
        "hadoop.security.group.mapping.ldap.search.filter.user": "(&(objectClass=user)(sAMAccountName={0}))",
        "ipc.[port_number].weighted-cost.lockshared": "10",
        "yarn.nodemanager.container-localizer.log.level": "INFO",
        "mapreduce.job.ubertask.maxmaps": "9",
        "fs.s3a.threads.keepalivetime": "60",
        "mapreduce.jobhistory.webapp.rest-csrf.methods-to-ignore": "GET,OPTIONS,HEAD",
        "mapreduce.task.files.preserve.failedtasks": "false",
        "yarn.app.mapreduce.client.job.retry-interval": "2000",
        "fs.s3a.select.output.csv.quote.escape.character": "\\\\",
        "yarn.timeline-service.client.drain-entities.timeout.ms": "2000",
        "yarn.nodemanager.resource-plugins.fpga.vendor-plugin.class": "org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin",
        "mapreduce.job.encrypted-intermediate-data.buffer.kb": "128",
        "fs.client.resolve.remote.symlinks": "true",
        "fs.s3a.executor.capacity": "16",
        "yarn.timeline-service.entity-group-fs-store.retain-seconds": "604800",
        "yarn.nodemanager.local-dirs": "${hadoop.tmp.dir}/nm-local-dir",
        "yarn.sharedcache.store.in-memory.staleness-period-mins": "10080",
        "fs.adl.impl": "org.apache.hadoop.fs.adl.AdlFileSystem",
        "yarn.resourcemanager.nodemanager.minimum.version": "NONE",
        "yarn.timeline-service.reader.webapp.https.address": "${yarn.timeline-service.webapp.https.address}",
        "yarn.resourcemanager.delegation.token.max-lifetime": "*********(redacted)",
        "hadoop.kerberos.keytab.login.autorenewal.enabled": "false",
        "yarn.resourcemanager.nodemanagers.heartbeat-interval-min-ms": "1000",
        "yarn.timeline-service.entity-group-fs-store.summary-store": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore",
        "mapreduce.reduce.cpu.vcores": "1",
        "yarn.nodemanager.webapp.https.address": "0.0.0.0:8044",
        "hadoop.http.cross-origin.allowed-origins": "*",
        "mapreduce.job.encrypted-intermediate-data": "false",
        "yarn.nodemanager.disk-health-checker.disk-utilization-threshold.enabled": "true",
        "yarn.resourcemanager.metrics.runtime.buckets": "60,300,1440",
        "yarn.timeline-service.generic-application-history.max-applications": "10000",
        "mapreduce.shuffle.connection-keep-alive.enable": "false",
        "yarn.node-labels.configuration-type": "centralized",
        "fs.s3a.path.style.access": "false",
        "yarn.nodemanager.aux-services.mapreduce_shuffle.class": "org.apache.hadoop.mapred.ShuffleHandler",
        "yarn.resourcemanager.application.max-tags": "10",
        "hadoop.domainname.resolver.impl": "org.apache.hadoop.net.DNSDomainNameResolver",
        "mapreduce.jobhistory.webapp.xfs-filter.xframe-options": "SAMEORIGIN",
        "yarn.app.mapreduce.am.staging-dir.erasurecoding.enabled": "false",
        "net.topology.impl": "org.apache.hadoop.net.NetworkTopology",
        "io.map.index.skip": "0",
        "fs.ftp.data.connection.mode": "ACTIVE_LOCAL_DATA_CONNECTION_MODE",
        "yarn.nodemanager.log-aggregation.compression-type": "none",
        "yarn.timeline-service.version": "1.0f",
        "yarn.resourcemanager.system-metrics-publisher.timeline-server-v1.batch-size": "1000",
        "fs.s3a.select.errors.include.sql": "false",
        "fs.s3a.connection.request.timeout": "0",
        "yarn.nodemanager.runtime.linux.docker.host-pid-namespace.allowed": "false",
        "yarn.nodemanager.recovery.dir": "${hadoop.tmp.dir}/yarn-nm-recovery",
        "fs.s3a.max.total.tasks": "32",
        "mapreduce.job.local-fs.single-disk-limit.check.kill-limit-exceed": "true",
        "fs.azure.buffer.dir": "${hadoop.tmp.dir}/abfs",
        "yarn.scheduler.maximum-allocation-vcores": "4",
        "hadoop.http.cross-origin.allowed-headers": "X-Requested-With,Content-Type,Accept,Origin",
        "yarn.ipc.rpc.class": "org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC",
        "mapreduce.reduce.maxattempts": "4",
        "hadoop.security.dns.log-slow-lookups.enabled": "false",
        "mapreduce.job.committer.setup.cleanup.needed": "true",
        "hadoop.security.secure.random.impl": "org.apache.hadoop.crypto.random.OpensslSecureRandom",
        "mapreduce.job.running.reduce.limit": "0",
        "ipc.maximum.response.length": "134217728",
        "yarn.resourcemanager.webapp.rest-csrf.methods-to-ignore": "GET,OPTIONS,HEAD",
        "mapreduce.job.token.tracking.ids.enabled": "*********(redacted)",
        "hadoop.caller.context.max.size": "128",
        "yarn.nodemanager.runtime.linux.docker.delayed-removal.allowed": "false",
        "hadoop.registry.system.acls": "sasl:yarn@, sasl:mapred@, sasl:hdfs@",
        "fs.s3a.fast.upload.buffer": "disk",
        "mapreduce.jobhistory.intermediate-done-dir": "${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate",
        "yarn.app.mapreduce.shuffle.log.separate": "true",
        "yarn.log-aggregation.debug.filesize": "104857600",
        "fs.s3a.readahead.range": "64K",
        "hadoop.http.authentication.simple.anonymous.allowed": "true",
        "fs.s3a.attempts.maximum": "20",
        "yarn.resourcemanager.delegation-token-renewer.thread-timeout": "*********(redacted)",
        "yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size": "10000",
        "yarn.nodemanager.aux-services.manifest.reload-ms": "0",
        "yarn.nodemanager.emit-container-events": "true",
        "yarn.resourcemanager.resource-profiles.enabled": "false",
        "yarn.timeline-service.hbase-schema.prefix": "prod.",
        "fs.azure.authorization": "false",
        "yarn.resourcemanager.decommissioning-nodes-watcher.poll-interval-secs": "20",
        "hadoop.security.group.mapping.ldap.search.group.hierarchy.levels": "0",
        "yarn.resourcemanager.fs.state-store.retry-interval-ms": "1000",
        "yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin.hdfs-hash-file": "/runc-root/image-tag-to-hash",
        "mapreduce.job.speculative.retry-after-speculate": "15000",
        "hadoop.registry.zk.connection.timeout.ms": "15000",
        "yarn.resourcemanager.delegation-token-renewer.thread-count": "*********(redacted)",
        "mapreduce.map.log.level": "INFO",
        "ha.failover-controller.active-standby-elector.zk.op.retries": "3",
        "mapreduce.output.fileoutputformat.compress.type": "RECORD",
        "yarn.resourcemanager.leveldb-state-store.path": "${hadoop.tmp.dir}/yarn/system/rmstore",
        "yarn.timeline-service.webapp.rest-csrf.custom-header": "X-XSRF-Header",
        "mapreduce.ifile.readahead.bytes": "4194304",
        "yarn.sharedcache.app-checker.class": "org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker",
        "yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users": "true",
        "yarn.nodemanager.resource.detect-hardware-capabilities": "false",
        "mapreduce.cluster.acls.enabled": "false",
        "mapreduce.job.speculative.retry-after-no-speculate": "1000",
        "fs.viewfs.overload.scheme.target.abfs.impl": "org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem",
        "file.stream-buffer-size": "4096",
        "yarn.resourcemanager.application-timeouts.monitor.interval-ms": "3000",
        "mapreduce.map.output.compress.codec": "org.apache.hadoop.io.compress.DefaultCodec",
        "mapreduce.map.speculative": "true",
        "yarn.nodemanager.linux-container-executor.cgroups.mount": "false",
        "mapreduce.job.reduce.slowstart.completedmaps": "0.05",
        "yarn.timeline-service.client.internal-timers-ttl-secs": "420",
        "fs.s3a.select.output.csv.quote.character": "\"",
        "hadoop.http.logs.enabled": "true",
        "yarn.nodemanager.logaggregation.threadpool-size-max": "100",
        "fs.AbstractFileSystem.hdfs.impl": "org.apache.hadoop.fs.Hdfs",
        "yarn.nodemanager.disk-health-checker.enable": "true",
        "fs.s3a.select.output.csv.quote.fields": "always",
        "yarn.resourcemanager.delegation-token-renewer.thread-retry-max-attempts": "*********(redacted)",
        "yarn.app.mapreduce.am.container.log.backups": "0",
        "yarn.app.mapreduce.am.log.level": "INFO",
        "yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin": "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.ImageTagToManifestPlugin",
        "io.bytes.per.checksum": "512",
        "yarn.timeline-service.http-authentication.type": "simple",
        "hadoop.security.group.mapping.ldap.search.attr.group.name": "cn",
        "yarn.nodemanager.resource-plugins.fpga.allowed-fpga-devices": "auto",
        "fs.s3a.block.size": "32M",
        "yarn.sharedcache.client-server.address": "0.0.0.0:8045",
        "yarn.resourcemanager.hostname": "0.0.0.0",
        "yarn.resourcemanager.delegation.key.update-interval": "86400000",
        "mapreduce.reduce.shuffle.fetch.retry.enabled": "${yarn.nodemanager.recovery.enabled}",
        "mapreduce.map.memory.mb": "-1",
        "mapreduce.task.skip.start.attempts": "2",
        "ipc.client.tcpnodelay": "true",
        "ipc.client.rpc-timeout.ms": "0",
        "yarn.nodemanager.webapp.rest-csrf.methods-to-ignore": "GET,OPTIONS,HEAD",
        "yarn.router.interceptor.user.threadpool-size": "5",
        "fs.AbstractFileSystem.har.impl": "org.apache.hadoop.fs.HarFs",
        "mapreduce.job.split.metainfo.maxsize": "10000000",
        "yarn.am.liveness-monitor.expiry-interval-ms": "600000",
        "yarn.nodemanager.runtime.linux.runc.hdfs-manifest-to-resources-plugin.stat-cache-timeout-interval-secs": "360",
        "fs.s3a.socket.recv.buffer": "8192",
        "rpc.metrics.timeunit": "MILLISECONDS",
        "yarn.scheduler.configuration.fs.path": "file://${hadoop.tmp.dir}/yarn/system/schedconf",
        "mapreduce.reduce.memory.mb": "-1",
        "ipc.client.low-latency": "false",
        "mapreduce.input.lineinputformat.linespermap": "1",
        "ipc.client.connect.max.retries.on.timeouts": "45",
        "yarn.timeline-service.leveldb-timeline-store.read-cache-size": "104857600",
        "yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs": "*********(redacted)",
        "yarn.timeline-service.entity-group-fs-store.app-cache-size": "10",
        "yarn.resourcemanager.resource-tracker.address": "${yarn.resourcemanager.hostname}:8031",
        "yarn.nodemanager.node-labels.provider.fetch-timeout-ms": "1200000",
        "mapreduce.job.heap.memory-mb.ratio": "0.8",
        "yarn.resourcemanager.leveldb-state-store.compaction-interval-secs": "3600",
        "yarn.resourcemanager.webapp.rest-csrf.custom-header": "X-XSRF-Header",
        "yarn.nodemanager.pluggable-device-framework.enabled": "false",
        "mapreduce.client.output.filter": "FAILED",
        "hadoop.http.filter.initializers": "org.apache.hadoop.http.lib.StaticUserWebFilter",
        "mapreduce.fileoutputcommitter.task.cleanup.enabled": "false",
        "ipc.[port_number].weighted-cost.lockfree": "1",
        "yarn.nodemanager.opportunistic-containers-max-queue-length": "0",
        "yarn.resourcemanager.state-store.max-completed-applications": "${yarn.resourcemanager.max-completed-applications}",
        "mapreduce.job.speculative.minimum-allowed-tasks": "10",
        "fs.s3a.aws.credentials.provider": "\n    org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider,\n    org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider,\n    com.amazonaws.auth.EnvironmentVariableCredentialsProvider,\n    org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider\n  ",
        "yarn.log-aggregation.retain-seconds": "-1",
        "yarn.timeline-service.hostname": "0.0.0.0",
        "file.replication": "1",
        "yarn.nodemanager.container-metrics.unregister-delay-ms": "10000",
        "yarn.nodemanager.container-metrics.period-ms": "-1",
        "yarn.nodemanager.log.retain-seconds": "10800",
        "yarn.timeline-service.entity-group-fs-store.cleaner-interval-seconds": "3600",
        "ipc.[port_number].callqueue.impl": "java.util.concurrent.LinkedBlockingQueue",
        "yarn.resourcemanager.keytab": "/etc/krb5.keytab",
        "hadoop.security.group.mapping.providers.combined": "true",
        "mapreduce.reduce.merge.inmem.threshold": "1000",
        "yarn.timeline-service.recovery.enabled": "false",
        "fs.azure.saskey.usecontainersaskeyforallaccess": "true",
        "yarn.sharedcache.nm.uploader.thread-count": "20",
        "yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs": "3600",
        "mapreduce.shuffle.ssl.enabled": "false",
        "yarn.timeline-service.hbase.coprocessor.app-final-value-retention-milliseconds": "259200000",
        "yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb": "0",
        "mapreduce.jobhistory.max-age-ms": "604800000",
        "hadoop.http.cross-origin.allowed-methods": "GET,POST,HEAD",
        "yarn.resourcemanager.opportunistic-container-allocation.enabled": "false",
        "mapreduce.jobhistory.webapp.address": "0.0.0.0:19888",
        "hadoop.system.tags": "YARN,HDFS,NAMENODE,DATANODE,REQUIRED,SECURITY,KERBEROS,PERFORMANCE,CLIENT\n      ,SERVER,DEBUG,DEPRECATED,COMMON,OPTIONAL",
        "yarn.log-aggregation.file-controller.TFile.class": "org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController",
        "yarn.client.nodemanager-connect.max-wait-ms": "180000",
        "yarn.resourcemanager.webapp.address": "${yarn.resourcemanager.hostname}:8088",
        "mapreduce.jobhistory.recovery.enable": "false",
        "mapreduce.reduce.shuffle.parallelcopies": "5",
        "yarn.app.mapreduce.client.max-retries": "3",
        "hadoop.security.authentication": "simple",
        "fs.s3a.select.input.csv.comment.marker": "#",
        "mapreduce.job.queuename": "default",
        "mapreduce.job.encrypted-intermediate-data-key-size-bits": "128",
        "yarn.nodemanager.webapp.xfs-filter.xframe-options": "SAMEORIGIN",
        "fs.AbstractFileSystem.webhdfs.impl": "org.apache.hadoop.fs.WebHdfs",
        "fs.trash.interval": "0",
        "mapreduce.task.profile.reduce.params": "${mapreduce.task.profile.params}",
        "yarn.app.mapreduce.am.resource.mb": "1536",
        "mapreduce.input.fileinputformat.list-status.num-threads": "1",
        "yarn.nodemanager.container-executor.class": "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor",
        "io.mapfile.bloom.size": "1048576",
        "yarn.timeline-service.ttl-ms": "604800000",
        "yarn.resourcemanager.nm-container-queuing.min-queue-length": "5",
        "yarn.nodemanager.resource.cpu-vcores": "-1",
        "mapreduce.job.reduces": "1",
        "fs.s3a.multipart.size": "64M",
        "yarn.scheduler.minimum-allocation-vcores": "1",
        "mapreduce.job.speculative.speculative-cap-total-tasks": "0.01",
        "hadoop.ssl.client.conf": "ssl-client.xml",
        "fs.s3a.metadatastore.authoritative": "false",
        "ipc.[port_number].weighted-cost.response": "1",
        "ha.health-monitor.sleep-after-disconnect.ms": "1000",
        "yarn.app.mapreduce.shuffle.log.limit.kb": "0",
        "hadoop.security.group.mapping": "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback",
        "yarn.client.application-client-protocol.poll-timeout-ms": "-1",
        "yarn.resourcemanager.application.max-tag.length": "100",
        "hadoop.http.staticuser.user": "dr.who",
        "yarn.nodemanager.linux-container-executor.resources-handler.class": "org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler",
        "mapreduce.reduce.shuffle.memory.limit.percent": "0.25",
        "yarn.resourcemanager.reservation-system.enable": "false",
        "mapreduce.map.output.compress": "false",
        "ha.zookeeper.acl": "world:anyone:rwcda",
        "ipc.server.max.connections": "0",
        "yarn.nodemanager.runtime.linux.docker.default-container-network": "host",
        "yarn.router.webapp.address": "0.0.0.0:8089",
        "yarn.scheduler.maximum-allocation-mb": "8192",
        "yarn.nodemanager.resource-plugins.gpu.docker-plugin.nvidia-docker-v1.endpoint": "http://localhost:3476/v1.0/docker/cli",
        "yarn.app.mapreduce.am.container.log.limit.kb": "0",
        "mapreduce.jobhistory.jhist.format": "binary",
        "mapreduce.task.stuck.timeout-ms": "600000",
        "yarn.resourcemanager.ha.enabled": "false",
        "dfs.client.ignore.namenode.default.kms.uri": "false",
        "mapreduce.task.exit.timeout.check-interval-ms": "20000",
        "mapreduce.jobhistory.intermediate-user-done-dir.permissions": "770",
        "mapreduce.task.exit.timeout": "60000",
        "yarn.resourcemanager.scheduler.monitor.policies": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy",
        "yarn.sharedcache.cleaner.period-mins": "1440",
        "ipc.client.connect.retry.interval": "1000",
        "yarn.timeline-service.http-cross-origin.enabled": "false",
        "fs.wasbs.impl": "org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure",
        "yarn.resourcemanager.nodemanagers.heartbeat-interval-max-ms": "1000",
        "yarn.federation.subcluster-resolver.class": "org.apache.hadoop.yarn.server.federation.resolver.DefaultSubClusterResolverImpl",
        "yarn.resourcemanager.zk-state-store.parent-path": "/rmstore",
        "fs.s3a.select.input.csv.field.delimiter": ",",
        "mapreduce.jobhistory.cleaner.enable": "true",
        "yarn.timeline-service.client.fd-flush-interval-secs": "10",
        "hadoop.security.kms.client.encrypted.key.cache.expiry": "43200000",
        "yarn.resourcemanager.nm-container-queuing.sorting-nodes-interval-ms": "1000",
        "fs.s3a.committer.staging.tmp.path": "tmp/staging",
        "yarn.nodemanager.sleep-delay-before-sigkill.ms": "250",
        "yarn.nodemanager.resource.count-logical-processors-as-cores": "false",
        "hadoop.registry.zk.root": "/registry",
        "mapreduce.client.libjars.wildcard": "true",
        "fs.s3a.committer.staging.unique-filenames": "true",
        "yarn.nodemanager.node-attributes.provider.fetch-timeout-ms": "1200000",
        "yarn.client.nodemanager-client-async.thread-pool-max-size": "500",
        "mapreduce.map.maxattempts": "4",
        "yarn.resourcemanager.nm-container-queuing.min-queue-wait-time-ms": "10",
        "mapreduce.job.end-notification.retry.attempts": "0",
        "adl.feature.ownerandgroup.enableupn": "false",
        "yarn.resourcemanager.zk-max-znode-size.bytes": "1048576",
        "mapreduce.job.reduce.shuffle.consumer.plugin.class": "org.apache.hadoop.mapreduce.task.reduce.Shuffle",
        "yarn.resourcemanager.delayed.delegation-token.removal-interval-ms": "*********(redacted)",
        "yarn.nodemanager.localizer.cache.target-size-mb": "10240",
        "fs.s3a.committer.staging.conflict-mode": "append",
        "fs.s3a.list.version": "2",
        "ftp.client-write-packet-size": "65536",
        "yarn.nodemanager.container-log-monitor.enable": "false",
        "hadoop.security.key.default.cipher": "AES/CTR/NoPadding",
        "mapreduce.job.local-fs.single-disk-limit.check.interval-ms": "5000",
        "net.topology.node.switch.mapping.impl": "org.apache.hadoop.net.ScriptBasedMapping",
        "ipc.[port_number].decay-scheduler.period-ms": "5000",
        "yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin.cache-refresh-interval-secs": "60",
        "map.sort.class": "org.apache.hadoop.util.QuickSort",
        "fs.viewfs.rename.strategy": "SAME_MOUNTPOINT",
        "hadoop.security.kms.client.authentication.retry-count": "1",
        "yarn.nodemanager.runtime.linux.runc.privileged-containers.allowed": "false",
        "ipc.[port_number].weighted-cost.lockexclusive": "100",
        "fs.AbstractFileSystem.adl.impl": "org.apache.hadoop.fs.adl.Adl",
        "yarn.client.failover-retries": "0",
        "fs.s3a.multipart.purge.age": "86400",
        "yarn.nodemanager.amrmproxy.address": "0.0.0.0:8049",
        "ipc.server.listen.queue.size": "256",
        "fs.permissions.umask-mode": "022",
        "fs.s3a.assumed.role.credentials.provider": "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider",
        "yarn.nodemanager.vmem-check-enabled": "true",
        "yarn.nodemanager.health-checker.run-before-startup": "false",
        "mapreduce.job.max.map": "-1",
        "mapreduce.job.ubertask.maxreduces": "1",
        "mapreduce.shuffle.pathcache.max-weight": "10485760",
        "hadoop.security.kms.client.encrypted.key.cache.size": "500",
        "yarn.nodemanager.env-whitelist": "JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ",
        "yarn.registry.class": "org.apache.hadoop.registry.client.impl.FSRegistryOperationsService",
        "mapreduce.jobhistory.admin.acl": "*",
        "yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size": "10",
        "yarn.nodemanager.runtime.linux.runc.hdfs-manifest-to-resources-plugin.stat-cache-size": "500",
        "yarn.timeline-service.webapp.rest-csrf.enabled": "false",
        "yarn.nodemanager.disk-health-checker.min-free-space-per-disk-watermark-high-mb": "0",
        "yarn.nodemanager.numa-awareness.enabled": "false",
        "yarn.nodemanager.recovery.compaction-interval-secs": "3600",
        "yarn.app.mapreduce.client-am.ipc.max-retries": "3",
        "yarn.resourcemanager.system-metrics-publisher.timeline-server-v1.interval-seconds": "60",
        "yarn.federation.registry.base-dir": "yarnfederation/",
        "mapreduce.job.local-fs.single-disk-limit.bytes": "-1",
        "mapreduce.shuffle.pathcache.concurrency-level": "16",
        "hadoop.security.java.secure.random.algorithm": "SHA1PRNG",
        "ha.failover-controller.cli-check.rpc-timeout.ms": "20000",
        "mapreduce.jobhistory.jobname.limit": "50",
        "fs.s3a.select.input.compression": "none",
        "yarn.client.nodemanager-connect.retry-interval-ms": "10000",
        "ipc.[port_number].scheduler.priority.levels": "4",
        "yarn.timeline-service.state-store-class": "org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore",
        "yarn.sharedcache.nested-level": "3",
        "yarn.timeline-service.webapp.rest-csrf.methods-to-ignore": "GET,OPTIONS,HEAD",
        "fs.azure.user.agent.prefix": "unknown",
        "yarn.resourcemanager.zk-delegation-token-node.split-index": "*********(redacted)",
        "yarn.nodemanager.numa-awareness.read-topology": "false",
        "yarn.nodemanager.webapp.address": "${yarn.nodemanager.hostname}:8042",
        "rpc.metrics.quantile.enable": "false",
        "yarn.scheduler.queue-placement-rules": "user-group",
        "hadoop.http.authentication.kerberos.keytab": "${user.home}/hadoop.keytab",
        "yarn.resourcemanager.recovery.enabled": "false",
        "fs.s3a.select.input.csv.header": "none"
    },
    "System Properties": {
        "java.io.tmpdir": "/tmp",
        "line.separator": "\n",
        "path.separator": ":",
        "user.home": "/home/chenhao",
        "file.separator": "/",
        "user.timezone": "PRC",
        "sun.os.patch.level": "unknown",
        "java.vm.specification.vendor": "Oracle Corporation",
        "user.country": "US",
        "awt.toolkit": "sun.awt.X11.XToolkit",
        "os.name": "Linux",
        "sun.management.compiler": "HotSpot 64-Bit Tiered Compilers",
        "SPARK_SUBMIT": "true",
        "sun.cpu.endian": "little",
        "java.specification.version": "11",
        "java.vm.specification.name": "Java Virtual Machine Specification",
        "java.vendor": "Oracle Corporation",
        "java.vm.specification.version": "11",
        "sun.arch.data.model": "64",
        "sun.boot.library.path": "/home/chenhao/libs/jdk11/lib",
        "user.dir": "/home/chenhao/workspace",
        "java.library.path": "/usr/lib64:/lib64:/public/software/gcc/lib64:/public/software//mpi/openmpi/1.8.7/intel/lib/openmpi:/public/software//mpi/openmpi/1.8.7/intel/lib:/public/software/compiler/composer_xe_2015.0.090/compiler/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/mpirt/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/ipp/../compiler/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/ipp/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/compiler/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/mkl/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/tbb/lib/intel64/gcc4.4:/public/software/compiler/composer_xe_2015.0.090/compiler/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/mpirt/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/ipp/../compiler/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/ipp/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/compiler/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/mkl/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/tbb/lib/intel64/gcc4.4:/public/software/compiler/composer_xe_2015.0.090/compiler/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/mpirt/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/ipp/../compiler/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/ipp/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/compiler/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/mkl/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/tbb/lib/intel64/gcc4.4:/public/software/g09/bsd:/public/software/g09/local:/public/software/g09/extras:/public/software/g09:/opt/gridview//pbs//dispatcher//lib:/public/software/compiler/composer_xe_2015.0.090/compiler/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/mkl/lib/intel64::/usr/local/lib64:/usr/local/lib:/public/software/g09:/public/software/gv/lib:/public/software/MaterialsStudio8.0/lib:/public/software/MaterialsStudio8.0/lib/32:/public/software/gcc/mpc/lib:/opt/hadoop/hadoop-2.7.6-5nodes/lib/native:/usr/local/lib/libdisni/lib:/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib",
        "sun.cpu.isalist": "",
        "os.arch": "amd64",
        "java.vm.version": "11.0.16.1+1-LTS-1",
        "jetty.git.hash": "6b67c5719d1f4371b33655ff2d047d24e171e49a",
        "java.runtime.version": "11.0.16.1+1-LTS-1",
        "java.vm.info": "mixed mode",
        "java.runtime.name": "Java(TM) SE Runtime Environment",
        "java.version.date": "2022-08-18",
        "java.class.version": "55.0",
        "java.specification.name": "Java Platform API Specification",
        "file.encoding": "UTF-8",
        "java.specification.vendor": "Oracle Corporation",
        "sun.java.launcher": "SUN_STANDARD",
        "java.vm.compressedOopsMode": "32-bit",
        "os.version": "2.6.32-220.el6.x86_64",
        "sun.jnu.encoding": "UTF-8",
        "user.language": "en",
        "java.vendor.version": "18.9",
        "java.vendor.url": "https://openjdk.java.net/",
        "java.awt.printerjob": "sun.print.PSPrinterJob",
        "java.awt.graphicsenv": "sun.awt.X11GraphicsEnvironment",
        "java.vm.vendor": "Oracle Corporation",
        "jdk.debug": "release",
        "java.vendor.url.bug": "https://bugreport.java.com/bugreport/",
        "user.name": "chenhao",
        "java.vm.name": "Java HotSpot(TM) 64-Bit Server VM",
        "sun.java.command": "org.apache.spark.deploy.SparkSubmit --master local[2] SparkTemplate-1.3-jar-with-dependencies.jar",
        "java.home": "/home/chenhao/libs/jdk11",
        "java.version": "11.0.16.1",
        "sun.io.unicode.encoding": "UnicodeLittle"
    },
    "Classpath Entries": {
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/javolution-5.5.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-serde-2.3.9.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/datanucleus-core-4.1.17.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-storageclass-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spire-util_2.13-0.17.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-beeline-2.3.9.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/scala-reflect-2.13.8.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/javassist-3.25.0-GA.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-transport-native-kqueue-4.1.74.Final-osx-x86_64.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-llap-common-2.3.9.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-yarn_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/libthrift-0.12.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/cats-kernel_2.13-2.1.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spire_2.13-0.17.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-service-rpc-3.1.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/parquet-encoding-1.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/arpack_combined_all-0.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jackson-core-2.13.4.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/json4s-scalap_2.13-3.7.0-M11.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-autoscaling-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jdo-api-3.0.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/minlog-1.3.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/metrics-graphite-4.2.7.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-discovery-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-codec-1.15.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/httpcore-4.4.14.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/json-1.8.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/protobuf-java-2.5.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/guava-14.0.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spire-macros_2.13-0.17.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/HikariCP-2.5.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/rocksdbjni-6.20.3.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/chill-java-0.10.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-resolver-4.1.74.Final.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jakarta.servlet-api-4.0.3.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hadoop-client-runtime-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jna-5.9.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-logging-1.1.3.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/chill_2.13-0.10.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-math3-3.6.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-tags_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-cli-1.5.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/curator-recipes-2.13.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jersey-client-2.36.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/parquet-format-structures-1.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/json4s-ast_2.13-3.7.0-M11.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/osgi-resource-locator-1.0.3.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/compress-lzf-1.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/curator-framework-2.13.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/conf/": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/datanucleus-rdbms-4.1.19.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/oro-2.0.8.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/xz-1.9.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-cli-2.3.9.jar": "System Classpath",
        "spark://node86:50612/jars/SparkTemplate-1.3-jar-with-dependencies.jar": "Added By User",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-shims-0.23-2.3.9.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-jdbc-2.3.9.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/leveldbjni-all-1.8.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/scala-parser-combinators_2.13-1.1.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/zstd-jni-1.5.2-1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-metastore-2.3.9.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-graphx_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/threeten-extra-1.5.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/JTransforms-3.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/log4j-core-2.17.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/flatbuffers-java-1.12.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-tags_2.13-3.3.2-tests.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/istack-commons-runtime-3.0.8.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jackson-mapper-asl-1.9.13.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jersey-common-2.36.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-network-shuffle_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-coordination-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/annotations-17.0.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/avro-ipc-1.11.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/parquet-jackson-1.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/univocity-parsers-2.9.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/log4j-api-2.17.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-transport-native-epoll-4.1.74.Final-linux-x86_64.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jakarta.validation-api-2.0.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jakarta.xml.bind-api-2.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/slf4j-api-1.7.32.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jakarta.annotation-api-1.3.5.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/json4s-jackson_2.13-3.7.0-M11.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jpam-1.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/lz4-java-1.8.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-collections4-4.4.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/gson-2.2.4.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-buffer-4.1.74.Final.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-tcnative-classes-2.0.48.Final.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-extensions-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/orc-core-1.7.8.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-transport-4.1.74.Final.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/scala-collection-compat_2.13-2.1.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-io-2.11.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/metrics-core-4.2.7.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/zjsonpatch-0.3.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/paranamer-2.8.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/lapack-2.2.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/tink-1.6.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hadoop-client-api-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hk2-locator-2.6.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/okio-1.14.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/bonecp-0.8.0.RELEASE.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/snappy-java-1.1.8.4.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jersey-hk2-2.36.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spire-platform_2.13-0.17.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-mllib-local_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/JLargeArrays-1.5.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-unsafe_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/core-1.1.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jackson-module-scala_2.13-2.13.4.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-transport-native-epoll-4.1.74.Final-linux-aarch_64.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/breeze-macros_2.13-1.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/httpclient-4.5.13.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jta-1.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/derby-10.14.2.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-sql_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jakarta.inject-2.6.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/scala-compiler-2.13.8.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-vector-code-gen-2.3.9.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-metrics-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/curator-client-2.13.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-shims-common-2.3.9.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/zookeeper-3.6.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/antlr4-runtime-4.8.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jersey-container-servlet-2.36.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/algebra_2.13-2.0.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-compiler-3.0.16.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/datanucleus-api-jdo-4.2.4.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/stream-2.9.6.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/janino-3.0.16.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/shapeless_2.13-2.3.7.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-transport-classes-epoll-4.1.74.Final.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/zookeeper-jute-3.6.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/stax-api-1.0.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/pickle-1.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-dbcp-1.4.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jline-3.21.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/arrow-vector-7.0.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-pool-1.5.4.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/scala-library-2.13.8.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jackson-dataformat-yaml-2.13.4.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-repl_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/metrics-json-4.2.7.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/shims-0.9.25.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/json4s-core_2.13-3.7.0-M11.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-shims-scheduler-2.3.9.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-common-2.3.9.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-launcher_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/opencsv-2.3.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-lang-2.6.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/aopalliance-repackaged-2.6.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-text-1.10.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-shims-2.3.9.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hadoop-yarn-server-web-proxy-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/transaction-api-1.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/okhttp-3.12.12.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-mllib_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hk2-utils-2.6.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jcl-over-slf4j-1.7.32.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-node-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/arrow-memory-core-7.0.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jackson-annotations-2.13.4.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/ivy-2.5.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jakarta.ws.rs-api-2.1.6.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hadoop-shaded-guava-1.1.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/arrow-format-7.0.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/audience-annotations-0.5.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-networking-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jline-2.14.6.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-events-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-exec-2.3.9-core.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/avro-mapred-1.11.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/metrics-jvm-4.2.7.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-kubernetes_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-transport-native-kqueue-4.1.74.Final-osx-aarch_64.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/metrics-jmx-4.2.7.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/javax.jdo-3.2.0-m3.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-collections-3.2.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-hive_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/arrow-memory-netty-7.0.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jul-to-slf4j-1.7.32.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jaxb-runtime-2.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-all-4.1.74.Final.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-lang3-3.12.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/avro-1.11.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-sketch_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jsr305-3.0.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-client-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/breeze_2.13-1.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-flowcontrol-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-network-common_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/scala-parallel-collections_2.13-1.0.3.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/objenesis-3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-admissionregistration-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-apps-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-streaming_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/logging-interceptor-3.12.12.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/orc-mapreduce-1.7.8.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-transport-classes-kqueue-4.1.74.Final.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/antlr-runtime-3.5.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-kvstore_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-common-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-core_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/scala-xml_2.13-1.2.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-common-4.1.74.Final.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-policy-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-handler-4.1.74.Final.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-apiextensions-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hk2-api-2.6.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/velocity-1.5.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-storage-api-2.7.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/parquet-common-1.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-catalyst_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/activation-1.1.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/xbean-asm9-shaded-4.20.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-rbac-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jersey-container-servlet-core-2.36.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kryo-shaded-4.0.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jersey-server-2.36.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/blas-2.2.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/libfb303-0.9.3.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/joda-time-2.10.13.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-scheduling-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jackson-core-asl-1.9.13.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/generex-1.0.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/RoaringBitmap-0.9.25.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jodd-core-3.5.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/aircompressor-0.21.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-batch-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/parquet-hadoop-1.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-hive-thriftserver_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/log4j-slf4j-impl-2.17.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/py4j-0.10.9.5.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/ST4-4.0.4.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-compress-1.21.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/automaton-1.11-8.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-core-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-crypto-1.1.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/orc-shims-1.7.8.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-codec-4.1.74.Final.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-transport-native-unix-common-4.1.74.Final.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/super-csv-2.2.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/log4j-1.2-api-2.17.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/parquet-column-1.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-mesos_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jackson-databind-2.13.4.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-certificates-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jackson-datatype-jsr310-2.13.4.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/arpack-2.2.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/snakeyaml-1.31.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/mesos-1.4.3-shaded-protobuf.jar": "System Classpath"
    }
}
{
    "Event": "SparkListenerApplicationStart",
    "App Name": "WordCount",
    "App ID": "local-1680515062694",
    "Timestamp": 1680515061685,
    "User": "chenhao"
}
{
    "Event": "SparkListenerJobStart",
    "Job ID": 0,
    "Submission Time": 1680515066918,
    "Stage Infos": [
        {
            "Stage ID": 0,
            "Stage Attempt ID": 0,
            "Stage Name": "map at main.scala:46",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 2,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"3\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515066000\",\"name\":\"foreachRDD @ 17:44:26\"}}",
                    "Callsite": "map at main.scala:46",
                    "Parent IDs": [
                        1
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 0,
                    "Name": "KafkaRDD",
                    "Scope": "{\"id\":\"0_1680515066000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:26\"}",
                    "Callsite": "createDirectStream at main.scala:31",
                    "Parent IDs": [],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 1,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"2\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515066000\",\"name\":\"foreachRDD @ 17:44:26\"}}",
                    "Callsite": "flatMap at main.scala:41",
                    "Parent IDs": [
                        0
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [],
            "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
            "Accumulables": [],
            "Resource Profile Id": 0
        },
        {
            "Stage ID": 1,
            "Stage Attempt ID": 0,
            "Stage Name": "runJob at SparkHadoopWriter.scala:83",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 4,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"5\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515066000\",\"name\":\"foreachRDD @ 17:44:26\"}}",
                    "Callsite": "saveAsTextFile at main.scala:48",
                    "Parent IDs": [
                        3
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 3,
                    "Name": "ShuffledRDD",
                    "Scope": "{\"id\":\"4\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515066000\",\"name\":\"foreachRDD @ 17:44:26\"}}",
                    "Callsite": "reduceByKey at main.scala:46",
                    "Parent IDs": [
                        2
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [
                0
            ],
            "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
            "Accumulables": [],
            "Resource Profile Id": 0
        }
    ],
    "Stage IDs": [
        0,
        1
    ],
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"5\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515066000\",\"name\":\"foreachRDD @ 17:44:26\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515066000\">[output operation 0, batch time 17:44:26]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515066000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 0,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 2,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"3\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515066000\",\"name\":\"foreachRDD @ 17:44:26\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    1
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 0,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515066000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:26\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 1,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"2\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515066000\",\"name\":\"foreachRDD @ 17:44:26\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    0
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515066953,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"5\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515066000\",\"name\":\"foreachRDD @ 17:44:26\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515066000\">[output operation 0, batch time 17:44:26]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515066000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 0,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 0,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515067172,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 0,
    "Stage Attempt ID": 0,
    "Task Type": "ShuffleMapTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 0,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515067172,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515069671,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 0,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 58,
                "Value": 58,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 1,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 56292092,
                "Value": 56292092,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 2,
                "Name": "internal.metrics.executorRunTime",
                "Update": 2363,
                "Value": 2363,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 3,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 2307975492,
                "Value": 2307975492,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 4,
                "Name": "internal.metrics.resultSize",
                "Update": 1338,
                "Value": 1338,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 5,
                "Name": "internal.metrics.jvmGCTime",
                "Update": 30,
                "Value": 30,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 6,
                "Name": "internal.metrics.resultSerializationTime",
                "Update": 1,
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 7,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 8,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 9,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 29266180,
                "Value": 29266180,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 18,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Update": 2590363,
                "Value": 2590363,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 19,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Update": 196218,
                "Value": 196218,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 20,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Update": 6267582,
                "Value": 6267582,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 58,
        "Executor Deserialize CPU Time": 56292092,
        "Executor Run Time": 2363,
        "Executor CPU Time": 2307975492,
        "Peak Execution Memory": 29266180,
        "Result Size": 1338,
        "JVM GC Time": 30,
        "Result Serialization Time": 1,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 0,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 0,
            "Total Records Read": 0
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 2590363,
            "Shuffle Write Time": 6267582,
            "Shuffle Records Written": 196218
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 0,
            "Records Written": 0
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 0,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 2,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"3\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515066000\",\"name\":\"foreachRDD @ 17:44:26\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    1
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 0,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515066000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:26\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 1,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"2\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515066000\",\"name\":\"foreachRDD @ 17:44:26\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    0
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515066953,
        "Completion Time": 1680515069690,
        "Accumulables": [
            {
                "ID": 0,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 58,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 1,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 56292092,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 2,
                "Name": "internal.metrics.executorRunTime",
                "Value": 2363,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 3,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 2307975492,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 4,
                "Name": "internal.metrics.resultSize",
                "Value": 1338,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 5,
                "Name": "internal.metrics.jvmGCTime",
                "Value": 30,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 6,
                "Name": "internal.metrics.resultSerializationTime",
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 7,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 8,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 9,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 29266180,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 18,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Value": 2590363,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 19,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Value": 196218,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 20,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Value": 6267582,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 1,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 4,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"5\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515066000\",\"name\":\"foreachRDD @ 17:44:26\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    3
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 3,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"4\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515066000\",\"name\":\"foreachRDD @ 17:44:26\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    2
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            0
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515069701,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"5\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515066000\",\"name\":\"foreachRDD @ 17:44:26\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515066000\">[output operation 0, batch time 17:44:26]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515066000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 1,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 1,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515069742,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 1,
    "Stage Attempt ID": 0,
    "Task Type": "ResultTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 1,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515069742,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515070514,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 25,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 52,
                "Value": 52,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 26,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 50193915,
                "Value": 50193915,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 27,
                "Name": "internal.metrics.executorRunTime",
                "Update": 701,
                "Value": 701,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 28,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 631075794,
                "Value": 631075794,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 29,
                "Name": "internal.metrics.resultSize",
                "Update": 1866,
                "Value": 1866,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 30,
                "Name": "internal.metrics.jvmGCTime",
                "Update": 32,
                "Value": 32,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 31,
                "Name": "internal.metrics.resultSerializationTime",
                "Update": 3,
                "Value": 3,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 32,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 33,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 34,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 17414218,
                "Value": 17414218,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 36,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 37,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Update": 1,
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 38,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 39,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 40,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Update": 2590363,
                "Value": 2590363,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 41,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 42,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Update": 196218,
                "Value": 196218,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 48,
                "Name": "internal.metrics.output.bytesWritten",
                "Update": 4571431,
                "Value": 4571431,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 49,
                "Name": "internal.metrics.output.recordsWritten",
                "Update": 196218,
                "Value": 196218,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 52,
        "Executor Deserialize CPU Time": 50193915,
        "Executor Run Time": 701,
        "Executor CPU Time": 631075794,
        "Peak Execution Memory": 17414218,
        "Result Size": 1866,
        "JVM GC Time": 32,
        "Result Serialization Time": 3,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 1,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 2590363,
            "Total Records Read": 196218
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 0,
            "Shuffle Write Time": 0,
            "Shuffle Records Written": 0
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 4571431,
            "Records Written": 196218
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 1,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 4,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"5\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515066000\",\"name\":\"foreachRDD @ 17:44:26\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    3
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 3,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"4\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515066000\",\"name\":\"foreachRDD @ 17:44:26\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    2
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            0
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515069701,
        "Completion Time": 1680515070518,
        "Accumulables": [
            {
                "ID": 25,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 52,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 26,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 50193915,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 27,
                "Name": "internal.metrics.executorRunTime",
                "Value": 701,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 28,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 631075794,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 29,
                "Name": "internal.metrics.resultSize",
                "Value": 1866,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 30,
                "Name": "internal.metrics.jvmGCTime",
                "Value": 32,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 31,
                "Name": "internal.metrics.resultSerializationTime",
                "Value": 3,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 32,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 33,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 34,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 17414218,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 36,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 37,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 38,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 39,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 40,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Value": 2590363,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 41,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 42,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Value": 196218,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 48,
                "Name": "internal.metrics.output.bytesWritten",
                "Value": 4571431,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 49,
                "Name": "internal.metrics.output.recordsWritten",
                "Value": 196218,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerJobEnd",
    "Job ID": 0,
    "Completion Time": 1680515070528,
    "Job Result": {
        "Result": "JobSucceeded"
    }
}
{
    "Event": "SparkListenerJobStart",
    "Job ID": 1,
    "Submission Time": 1680515070643,
    "Stage Infos": [
        {
            "Stage ID": 2,
            "Stage Attempt ID": 0,
            "Stage Name": "map at main.scala:46",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 10,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"12\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515067000\",\"name\":\"foreachRDD @ 17:44:27\"}}",
                    "Callsite": "map at main.scala:46",
                    "Parent IDs": [
                        9
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 9,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"11\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515067000\",\"name\":\"foreachRDD @ 17:44:27\"}}",
                    "Callsite": "flatMap at main.scala:41",
                    "Parent IDs": [
                        5
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 5,
                    "Name": "KafkaRDD",
                    "Scope": "{\"id\":\"0_1680515067000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:27\"}",
                    "Callsite": "createDirectStream at main.scala:31",
                    "Parent IDs": [],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [],
            "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
            "Accumulables": [],
            "Resource Profile Id": 0
        },
        {
            "Stage ID": 3,
            "Stage Attempt ID": 0,
            "Stage Name": "runJob at SparkHadoopWriter.scala:83",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 12,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"14\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515067000\",\"name\":\"foreachRDD @ 17:44:27\"}}",
                    "Callsite": "saveAsTextFile at main.scala:48",
                    "Parent IDs": [
                        11
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 11,
                    "Name": "ShuffledRDD",
                    "Scope": "{\"id\":\"13\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515067000\",\"name\":\"foreachRDD @ 17:44:27\"}}",
                    "Callsite": "reduceByKey at main.scala:46",
                    "Parent IDs": [
                        10
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [
                2
            ],
            "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
            "Accumulables": [],
            "Resource Profile Id": 0
        }
    ],
    "Stage IDs": [
        2,
        3
    ],
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"14\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515067000\",\"name\":\"foreachRDD @ 17:44:27\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515067000\">[output operation 0, batch time 17:44:27]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515067000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 2,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 10,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"12\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515067000\",\"name\":\"foreachRDD @ 17:44:27\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    9
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 9,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"11\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515067000\",\"name\":\"foreachRDD @ 17:44:27\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    5
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 5,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515067000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:27\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515070646,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"14\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515067000\",\"name\":\"foreachRDD @ 17:44:27\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515067000\">[output operation 0, batch time 17:44:27]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515067000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 2,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 2,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515070658,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 2,
    "Stage Attempt ID": 0,
    "Task Type": "ShuffleMapTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 2,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515070658,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515072566,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 50,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 6,
                "Value": 6,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 51,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 6847188,
                "Value": 6847188,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 52,
                "Name": "internal.metrics.executorRunTime",
                "Update": 1893,
                "Value": 1893,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 53,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 1770068773,
                "Value": 1770068773,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 54,
                "Name": "internal.metrics.resultSize",
                "Update": 1295,
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 55,
                "Name": "internal.metrics.jvmGCTime",
                "Update": 37,
                "Value": 37,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 57,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 58,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 59,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 25740355,
                "Value": 25740355,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 68,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Update": 2678095,
                "Value": 2678095,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 69,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Update": 197976,
                "Value": 197976,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 70,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Update": 8886547,
                "Value": 8886547,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 6,
        "Executor Deserialize CPU Time": 6847188,
        "Executor Run Time": 1893,
        "Executor CPU Time": 1770068773,
        "Peak Execution Memory": 25740355,
        "Result Size": 1295,
        "JVM GC Time": 37,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 0,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 0,
            "Total Records Read": 0
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 2678095,
            "Shuffle Write Time": 8886547,
            "Shuffle Records Written": 197976
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 0,
            "Records Written": 0
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 2,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 10,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"12\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515067000\",\"name\":\"foreachRDD @ 17:44:27\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    9
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 9,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"11\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515067000\",\"name\":\"foreachRDD @ 17:44:27\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    5
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 5,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515067000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:27\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515070646,
        "Completion Time": 1680515072569,
        "Accumulables": [
            {
                "ID": 50,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 6,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 51,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 6847188,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 52,
                "Name": "internal.metrics.executorRunTime",
                "Value": 1893,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 53,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 1770068773,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 54,
                "Name": "internal.metrics.resultSize",
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 55,
                "Name": "internal.metrics.jvmGCTime",
                "Value": 37,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 57,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 58,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 59,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 25740355,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 68,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Value": 2678095,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 69,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Value": 197976,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 70,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Value": 8886547,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 3,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 12,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"14\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515067000\",\"name\":\"foreachRDD @ 17:44:27\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    11
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 11,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"13\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515067000\",\"name\":\"foreachRDD @ 17:44:27\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    10
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            2
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515072573,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"14\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515067000\",\"name\":\"foreachRDD @ 17:44:27\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515067000\">[output operation 0, batch time 17:44:27]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515067000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 3,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 3,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515072600,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 3,
    "Stage Attempt ID": 0,
    "Task Type": "ResultTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 3,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515072600,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515073009,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 75,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 11,
                "Value": 11,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 76,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 11397158,
                "Value": 11397158,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 77,
                "Name": "internal.metrics.executorRunTime",
                "Update": 389,
                "Value": 389,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 78,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 355966723,
                "Value": 355966723,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 79,
                "Name": "internal.metrics.resultSize",
                "Update": 1780,
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 82,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 83,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 84,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 17364211,
                "Value": 17364211,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 86,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 87,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Update": 1,
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 88,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 89,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 90,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Update": 2678095,
                "Value": 2678095,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 91,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 92,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Update": 197976,
                "Value": 197976,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 98,
                "Name": "internal.metrics.output.bytesWritten",
                "Update": 4970068,
                "Value": 4970068,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 99,
                "Name": "internal.metrics.output.recordsWritten",
                "Update": 197976,
                "Value": 197976,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 11,
        "Executor Deserialize CPU Time": 11397158,
        "Executor Run Time": 389,
        "Executor CPU Time": 355966723,
        "Peak Execution Memory": 17364211,
        "Result Size": 1780,
        "JVM GC Time": 0,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 1,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 2678095,
            "Total Records Read": 197976
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 0,
            "Shuffle Write Time": 0,
            "Shuffle Records Written": 0
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 4970068,
            "Records Written": 197976
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 3,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 12,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"14\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515067000\",\"name\":\"foreachRDD @ 17:44:27\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    11
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 11,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"13\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515067000\",\"name\":\"foreachRDD @ 17:44:27\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    10
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            2
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515072573,
        "Completion Time": 1680515073012,
        "Accumulables": [
            {
                "ID": 75,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 11,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 76,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 11397158,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 77,
                "Name": "internal.metrics.executorRunTime",
                "Value": 389,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 78,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 355966723,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 79,
                "Name": "internal.metrics.resultSize",
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 82,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 83,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 84,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 17364211,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 86,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 87,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 88,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 89,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 90,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Value": 2678095,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 91,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 92,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Value": 197976,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 98,
                "Name": "internal.metrics.output.bytesWritten",
                "Value": 4970068,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 99,
                "Name": "internal.metrics.output.recordsWritten",
                "Value": 197976,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerJobEnd",
    "Job ID": 1,
    "Completion Time": 1680515073013,
    "Job Result": {
        "Result": "JobSucceeded"
    }
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 0
}
{
    "Event": "SparkListenerJobStart",
    "Job ID": 2,
    "Submission Time": 1680515073114,
    "Stage Infos": [
        {
            "Stage ID": 4,
            "Stage Attempt ID": 0,
            "Stage Name": "map at main.scala:46",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 17,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"21\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515068000\",\"name\":\"foreachRDD @ 17:44:28\"}}",
                    "Callsite": "map at main.scala:46",
                    "Parent IDs": [
                        16
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 6,
                    "Name": "KafkaRDD",
                    "Scope": "{\"id\":\"0_1680515068000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:28\"}",
                    "Callsite": "createDirectStream at main.scala:31",
                    "Parent IDs": [],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 16,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"20\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515068000\",\"name\":\"foreachRDD @ 17:44:28\"}}",
                    "Callsite": "flatMap at main.scala:41",
                    "Parent IDs": [
                        6
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [],
            "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
            "Accumulables": [],
            "Resource Profile Id": 0
        },
        {
            "Stage ID": 5,
            "Stage Attempt ID": 0,
            "Stage Name": "runJob at SparkHadoopWriter.scala:83",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 19,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"23\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515068000\",\"name\":\"foreachRDD @ 17:44:28\"}}",
                    "Callsite": "saveAsTextFile at main.scala:48",
                    "Parent IDs": [
                        18
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 18,
                    "Name": "ShuffledRDD",
                    "Scope": "{\"id\":\"22\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515068000\",\"name\":\"foreachRDD @ 17:44:28\"}}",
                    "Callsite": "reduceByKey at main.scala:46",
                    "Parent IDs": [
                        17
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [
                4
            ],
            "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
            "Accumulables": [],
            "Resource Profile Id": 0
        }
    ],
    "Stage IDs": [
        4,
        5
    ],
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"23\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515068000\",\"name\":\"foreachRDD @ 17:44:28\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515068000\">[output operation 0, batch time 17:44:28]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515068000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 4,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 17,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"21\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515068000\",\"name\":\"foreachRDD @ 17:44:28\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    16
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 6,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515068000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:28\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 16,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"20\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515068000\",\"name\":\"foreachRDD @ 17:44:28\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    6
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515073116,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"23\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515068000\",\"name\":\"foreachRDD @ 17:44:28\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515068000\">[output operation 0, batch time 17:44:28]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515068000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 4,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 4,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515073123,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 4,
    "Stage Attempt ID": 0,
    "Task Type": "ShuffleMapTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 4,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515073123,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515074699,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 100,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 2,
                "Value": 2,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 101,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 2875856,
                "Value": 2875856,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 102,
                "Name": "internal.metrics.executorRunTime",
                "Update": 1569,
                "Value": 1569,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 103,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 1406934285,
                "Value": 1406934285,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 104,
                "Name": "internal.metrics.resultSize",
                "Update": 1295,
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 105,
                "Name": "internal.metrics.jvmGCTime",
                "Update": 56,
                "Value": 56,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 107,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 108,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 109,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 24668617,
                "Value": 24668617,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 118,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Update": 2775696,
                "Value": 2775696,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 119,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Update": 212803,
                "Value": 212803,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 120,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Update": 3790747,
                "Value": 3790747,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 2,
        "Executor Deserialize CPU Time": 2875856,
        "Executor Run Time": 1569,
        "Executor CPU Time": 1406934285,
        "Peak Execution Memory": 24668617,
        "Result Size": 1295,
        "JVM GC Time": 56,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 0,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 0,
            "Total Records Read": 0
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 2775696,
            "Shuffle Write Time": 3790747,
            "Shuffle Records Written": 212803
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 0,
            "Records Written": 0
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 4,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 17,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"21\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515068000\",\"name\":\"foreachRDD @ 17:44:28\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    16
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 6,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515068000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:28\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 16,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"20\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515068000\",\"name\":\"foreachRDD @ 17:44:28\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    6
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515073116,
        "Completion Time": 1680515074702,
        "Accumulables": [
            {
                "ID": 100,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 2,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 101,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 2875856,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 102,
                "Name": "internal.metrics.executorRunTime",
                "Value": 1569,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 103,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 1406934285,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 104,
                "Name": "internal.metrics.resultSize",
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 105,
                "Name": "internal.metrics.jvmGCTime",
                "Value": 56,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 107,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 108,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 109,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 24668617,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 118,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Value": 2775696,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 119,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Value": 212803,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 120,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Value": 3790747,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 5,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 19,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"23\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515068000\",\"name\":\"foreachRDD @ 17:44:28\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    18
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 18,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"22\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515068000\",\"name\":\"foreachRDD @ 17:44:28\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    17
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            4
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515074706,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"23\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515068000\",\"name\":\"foreachRDD @ 17:44:28\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515068000\">[output operation 0, batch time 17:44:28]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515068000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 5,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 5,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515074735,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 5,
    "Stage Attempt ID": 0,
    "Task Type": "ResultTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 5,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515074735,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515075119,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 125,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 15,
                "Value": 15,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 126,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 15453526,
                "Value": 15453526,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 127,
                "Name": "internal.metrics.executorRunTime",
                "Update": 364,
                "Value": 364,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 128,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 326609953,
                "Value": 326609953,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 129,
                "Name": "internal.metrics.resultSize",
                "Update": 1780,
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 132,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 133,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 134,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 15081907,
                "Value": 15081907,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 136,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 137,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Update": 1,
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 138,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 139,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 140,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Update": 2775696,
                "Value": 2775696,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 141,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 142,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Update": 212803,
                "Value": 212803,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 148,
                "Name": "internal.metrics.output.bytesWritten",
                "Update": 4896256,
                "Value": 4896256,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 149,
                "Name": "internal.metrics.output.recordsWritten",
                "Update": 212803,
                "Value": 212803,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 15,
        "Executor Deserialize CPU Time": 15453526,
        "Executor Run Time": 364,
        "Executor CPU Time": 326609953,
        "Peak Execution Memory": 15081907,
        "Result Size": 1780,
        "JVM GC Time": 0,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 1,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 2775696,
            "Total Records Read": 212803
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 0,
            "Shuffle Write Time": 0,
            "Shuffle Records Written": 0
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 4896256,
            "Records Written": 212803
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 5,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 19,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"23\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515068000\",\"name\":\"foreachRDD @ 17:44:28\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    18
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 18,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"22\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515068000\",\"name\":\"foreachRDD @ 17:44:28\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    17
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            4
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515074706,
        "Completion Time": 1680515075121,
        "Accumulables": [
            {
                "ID": 125,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 15,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 126,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 15453526,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 127,
                "Name": "internal.metrics.executorRunTime",
                "Value": 364,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 128,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 326609953,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 129,
                "Name": "internal.metrics.resultSize",
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 132,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 133,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 134,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 15081907,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 136,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 137,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 138,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 139,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 140,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Value": 2775696,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 141,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 142,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Value": 212803,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 148,
                "Name": "internal.metrics.output.bytesWritten",
                "Value": 4896256,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 149,
                "Name": "internal.metrics.output.recordsWritten",
                "Value": 212803,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerJobEnd",
    "Job ID": 2,
    "Completion Time": 1680515075122,
    "Job Result": {
        "Result": "JobSucceeded"
    }
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 5
}
{
    "Event": "SparkListenerJobStart",
    "Job ID": 3,
    "Submission Time": 1680515075215,
    "Stage Infos": [
        {
            "Stage ID": 6,
            "Stage Attempt ID": 0,
            "Stage Name": "map at main.scala:46",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 23,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"30\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515069000\",\"name\":\"foreachRDD @ 17:44:29\"}}",
                    "Callsite": "map at main.scala:46",
                    "Parent IDs": [
                        22
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 22,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"29\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515069000\",\"name\":\"foreachRDD @ 17:44:29\"}}",
                    "Callsite": "flatMap at main.scala:41",
                    "Parent IDs": [
                        7
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 7,
                    "Name": "KafkaRDD",
                    "Scope": "{\"id\":\"0_1680515069000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:29\"}",
                    "Callsite": "createDirectStream at main.scala:31",
                    "Parent IDs": [],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [],
            "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
            "Accumulables": [],
            "Resource Profile Id": 0
        },
        {
            "Stage ID": 7,
            "Stage Attempt ID": 0,
            "Stage Name": "runJob at SparkHadoopWriter.scala:83",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 25,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"32\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515069000\",\"name\":\"foreachRDD @ 17:44:29\"}}",
                    "Callsite": "saveAsTextFile at main.scala:48",
                    "Parent IDs": [
                        24
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 24,
                    "Name": "ShuffledRDD",
                    "Scope": "{\"id\":\"31\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515069000\",\"name\":\"foreachRDD @ 17:44:29\"}}",
                    "Callsite": "reduceByKey at main.scala:46",
                    "Parent IDs": [
                        23
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [
                6
            ],
            "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
            "Accumulables": [],
            "Resource Profile Id": 0
        }
    ],
    "Stage IDs": [
        6,
        7
    ],
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"32\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515069000\",\"name\":\"foreachRDD @ 17:44:29\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515069000\">[output operation 0, batch time 17:44:29]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515069000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 6,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 23,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"30\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515069000\",\"name\":\"foreachRDD @ 17:44:29\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    22
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 22,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"29\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515069000\",\"name\":\"foreachRDD @ 17:44:29\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    7
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 7,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515069000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:29\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515075216,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"32\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515069000\",\"name\":\"foreachRDD @ 17:44:29\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515069000\">[output operation 0, batch time 17:44:29]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515069000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 6,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 6,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515075223,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 6,
    "Stage Attempt ID": 0,
    "Task Type": "ShuffleMapTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 6,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515075223,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515076751,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 150,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 2,
                "Value": 2,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 151,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 2435507,
                "Value": 2435507,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 152,
                "Name": "internal.metrics.executorRunTime",
                "Update": 1522,
                "Value": 1522,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 153,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 1345540228,
                "Value": 1345540228,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 154,
                "Name": "internal.metrics.resultSize",
                "Update": 1295,
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 155,
                "Name": "internal.metrics.jvmGCTime",
                "Update": 56,
                "Value": 56,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 157,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 158,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 159,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 23999338,
                "Value": 23999338,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 168,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Update": 2804962,
                "Value": 2804962,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 169,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Update": 205001,
                "Value": 205001,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 170,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Update": 3475914,
                "Value": 3475914,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 2,
        "Executor Deserialize CPU Time": 2435507,
        "Executor Run Time": 1522,
        "Executor CPU Time": 1345540228,
        "Peak Execution Memory": 23999338,
        "Result Size": 1295,
        "JVM GC Time": 56,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 0,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 0,
            "Total Records Read": 0
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 2804962,
            "Shuffle Write Time": 3475914,
            "Shuffle Records Written": 205001
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 0,
            "Records Written": 0
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 6,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 23,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"30\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515069000\",\"name\":\"foreachRDD @ 17:44:29\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    22
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 22,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"29\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515069000\",\"name\":\"foreachRDD @ 17:44:29\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    7
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 7,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515069000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:29\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515075216,
        "Completion Time": 1680515076754,
        "Accumulables": [
            {
                "ID": 150,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 2,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 151,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 2435507,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 152,
                "Name": "internal.metrics.executorRunTime",
                "Value": 1522,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 153,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 1345540228,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 154,
                "Name": "internal.metrics.resultSize",
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 155,
                "Name": "internal.metrics.jvmGCTime",
                "Value": 56,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 157,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 158,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 159,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 23999338,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 168,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Value": 2804962,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 169,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Value": 205001,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 170,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Value": 3475914,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 7,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 25,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"32\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515069000\",\"name\":\"foreachRDD @ 17:44:29\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    24
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 24,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"31\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515069000\",\"name\":\"foreachRDD @ 17:44:29\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    23
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            6
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515076757,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"32\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515069000\",\"name\":\"foreachRDD @ 17:44:29\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515069000\">[output operation 0, batch time 17:44:29]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515069000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 7,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 7,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515076784,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 7,
    "Stage Attempt ID": 0,
    "Task Type": "ResultTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 7,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515076784,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515077172,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 175,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 18,
                "Value": 18,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 176,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 18174905,
                "Value": 18174905,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 177,
                "Name": "internal.metrics.executorRunTime",
                "Update": 366,
                "Value": 366,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 178,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 331181442,
                "Value": 331181442,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 179,
                "Name": "internal.metrics.resultSize",
                "Update": 1780,
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 182,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 183,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 184,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 17833899,
                "Value": 17833899,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 186,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 187,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Update": 1,
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 188,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 189,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 190,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Update": 2804962,
                "Value": 2804962,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 191,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 192,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Update": 205001,
                "Value": 205001,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 198,
                "Name": "internal.metrics.output.bytesWritten",
                "Update": 4819830,
                "Value": 4819830,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 199,
                "Name": "internal.metrics.output.recordsWritten",
                "Update": 205001,
                "Value": 205001,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 18,
        "Executor Deserialize CPU Time": 18174905,
        "Executor Run Time": 366,
        "Executor CPU Time": 331181442,
        "Peak Execution Memory": 17833899,
        "Result Size": 1780,
        "JVM GC Time": 0,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 1,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 2804962,
            "Total Records Read": 205001
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 0,
            "Shuffle Write Time": 0,
            "Shuffle Records Written": 0
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 4819830,
            "Records Written": 205001
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 7,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 25,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"32\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515069000\",\"name\":\"foreachRDD @ 17:44:29\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    24
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 24,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"31\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515069000\",\"name\":\"foreachRDD @ 17:44:29\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    23
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            6
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515076757,
        "Completion Time": 1680515077173,
        "Accumulables": [
            {
                "ID": 175,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 18,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 176,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 18174905,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 177,
                "Name": "internal.metrics.executorRunTime",
                "Value": 366,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 178,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 331181442,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 179,
                "Name": "internal.metrics.resultSize",
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 182,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 183,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 184,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 17833899,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 186,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 187,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 188,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 189,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 190,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Value": 2804962,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 191,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 192,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Value": 205001,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 198,
                "Name": "internal.metrics.output.bytesWritten",
                "Value": 4819830,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 199,
                "Name": "internal.metrics.output.recordsWritten",
                "Value": 205001,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerJobEnd",
    "Job ID": 3,
    "Completion Time": 1680515077174,
    "Job Result": {
        "Result": "JobSucceeded"
    }
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 6
}
{
    "Event": "SparkListenerJobStart",
    "Job ID": 4,
    "Submission Time": 1680515077249,
    "Stage Infos": [
        {
            "Stage ID": 8,
            "Stage Attempt ID": 0,
            "Stage Name": "map at main.scala:46",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 29,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"39\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515070000\",\"name\":\"foreachRDD @ 17:44:30\"}}",
                    "Callsite": "map at main.scala:46",
                    "Parent IDs": [
                        28
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 28,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"38\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515070000\",\"name\":\"foreachRDD @ 17:44:30\"}}",
                    "Callsite": "flatMap at main.scala:41",
                    "Parent IDs": [
                        8
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 8,
                    "Name": "KafkaRDD",
                    "Scope": "{\"id\":\"0_1680515070000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:30\"}",
                    "Callsite": "createDirectStream at main.scala:31",
                    "Parent IDs": [],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [],
            "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
            "Accumulables": [],
            "Resource Profile Id": 0
        },
        {
            "Stage ID": 9,
            "Stage Attempt ID": 0,
            "Stage Name": "runJob at SparkHadoopWriter.scala:83",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 31,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"41\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515070000\",\"name\":\"foreachRDD @ 17:44:30\"}}",
                    "Callsite": "saveAsTextFile at main.scala:48",
                    "Parent IDs": [
                        30
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 30,
                    "Name": "ShuffledRDD",
                    "Scope": "{\"id\":\"40\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515070000\",\"name\":\"foreachRDD @ 17:44:30\"}}",
                    "Callsite": "reduceByKey at main.scala:46",
                    "Parent IDs": [
                        29
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [
                8
            ],
            "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
            "Accumulables": [],
            "Resource Profile Id": 0
        }
    ],
    "Stage IDs": [
        8,
        9
    ],
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"41\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515070000\",\"name\":\"foreachRDD @ 17:44:30\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515070000\">[output operation 0, batch time 17:44:30]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515070000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 8,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 29,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"39\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515070000\",\"name\":\"foreachRDD @ 17:44:30\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    28
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 28,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"38\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515070000\",\"name\":\"foreachRDD @ 17:44:30\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    8
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 8,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515070000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:30\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515077251,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"41\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515070000\",\"name\":\"foreachRDD @ 17:44:30\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515070000\">[output operation 0, batch time 17:44:30]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515070000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 8,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 8,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515077257,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 8,
    "Stage Attempt ID": 0,
    "Task Type": "ShuffleMapTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 8,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515077257,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515078885,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 200,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 5,
                "Value": 5,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 201,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 5539988,
                "Value": 5539988,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 202,
                "Name": "internal.metrics.executorRunTime",
                "Update": 1618,
                "Value": 1618,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 203,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 1462466319,
                "Value": 1462466319,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 204,
                "Name": "internal.metrics.resultSize",
                "Update": 1295,
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 205,
                "Name": "internal.metrics.jvmGCTime",
                "Update": 32,
                "Value": 32,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 207,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 208,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 209,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 25592774,
                "Value": 25592774,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 218,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Update": 3267122,
                "Value": 3267122,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 219,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Update": 227751,
                "Value": 227751,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 220,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Update": 3522327,
                "Value": 3522327,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 5,
        "Executor Deserialize CPU Time": 5539988,
        "Executor Run Time": 1618,
        "Executor CPU Time": 1462466319,
        "Peak Execution Memory": 25592774,
        "Result Size": 1295,
        "JVM GC Time": 32,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 0,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 0,
            "Total Records Read": 0
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 3267122,
            "Shuffle Write Time": 3522327,
            "Shuffle Records Written": 227751
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 0,
            "Records Written": 0
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 8,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 29,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"39\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515070000\",\"name\":\"foreachRDD @ 17:44:30\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    28
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 28,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"38\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515070000\",\"name\":\"foreachRDD @ 17:44:30\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    8
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 8,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515070000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:30\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515077251,
        "Completion Time": 1680515078887,
        "Accumulables": [
            {
                "ID": 200,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 5,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 201,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 5539988,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 202,
                "Name": "internal.metrics.executorRunTime",
                "Value": 1618,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 203,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 1462466319,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 204,
                "Name": "internal.metrics.resultSize",
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 205,
                "Name": "internal.metrics.jvmGCTime",
                "Value": 32,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 207,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 208,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 209,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 25592774,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 218,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Value": 3267122,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 219,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Value": 227751,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 220,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Value": 3522327,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 9,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 31,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"41\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515070000\",\"name\":\"foreachRDD @ 17:44:30\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    30
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 30,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"40\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515070000\",\"name\":\"foreachRDD @ 17:44:30\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    29
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            8
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515078889,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"41\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515070000\",\"name\":\"foreachRDD @ 17:44:30\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515070000\">[output operation 0, batch time 17:44:30]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515070000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 9,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 9,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515078908,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 9,
    "Stage Attempt ID": 0,
    "Task Type": "ResultTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 9,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515078908,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515079368,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 225,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 11,
                "Value": 11,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 226,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 11087445,
                "Value": 11087445,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 227,
                "Name": "internal.metrics.executorRunTime",
                "Update": 443,
                "Value": 443,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 228,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 360983414,
                "Value": 360983414,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 229,
                "Name": "internal.metrics.resultSize",
                "Update": 1823,
                "Value": 1823,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 230,
                "Name": "internal.metrics.jvmGCTime",
                "Update": 45,
                "Value": 45,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 232,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 233,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 234,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 22875117,
                "Value": 22875117,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 236,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 237,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Update": 1,
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 238,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 239,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 240,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Update": 3267122,
                "Value": 3267122,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 241,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 242,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Update": 227751,
                "Value": 227751,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 248,
                "Name": "internal.metrics.output.bytesWritten",
                "Update": 8495961,
                "Value": 8495961,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 249,
                "Name": "internal.metrics.output.recordsWritten",
                "Update": 227751,
                "Value": 227751,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 11,
        "Executor Deserialize CPU Time": 11087445,
        "Executor Run Time": 443,
        "Executor CPU Time": 360983414,
        "Peak Execution Memory": 22875117,
        "Result Size": 1823,
        "JVM GC Time": 45,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 1,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 3267122,
            "Total Records Read": 227751
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 0,
            "Shuffle Write Time": 0,
            "Shuffle Records Written": 0
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 8495961,
            "Records Written": 227751
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 9,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 31,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"41\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515070000\",\"name\":\"foreachRDD @ 17:44:30\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    30
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 30,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"40\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515070000\",\"name\":\"foreachRDD @ 17:44:30\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    29
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            8
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515078889,
        "Completion Time": 1680515079370,
        "Accumulables": [
            {
                "ID": 225,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 11,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 226,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 11087445,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 227,
                "Name": "internal.metrics.executorRunTime",
                "Value": 443,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 228,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 360983414,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 229,
                "Name": "internal.metrics.resultSize",
                "Value": 1823,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 230,
                "Name": "internal.metrics.jvmGCTime",
                "Value": 45,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 232,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 233,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 234,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 22875117,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 236,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 237,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 238,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 239,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 240,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Value": 3267122,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 241,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 242,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Value": 227751,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 248,
                "Name": "internal.metrics.output.bytesWritten",
                "Value": 8495961,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 249,
                "Name": "internal.metrics.output.recordsWritten",
                "Value": 227751,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerJobEnd",
    "Job ID": 4,
    "Completion Time": 1680515079371,
    "Job Result": {
        "Result": "JobSucceeded"
    }
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 7
}
{
    "Event": "SparkListenerJobStart",
    "Job ID": 5,
    "Submission Time": 1680515079451,
    "Stage Infos": [
        {
            "Stage ID": 10,
            "Stage Attempt ID": 0,
            "Stage Name": "map at main.scala:46",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 35,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"48\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515071000\",\"name\":\"foreachRDD @ 17:44:31\"}}",
                    "Callsite": "map at main.scala:46",
                    "Parent IDs": [
                        34
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 13,
                    "Name": "KafkaRDD",
                    "Scope": "{\"id\":\"0_1680515071000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:31\"}",
                    "Callsite": "createDirectStream at main.scala:31",
                    "Parent IDs": [],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 34,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"47\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515071000\",\"name\":\"foreachRDD @ 17:44:31\"}}",
                    "Callsite": "flatMap at main.scala:41",
                    "Parent IDs": [
                        13
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [],
            "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
            "Accumulables": [],
            "Resource Profile Id": 0
        },
        {
            "Stage ID": 11,
            "Stage Attempt ID": 0,
            "Stage Name": "runJob at SparkHadoopWriter.scala:83",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 37,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"50\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515071000\",\"name\":\"foreachRDD @ 17:44:31\"}}",
                    "Callsite": "saveAsTextFile at main.scala:48",
                    "Parent IDs": [
                        36
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 36,
                    "Name": "ShuffledRDD",
                    "Scope": "{\"id\":\"49\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515071000\",\"name\":\"foreachRDD @ 17:44:31\"}}",
                    "Callsite": "reduceByKey at main.scala:46",
                    "Parent IDs": [
                        35
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [
                10
            ],
            "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
            "Accumulables": [],
            "Resource Profile Id": 0
        }
    ],
    "Stage IDs": [
        10,
        11
    ],
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"50\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515071000\",\"name\":\"foreachRDD @ 17:44:31\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515071000\">[output operation 0, batch time 17:44:31]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515071000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 10,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 35,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"48\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515071000\",\"name\":\"foreachRDD @ 17:44:31\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    34
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 13,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515071000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:31\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 34,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"47\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515071000\",\"name\":\"foreachRDD @ 17:44:31\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    13
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515079454,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"50\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515071000\",\"name\":\"foreachRDD @ 17:44:31\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515071000\">[output operation 0, batch time 17:44:31]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515071000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 10,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 10,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515079468,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 10,
    "Stage Attempt ID": 0,
    "Task Type": "ShuffleMapTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 10,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515079468,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515081025,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 250,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 5,
                "Value": 5,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 251,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 5189277,
                "Value": 5189277,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 252,
                "Name": "internal.metrics.executorRunTime",
                "Update": 1547,
                "Value": 1547,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 253,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 1394992941,
                "Value": 1394992941,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 254,
                "Name": "internal.metrics.resultSize",
                "Update": 1295,
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 255,
                "Name": "internal.metrics.jvmGCTime",
                "Update": 31,
                "Value": 31,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 257,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 258,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 259,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 24943850,
                "Value": 24943850,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 268,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Update": 2556097,
                "Value": 2556097,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 269,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Update": 223161,
                "Value": 223161,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 270,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Update": 3000941,
                "Value": 3000941,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 5,
        "Executor Deserialize CPU Time": 5189277,
        "Executor Run Time": 1547,
        "Executor CPU Time": 1394992941,
        "Peak Execution Memory": 24943850,
        "Result Size": 1295,
        "JVM GC Time": 31,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 0,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 0,
            "Total Records Read": 0
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 2556097,
            "Shuffle Write Time": 3000941,
            "Shuffle Records Written": 223161
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 0,
            "Records Written": 0
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 10,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 35,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"48\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515071000\",\"name\":\"foreachRDD @ 17:44:31\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    34
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 13,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515071000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:31\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 34,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"47\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515071000\",\"name\":\"foreachRDD @ 17:44:31\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    13
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515079454,
        "Completion Time": 1680515081027,
        "Accumulables": [
            {
                "ID": 250,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 5,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 251,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 5189277,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 252,
                "Name": "internal.metrics.executorRunTime",
                "Value": 1547,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 253,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 1394992941,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 254,
                "Name": "internal.metrics.resultSize",
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 255,
                "Name": "internal.metrics.jvmGCTime",
                "Value": 31,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 257,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 258,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 259,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 24943850,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 268,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Value": 2556097,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 269,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Value": 223161,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 270,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Value": 3000941,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 11,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 37,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"50\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515071000\",\"name\":\"foreachRDD @ 17:44:31\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    36
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 36,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"49\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515071000\",\"name\":\"foreachRDD @ 17:44:31\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    35
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            10
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515081030,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"50\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515071000\",\"name\":\"foreachRDD @ 17:44:31\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515071000\">[output operation 0, batch time 17:44:31]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515071000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 11,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 11,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515081055,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 11,
    "Stage Attempt ID": 0,
    "Task Type": "ResultTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 11,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515081055,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515081423,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 275,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 9,
                "Value": 9,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 276,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 9574949,
                "Value": 9574949,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 277,
                "Name": "internal.metrics.executorRunTime",
                "Update": 354,
                "Value": 354,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 278,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 322350412,
                "Value": 322350412,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 279,
                "Name": "internal.metrics.resultSize",
                "Update": 1780,
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 282,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 283,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 284,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 15371596,
                "Value": 15371596,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 286,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 287,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Update": 1,
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 288,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 289,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 290,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Update": 2556097,
                "Value": 2556097,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 291,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 292,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Update": 223161,
                "Value": 223161,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 298,
                "Name": "internal.metrics.output.bytesWritten",
                "Update": 5107782,
                "Value": 5107782,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 299,
                "Name": "internal.metrics.output.recordsWritten",
                "Update": 223161,
                "Value": 223161,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 9,
        "Executor Deserialize CPU Time": 9574949,
        "Executor Run Time": 354,
        "Executor CPU Time": 322350412,
        "Peak Execution Memory": 15371596,
        "Result Size": 1780,
        "JVM GC Time": 0,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 1,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 2556097,
            "Total Records Read": 223161
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 0,
            "Shuffle Write Time": 0,
            "Shuffle Records Written": 0
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 5107782,
            "Records Written": 223161
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 11,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 37,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"50\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515071000\",\"name\":\"foreachRDD @ 17:44:31\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    36
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 36,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"49\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515071000\",\"name\":\"foreachRDD @ 17:44:31\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    35
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            10
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515081030,
        "Completion Time": 1680515081424,
        "Accumulables": [
            {
                "ID": 275,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 9,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 276,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 9574949,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 277,
                "Name": "internal.metrics.executorRunTime",
                "Value": 354,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 278,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 322350412,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 279,
                "Name": "internal.metrics.resultSize",
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 282,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 283,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 284,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 15371596,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 286,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 287,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 288,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 289,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 290,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Value": 2556097,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 291,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 292,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Value": 223161,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 298,
                "Name": "internal.metrics.output.bytesWritten",
                "Value": 5107782,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 299,
                "Name": "internal.metrics.output.recordsWritten",
                "Value": 223161,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerJobEnd",
    "Job ID": 5,
    "Completion Time": 1680515081425,
    "Job Result": {
        "Result": "JobSucceeded"
    }
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 8
}
{
    "Event": "SparkListenerJobStart",
    "Job ID": 6,
    "Submission Time": 1680515081502,
    "Stage Infos": [
        {
            "Stage ID": 12,
            "Stage Attempt ID": 0,
            "Stage Name": "map at main.scala:46",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 41,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"57\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515072000\",\"name\":\"foreachRDD @ 17:44:32\"}}",
                    "Callsite": "map at main.scala:46",
                    "Parent IDs": [
                        40
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 40,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"56\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515072000\",\"name\":\"foreachRDD @ 17:44:32\"}}",
                    "Callsite": "flatMap at main.scala:41",
                    "Parent IDs": [
                        14
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 14,
                    "Name": "KafkaRDD",
                    "Scope": "{\"id\":\"0_1680515072000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:32\"}",
                    "Callsite": "createDirectStream at main.scala:31",
                    "Parent IDs": [],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [],
            "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
            "Accumulables": [],
            "Resource Profile Id": 0
        },
        {
            "Stage ID": 13,
            "Stage Attempt ID": 0,
            "Stage Name": "runJob at SparkHadoopWriter.scala:83",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 43,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"59\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515072000\",\"name\":\"foreachRDD @ 17:44:32\"}}",
                    "Callsite": "saveAsTextFile at main.scala:48",
                    "Parent IDs": [
                        42
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 42,
                    "Name": "ShuffledRDD",
                    "Scope": "{\"id\":\"58\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515072000\",\"name\":\"foreachRDD @ 17:44:32\"}}",
                    "Callsite": "reduceByKey at main.scala:46",
                    "Parent IDs": [
                        41
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [
                12
            ],
            "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
            "Accumulables": [],
            "Resource Profile Id": 0
        }
    ],
    "Stage IDs": [
        12,
        13
    ],
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"59\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515072000\",\"name\":\"foreachRDD @ 17:44:32\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515072000\">[output operation 0, batch time 17:44:32]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515072000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 12,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 41,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"57\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515072000\",\"name\":\"foreachRDD @ 17:44:32\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    40
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 40,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"56\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515072000\",\"name\":\"foreachRDD @ 17:44:32\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    14
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 14,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515072000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:32\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515081504,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"59\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515072000\",\"name\":\"foreachRDD @ 17:44:32\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515072000\">[output operation 0, batch time 17:44:32]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515072000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 12,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 12,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515081513,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 12,
    "Stage Attempt ID": 0,
    "Task Type": "ShuffleMapTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 12,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515081513,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515083062,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 300,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 2,
                "Value": 2,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 301,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 2465312,
                "Value": 2465312,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 302,
                "Name": "internal.metrics.executorRunTime",
                "Update": 1543,
                "Value": 1543,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 303,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 1378732591,
                "Value": 1378732591,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 304,
                "Name": "internal.metrics.resultSize",
                "Update": 1295,
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 305,
                "Name": "internal.metrics.jvmGCTime",
                "Update": 57,
                "Value": 57,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 307,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 308,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 309,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 28577797,
                "Value": 28577797,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 318,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Update": 2323427,
                "Value": 2323427,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 319,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Update": 219552,
                "Value": 219552,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 320,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Update": 2763589,
                "Value": 2763589,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 522147840,
        "JVMOffHeapMemory": 126135088,
        "OnHeapExecutionMemory": 5589892,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 42222,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 5632114,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 35660479,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 20,
        "MinorGCTime": 424,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 424
    },
    "Task Metrics": {
        "Executor Deserialize Time": 2,
        "Executor Deserialize CPU Time": 2465312,
        "Executor Run Time": 1543,
        "Executor CPU Time": 1378732591,
        "Peak Execution Memory": 28577797,
        "Result Size": 1295,
        "JVM GC Time": 57,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 0,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 0,
            "Total Records Read": 0
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 2323427,
            "Shuffle Write Time": 2763589,
            "Shuffle Records Written": 219552
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 0,
            "Records Written": 0
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 12,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 41,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"57\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515072000\",\"name\":\"foreachRDD @ 17:44:32\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    40
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 40,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"56\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515072000\",\"name\":\"foreachRDD @ 17:44:32\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    14
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 14,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515072000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:32\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515081504,
        "Completion Time": 1680515083064,
        "Accumulables": [
            {
                "ID": 300,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 2,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 301,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 2465312,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 302,
                "Name": "internal.metrics.executorRunTime",
                "Value": 1543,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 303,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 1378732591,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 304,
                "Name": "internal.metrics.resultSize",
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 305,
                "Name": "internal.metrics.jvmGCTime",
                "Value": 57,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 307,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 308,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 309,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 28577797,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 318,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Value": 2323427,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 319,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Value": 219552,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 320,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Value": 2763589,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 13,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 43,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"59\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515072000\",\"name\":\"foreachRDD @ 17:44:32\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    42
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 42,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"58\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515072000\",\"name\":\"foreachRDD @ 17:44:32\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    41
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            12
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515083067,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"59\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515072000\",\"name\":\"foreachRDD @ 17:44:32\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515072000\">[output operation 0, batch time 17:44:32]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515072000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 13,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 13,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515083094,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 13,
    "Stage Attempt ID": 0,
    "Task Type": "ResultTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 13,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515083094,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515083453,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 325,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 7,
                "Value": 7,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 326,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 7714291,
                "Value": 7714291,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 327,
                "Name": "internal.metrics.executorRunTime",
                "Update": 345,
                "Value": 345,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 328,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 315668707,
                "Value": 315668707,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 329,
                "Name": "internal.metrics.resultSize",
                "Update": 1780,
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 332,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 333,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 334,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 22335443,
                "Value": 22335443,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 336,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 337,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Update": 1,
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 338,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 339,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 340,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Update": 2323427,
                "Value": 2323427,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 341,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 342,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Update": 219552,
                "Value": 219552,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 348,
                "Name": "internal.metrics.output.bytesWritten",
                "Update": 5051905,
                "Value": 5051905,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 349,
                "Name": "internal.metrics.output.recordsWritten",
                "Update": 219552,
                "Value": 219552,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 7,
        "Executor Deserialize CPU Time": 7714291,
        "Executor Run Time": 345,
        "Executor CPU Time": 315668707,
        "Peak Execution Memory": 22335443,
        "Result Size": 1780,
        "JVM GC Time": 0,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 1,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 2323427,
            "Total Records Read": 219552
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 0,
            "Shuffle Write Time": 0,
            "Shuffle Records Written": 0
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 5051905,
            "Records Written": 219552
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 13,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 43,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"59\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515072000\",\"name\":\"foreachRDD @ 17:44:32\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    42
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 42,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"58\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515072000\",\"name\":\"foreachRDD @ 17:44:32\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    41
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            12
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515083067,
        "Completion Time": 1680515083455,
        "Accumulables": [
            {
                "ID": 325,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 7,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 326,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 7714291,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 327,
                "Name": "internal.metrics.executorRunTime",
                "Value": 345,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 328,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 315668707,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 329,
                "Name": "internal.metrics.resultSize",
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 332,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 333,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 334,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 22335443,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 336,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 337,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 338,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 339,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 340,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Value": 2323427,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 341,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 342,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Value": 219552,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 348,
                "Name": "internal.metrics.output.bytesWritten",
                "Value": 5051905,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 349,
                "Name": "internal.metrics.output.recordsWritten",
                "Value": 219552,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerJobEnd",
    "Job ID": 6,
    "Completion Time": 1680515083455,
    "Job Result": {
        "Result": "JobSucceeded"
    }
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 13
}
{
    "Event": "SparkListenerJobStart",
    "Job ID": 7,
    "Submission Time": 1680515083531,
    "Stage Infos": [
        {
            "Stage ID": 14,
            "Stage Attempt ID": 0,
            "Stage Name": "map at main.scala:46",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 47,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"66\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515073000\",\"name\":\"foreachRDD @ 17:44:33\"}}",
                    "Callsite": "map at main.scala:46",
                    "Parent IDs": [
                        46
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 15,
                    "Name": "KafkaRDD",
                    "Scope": "{\"id\":\"0_1680515073000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:33\"}",
                    "Callsite": "createDirectStream at main.scala:31",
                    "Parent IDs": [],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 46,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"65\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515073000\",\"name\":\"foreachRDD @ 17:44:33\"}}",
                    "Callsite": "flatMap at main.scala:41",
                    "Parent IDs": [
                        15
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [],
            "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
            "Accumulables": [],
            "Resource Profile Id": 0
        },
        {
            "Stage ID": 15,
            "Stage Attempt ID": 0,
            "Stage Name": "runJob at SparkHadoopWriter.scala:83",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 49,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"68\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515073000\",\"name\":\"foreachRDD @ 17:44:33\"}}",
                    "Callsite": "saveAsTextFile at main.scala:48",
                    "Parent IDs": [
                        48
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 48,
                    "Name": "ShuffledRDD",
                    "Scope": "{\"id\":\"67\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515073000\",\"name\":\"foreachRDD @ 17:44:33\"}}",
                    "Callsite": "reduceByKey at main.scala:46",
                    "Parent IDs": [
                        47
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [
                14
            ],
            "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
            "Accumulables": [],
            "Resource Profile Id": 0
        }
    ],
    "Stage IDs": [
        14,
        15
    ],
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"68\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515073000\",\"name\":\"foreachRDD @ 17:44:33\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515073000\">[output operation 0, batch time 17:44:33]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515073000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 14,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 47,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"66\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515073000\",\"name\":\"foreachRDD @ 17:44:33\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    46
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 15,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515073000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:33\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 46,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"65\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515073000\",\"name\":\"foreachRDD @ 17:44:33\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    15
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515083533,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"68\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515073000\",\"name\":\"foreachRDD @ 17:44:33\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515073000\">[output operation 0, batch time 17:44:33]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515073000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 14,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 14,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515083540,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 14,
    "Stage Attempt ID": 0,
    "Task Type": "ShuffleMapTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 14,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515083540,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515085102,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 350,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 4,
                "Value": 4,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 351,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 4242402,
                "Value": 4242402,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 352,
                "Name": "internal.metrics.executorRunTime",
                "Update": 1555,
                "Value": 1555,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 353,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 1348308875,
                "Value": 1348308875,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 354,
                "Name": "internal.metrics.resultSize",
                "Update": 1295,
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 355,
                "Name": "internal.metrics.jvmGCTime",
                "Update": 68,
                "Value": 68,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 357,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 358,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 359,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 22601417,
                "Value": 22601417,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 368,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Update": 2544439,
                "Value": 2544439,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 369,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Update": 225332,
                "Value": 225332,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 370,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Update": 3044334,
                "Value": 3044334,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 4,
        "Executor Deserialize CPU Time": 4242402,
        "Executor Run Time": 1555,
        "Executor CPU Time": 1348308875,
        "Peak Execution Memory": 22601417,
        "Result Size": 1295,
        "JVM GC Time": 68,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 0,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 0,
            "Total Records Read": 0
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 2544439,
            "Shuffle Write Time": 3044334,
            "Shuffle Records Written": 225332
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 0,
            "Records Written": 0
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 14,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 47,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"66\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515073000\",\"name\":\"foreachRDD @ 17:44:33\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    46
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 15,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515073000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:33\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 46,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"65\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515073000\",\"name\":\"foreachRDD @ 17:44:33\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    15
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515083533,
        "Completion Time": 1680515085104,
        "Accumulables": [
            {
                "ID": 350,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 4,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 351,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 4242402,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 352,
                "Name": "internal.metrics.executorRunTime",
                "Value": 1555,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 353,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 1348308875,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 354,
                "Name": "internal.metrics.resultSize",
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 355,
                "Name": "internal.metrics.jvmGCTime",
                "Value": 68,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 357,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 358,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 359,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 22601417,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 368,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Value": 2544439,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 369,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Value": 225332,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 370,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Value": 3044334,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 15,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 49,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"68\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515073000\",\"name\":\"foreachRDD @ 17:44:33\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    48
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 48,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"67\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515073000\",\"name\":\"foreachRDD @ 17:44:33\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    47
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            14
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515085107,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"68\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515073000\",\"name\":\"foreachRDD @ 17:44:33\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515073000\">[output operation 0, batch time 17:44:33]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515073000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 15,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 15,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515085136,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 15,
    "Stage Attempt ID": 0,
    "Task Type": "ResultTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 15,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515085136,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515085494,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 375,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 10,
                "Value": 10,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 376,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 10416036,
                "Value": 10416036,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 377,
                "Name": "internal.metrics.executorRunTime",
                "Update": 343,
                "Value": 343,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 378,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 308572036,
                "Value": 308572036,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 379,
                "Name": "internal.metrics.resultSize",
                "Update": 1780,
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 382,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 383,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 384,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 15789675,
                "Value": 15789675,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 386,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 387,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Update": 1,
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 388,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 389,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 390,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Update": 2544439,
                "Value": 2544439,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 391,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 392,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Update": 225332,
                "Value": 225332,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 398,
                "Name": "internal.metrics.output.bytesWritten",
                "Update": 5224800,
                "Value": 5224800,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 399,
                "Name": "internal.metrics.output.recordsWritten",
                "Update": 225332,
                "Value": 225332,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 10,
        "Executor Deserialize CPU Time": 10416036,
        "Executor Run Time": 343,
        "Executor CPU Time": 308572036,
        "Peak Execution Memory": 15789675,
        "Result Size": 1780,
        "JVM GC Time": 0,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 1,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 2544439,
            "Total Records Read": 225332
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 0,
            "Shuffle Write Time": 0,
            "Shuffle Records Written": 0
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 5224800,
            "Records Written": 225332
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 15,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 49,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"68\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515073000\",\"name\":\"foreachRDD @ 17:44:33\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    48
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 48,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"67\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515073000\",\"name\":\"foreachRDD @ 17:44:33\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    47
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            14
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515085107,
        "Completion Time": 1680515085496,
        "Accumulables": [
            {
                "ID": 375,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 10,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 376,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 10416036,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 377,
                "Name": "internal.metrics.executorRunTime",
                "Value": 343,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 378,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 308572036,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 379,
                "Name": "internal.metrics.resultSize",
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 382,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 383,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 384,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 15789675,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 386,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 387,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 388,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 389,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 390,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Value": 2544439,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 391,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 392,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Value": 225332,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 398,
                "Name": "internal.metrics.output.bytesWritten",
                "Value": 5224800,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 399,
                "Name": "internal.metrics.output.recordsWritten",
                "Value": 225332,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerJobEnd",
    "Job ID": 7,
    "Completion Time": 1680515085496,
    "Job Result": {
        "Result": "JobSucceeded"
    }
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 14
}
{
    "Event": "SparkListenerJobStart",
    "Job ID": 8,
    "Submission Time": 1680515085582,
    "Stage Infos": [
        {
            "Stage ID": 16,
            "Stage Attempt ID": 0,
            "Stage Name": "map at main.scala:46",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 53,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"75\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515074000\",\"name\":\"foreachRDD @ 17:44:34\"}}",
                    "Callsite": "map at main.scala:46",
                    "Parent IDs": [
                        52
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 20,
                    "Name": "KafkaRDD",
                    "Scope": "{\"id\":\"0_1680515074000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:34\"}",
                    "Callsite": "createDirectStream at main.scala:31",
                    "Parent IDs": [],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 52,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"74\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515074000\",\"name\":\"foreachRDD @ 17:44:34\"}}",
                    "Callsite": "flatMap at main.scala:41",
                    "Parent IDs": [
                        20
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [],
            "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
            "Accumulables": [],
            "Resource Profile Id": 0
        },
        {
            "Stage ID": 17,
            "Stage Attempt ID": 0,
            "Stage Name": "runJob at SparkHadoopWriter.scala:83",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 55,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"77\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515074000\",\"name\":\"foreachRDD @ 17:44:34\"}}",
                    "Callsite": "saveAsTextFile at main.scala:48",
                    "Parent IDs": [
                        54
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 54,
                    "Name": "ShuffledRDD",
                    "Scope": "{\"id\":\"76\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515074000\",\"name\":\"foreachRDD @ 17:44:34\"}}",
                    "Callsite": "reduceByKey at main.scala:46",
                    "Parent IDs": [
                        53
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [
                16
            ],
            "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
            "Accumulables": [],
            "Resource Profile Id": 0
        }
    ],
    "Stage IDs": [
        16,
        17
    ],
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"77\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515074000\",\"name\":\"foreachRDD @ 17:44:34\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515074000\">[output operation 0, batch time 17:44:34]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515074000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 16,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 53,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"75\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515074000\",\"name\":\"foreachRDD @ 17:44:34\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    52
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 20,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515074000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:34\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 52,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"74\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515074000\",\"name\":\"foreachRDD @ 17:44:34\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    20
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515085584,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"77\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515074000\",\"name\":\"foreachRDD @ 17:44:34\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515074000\">[output operation 0, batch time 17:44:34]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515074000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 16,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 16,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515085590,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 16,
    "Stage Attempt ID": 0,
    "Task Type": "ShuffleMapTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 16,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515085590,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515087154,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 400,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 2,
                "Value": 2,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 401,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 2117436,
                "Value": 2117436,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 402,
                "Name": "internal.metrics.executorRunTime",
                "Update": 1558,
                "Value": 1558,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 403,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 1393083250,
                "Value": 1393083250,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 404,
                "Name": "internal.metrics.resultSize",
                "Update": 1295,
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 405,
                "Name": "internal.metrics.jvmGCTime",
                "Update": 16,
                "Value": 16,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 407,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 408,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 409,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 23806467,
                "Value": 23806467,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 418,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Update": 2823522,
                "Value": 2823522,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 419,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Update": 221549,
                "Value": 221549,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 420,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Update": 3795318,
                "Value": 3795318,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 2,
        "Executor Deserialize CPU Time": 2117436,
        "Executor Run Time": 1558,
        "Executor CPU Time": 1393083250,
        "Peak Execution Memory": 23806467,
        "Result Size": 1295,
        "JVM GC Time": 16,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 0,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 0,
            "Total Records Read": 0
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 2823522,
            "Shuffle Write Time": 3795318,
            "Shuffle Records Written": 221549
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 0,
            "Records Written": 0
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 16,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 53,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"75\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515074000\",\"name\":\"foreachRDD @ 17:44:34\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    52
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 20,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515074000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:34\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 52,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"74\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515074000\",\"name\":\"foreachRDD @ 17:44:34\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    20
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515085584,
        "Completion Time": 1680515087155,
        "Accumulables": [
            {
                "ID": 400,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 2,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 401,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 2117436,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 402,
                "Name": "internal.metrics.executorRunTime",
                "Value": 1558,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 403,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 1393083250,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 404,
                "Name": "internal.metrics.resultSize",
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 405,
                "Name": "internal.metrics.jvmGCTime",
                "Value": 16,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 407,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 408,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 409,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 23806467,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 418,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Value": 2823522,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 419,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Value": 221549,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 420,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Value": 3795318,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 17,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 55,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"77\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515074000\",\"name\":\"foreachRDD @ 17:44:34\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    54
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 54,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"76\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515074000\",\"name\":\"foreachRDD @ 17:44:34\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    53
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            16
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515087158,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"77\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515074000\",\"name\":\"foreachRDD @ 17:44:34\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515074000\">[output operation 0, batch time 17:44:34]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515074000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 17,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 17,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515087182,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 17,
    "Stage Attempt ID": 0,
    "Task Type": "ResultTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 17,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515087182,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515087558,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 425,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 12,
                "Value": 12,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 426,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 12708992,
                "Value": 12708992,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 427,
                "Name": "internal.metrics.executorRunTime",
                "Update": 359,
                "Value": 359,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 428,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 326950357,
                "Value": 326950357,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 429,
                "Name": "internal.metrics.resultSize",
                "Update": 1780,
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 432,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 433,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 434,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 14352150,
                "Value": 14352150,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 436,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 437,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Update": 1,
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 438,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 439,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 440,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Update": 2823522,
                "Value": 2823522,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 441,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 442,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Update": 221549,
                "Value": 221549,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 448,
                "Name": "internal.metrics.output.bytesWritten",
                "Update": 5250700,
                "Value": 5250700,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 449,
                "Name": "internal.metrics.output.recordsWritten",
                "Update": 221549,
                "Value": 221549,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 12,
        "Executor Deserialize CPU Time": 12708992,
        "Executor Run Time": 359,
        "Executor CPU Time": 326950357,
        "Peak Execution Memory": 14352150,
        "Result Size": 1780,
        "JVM GC Time": 0,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 1,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 2823522,
            "Total Records Read": 221549
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 0,
            "Shuffle Write Time": 0,
            "Shuffle Records Written": 0
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 5250700,
            "Records Written": 221549
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 17,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 55,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"77\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515074000\",\"name\":\"foreachRDD @ 17:44:34\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    54
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 54,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"76\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515074000\",\"name\":\"foreachRDD @ 17:44:34\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    53
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            16
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515087158,
        "Completion Time": 1680515087560,
        "Accumulables": [
            {
                "ID": 425,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 12,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 426,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 12708992,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 427,
                "Name": "internal.metrics.executorRunTime",
                "Value": 359,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 428,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 326950357,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 429,
                "Name": "internal.metrics.resultSize",
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 432,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 433,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 434,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 14352150,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 436,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 437,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 438,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 439,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 440,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Value": 2823522,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 441,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 442,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Value": 221549,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 448,
                "Name": "internal.metrics.output.bytesWritten",
                "Value": 5250700,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 449,
                "Name": "internal.metrics.output.recordsWritten",
                "Value": 221549,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerJobEnd",
    "Job ID": 8,
    "Completion Time": 1680515087560,
    "Job Result": {
        "Result": "JobSucceeded"
    }
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 15
}
{
    "Event": "SparkListenerJobStart",
    "Job ID": 9,
    "Submission Time": 1680515087647,
    "Stage Infos": [
        {
            "Stage ID": 18,
            "Stage Attempt ID": 0,
            "Stage Name": "map at main.scala:46",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 59,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"84\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515075000\",\"name\":\"foreachRDD @ 17:44:35\"}}",
                    "Callsite": "map at main.scala:46",
                    "Parent IDs": [
                        58
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 21,
                    "Name": "KafkaRDD",
                    "Scope": "{\"id\":\"0_1680515075000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:35\"}",
                    "Callsite": "createDirectStream at main.scala:31",
                    "Parent IDs": [],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 58,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"83\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515075000\",\"name\":\"foreachRDD @ 17:44:35\"}}",
                    "Callsite": "flatMap at main.scala:41",
                    "Parent IDs": [
                        21
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [],
            "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
            "Accumulables": [],
            "Resource Profile Id": 0
        },
        {
            "Stage ID": 19,
            "Stage Attempt ID": 0,
            "Stage Name": "runJob at SparkHadoopWriter.scala:83",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 61,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"86\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515075000\",\"name\":\"foreachRDD @ 17:44:35\"}}",
                    "Callsite": "saveAsTextFile at main.scala:48",
                    "Parent IDs": [
                        60
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 60,
                    "Name": "ShuffledRDD",
                    "Scope": "{\"id\":\"85\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515075000\",\"name\":\"foreachRDD @ 17:44:35\"}}",
                    "Callsite": "reduceByKey at main.scala:46",
                    "Parent IDs": [
                        59
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [
                18
            ],
            "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
            "Accumulables": [],
            "Resource Profile Id": 0
        }
    ],
    "Stage IDs": [
        18,
        19
    ],
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"86\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515075000\",\"name\":\"foreachRDD @ 17:44:35\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515075000\">[output operation 0, batch time 17:44:35]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515075000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 18,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 59,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"84\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515075000\",\"name\":\"foreachRDD @ 17:44:35\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    58
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 21,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515075000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:35\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 58,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"83\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515075000\",\"name\":\"foreachRDD @ 17:44:35\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    21
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515087650,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"86\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515075000\",\"name\":\"foreachRDD @ 17:44:35\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515075000\">[output operation 0, batch time 17:44:35]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515075000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 18,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 18,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515087664,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 18,
    "Stage Attempt ID": 0,
    "Task Type": "ShuffleMapTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 18,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515087664,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515089188,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 450,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 4,
                "Value": 4,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 451,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 4866399,
                "Value": 4866399,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 452,
                "Name": "internal.metrics.executorRunTime",
                "Update": 1515,
                "Value": 1515,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 453,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 1365638209,
                "Value": 1365638209,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 454,
                "Name": "internal.metrics.resultSize",
                "Update": 1295,
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 455,
                "Name": "internal.metrics.jvmGCTime",
                "Update": 27,
                "Value": 27,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 457,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 458,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 459,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 28595476,
                "Value": 28595476,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 468,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Update": 2952678,
                "Value": 2952678,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 469,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Update": 212034,
                "Value": 212034,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 470,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Update": 3403974,
                "Value": 3403974,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 4,
        "Executor Deserialize CPU Time": 4866399,
        "Executor Run Time": 1515,
        "Executor CPU Time": 1365638209,
        "Peak Execution Memory": 28595476,
        "Result Size": 1295,
        "JVM GC Time": 27,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 0,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 0,
            "Total Records Read": 0
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 2952678,
            "Shuffle Write Time": 3403974,
            "Shuffle Records Written": 212034
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 0,
            "Records Written": 0
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 18,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 59,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"84\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515075000\",\"name\":\"foreachRDD @ 17:44:35\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    58
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 21,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515075000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:35\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 58,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"83\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515075000\",\"name\":\"foreachRDD @ 17:44:35\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    21
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515087650,
        "Completion Time": 1680515089188,
        "Accumulables": [
            {
                "ID": 450,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 4,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 451,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 4866399,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 452,
                "Name": "internal.metrics.executorRunTime",
                "Value": 1515,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 453,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 1365638209,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 454,
                "Name": "internal.metrics.resultSize",
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 455,
                "Name": "internal.metrics.jvmGCTime",
                "Value": 27,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 457,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 458,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 459,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 28595476,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 468,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Value": 2952678,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 469,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Value": 212034,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 470,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Value": 3403974,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 19,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 61,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"86\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515075000\",\"name\":\"foreachRDD @ 17:44:35\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    60
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 60,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"85\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515075000\",\"name\":\"foreachRDD @ 17:44:35\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    59
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            18
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515089189,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"86\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515075000\",\"name\":\"foreachRDD @ 17:44:35\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515075000\">[output operation 0, batch time 17:44:35]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515075000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 19,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 19,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515089212,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 19,
    "Stage Attempt ID": 0,
    "Task Type": "ResultTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 19,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515089212,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515089611,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 475,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 7,
                "Value": 7,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 476,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 7100511,
                "Value": 7100511,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 477,
                "Name": "internal.metrics.executorRunTime",
                "Update": 383,
                "Value": 383,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 478,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 311557992,
                "Value": 311557992,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 479,
                "Name": "internal.metrics.resultSize",
                "Update": 1823,
                "Value": 1823,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 480,
                "Name": "internal.metrics.jvmGCTime",
                "Update": 39,
                "Value": 39,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 482,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 483,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 484,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 15033227,
                "Value": 15033227,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 486,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 487,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Update": 1,
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 488,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 489,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 490,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Update": 2952678,
                "Value": 2952678,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 491,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 492,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Update": 212034,
                "Value": 212034,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 498,
                "Name": "internal.metrics.output.bytesWritten",
                "Update": 5056721,
                "Value": 5056721,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 499,
                "Name": "internal.metrics.output.recordsWritten",
                "Update": 212034,
                "Value": 212034,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 7,
        "Executor Deserialize CPU Time": 7100511,
        "Executor Run Time": 383,
        "Executor CPU Time": 311557992,
        "Peak Execution Memory": 15033227,
        "Result Size": 1823,
        "JVM GC Time": 39,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 1,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 2952678,
            "Total Records Read": 212034
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 0,
            "Shuffle Write Time": 0,
            "Shuffle Records Written": 0
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 5056721,
            "Records Written": 212034
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 19,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 61,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"86\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515075000\",\"name\":\"foreachRDD @ 17:44:35\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    60
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 60,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"85\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515075000\",\"name\":\"foreachRDD @ 17:44:35\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    59
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            18
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515089189,
        "Completion Time": 1680515089613,
        "Accumulables": [
            {
                "ID": 475,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 7,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 476,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 7100511,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 477,
                "Name": "internal.metrics.executorRunTime",
                "Value": 383,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 478,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 311557992,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 479,
                "Name": "internal.metrics.resultSize",
                "Value": 1823,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 480,
                "Name": "internal.metrics.jvmGCTime",
                "Value": 39,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 482,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 483,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 484,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 15033227,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 486,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 487,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 488,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 489,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 490,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Value": 2952678,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 491,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 492,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Value": 212034,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 498,
                "Name": "internal.metrics.output.bytesWritten",
                "Value": 5056721,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 499,
                "Name": "internal.metrics.output.recordsWritten",
                "Value": 212034,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerJobEnd",
    "Job ID": 9,
    "Completion Time": 1680515089613,
    "Job Result": {
        "Result": "JobSucceeded"
    }
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 20
}
{
    "Event": "SparkListenerJobStart",
    "Job ID": 10,
    "Submission Time": 1680515089690,
    "Stage Infos": [
        {
            "Stage ID": 20,
            "Stage Attempt ID": 0,
            "Stage Name": "map at main.scala:46",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 65,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"93\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515076000\",\"name\":\"foreachRDD @ 17:44:36\"}}",
                    "Callsite": "map at main.scala:46",
                    "Parent IDs": [
                        64
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 26,
                    "Name": "KafkaRDD",
                    "Scope": "{\"id\":\"0_1680515076000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:36\"}",
                    "Callsite": "createDirectStream at main.scala:31",
                    "Parent IDs": [],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 64,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"92\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515076000\",\"name\":\"foreachRDD @ 17:44:36\"}}",
                    "Callsite": "flatMap at main.scala:41",
                    "Parent IDs": [
                        26
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [],
            "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
            "Accumulables": [],
            "Resource Profile Id": 0
        },
        {
            "Stage ID": 21,
            "Stage Attempt ID": 0,
            "Stage Name": "runJob at SparkHadoopWriter.scala:83",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 67,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"95\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515076000\",\"name\":\"foreachRDD @ 17:44:36\"}}",
                    "Callsite": "saveAsTextFile at main.scala:48",
                    "Parent IDs": [
                        66
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 66,
                    "Name": "ShuffledRDD",
                    "Scope": "{\"id\":\"94\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515076000\",\"name\":\"foreachRDD @ 17:44:36\"}}",
                    "Callsite": "reduceByKey at main.scala:46",
                    "Parent IDs": [
                        65
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [
                20
            ],
            "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
            "Accumulables": [],
            "Resource Profile Id": 0
        }
    ],
    "Stage IDs": [
        20,
        21
    ],
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"95\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515076000\",\"name\":\"foreachRDD @ 17:44:36\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515076000\">[output operation 0, batch time 17:44:36]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515076000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 20,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 65,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"93\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515076000\",\"name\":\"foreachRDD @ 17:44:36\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    64
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 26,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515076000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:36\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 64,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"92\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515076000\",\"name\":\"foreachRDD @ 17:44:36\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    26
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515089694,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"95\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515076000\",\"name\":\"foreachRDD @ 17:44:36\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515076000\">[output operation 0, batch time 17:44:36]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515076000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 20,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 20,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515089703,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 20,
    "Stage Attempt ID": 0,
    "Task Type": "ShuffleMapTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 20,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515089703,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515091264,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 500,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 4,
                "Value": 4,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 501,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 4310902,
                "Value": 4310902,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 502,
                "Name": "internal.metrics.executorRunTime",
                "Update": 1552,
                "Value": 1552,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 503,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 1398438845,
                "Value": 1398438845,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 504,
                "Name": "internal.metrics.resultSize",
                "Update": 1295,
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 505,
                "Name": "internal.metrics.jvmGCTime",
                "Update": 33,
                "Value": 33,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 507,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 508,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 509,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 23724914,
                "Value": 23724914,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 518,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Update": 3004802,
                "Value": 3004802,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 519,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Update": 211569,
                "Value": 211569,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 520,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Update": 3472278,
                "Value": 3472278,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 4,
        "Executor Deserialize CPU Time": 4310902,
        "Executor Run Time": 1552,
        "Executor CPU Time": 1398438845,
        "Peak Execution Memory": 23724914,
        "Result Size": 1295,
        "JVM GC Time": 33,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 0,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 0,
            "Total Records Read": 0
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 3004802,
            "Shuffle Write Time": 3472278,
            "Shuffle Records Written": 211569
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 0,
            "Records Written": 0
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 20,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 65,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"93\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515076000\",\"name\":\"foreachRDD @ 17:44:36\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    64
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 26,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515076000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:36\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 64,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"92\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515076000\",\"name\":\"foreachRDD @ 17:44:36\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    26
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515089694,
        "Completion Time": 1680515091265,
        "Accumulables": [
            {
                "ID": 500,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 4,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 501,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 4310902,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 502,
                "Name": "internal.metrics.executorRunTime",
                "Value": 1552,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 503,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 1398438845,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 504,
                "Name": "internal.metrics.resultSize",
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 505,
                "Name": "internal.metrics.jvmGCTime",
                "Value": 33,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 507,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 508,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 509,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 23724914,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 518,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Value": 3004802,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 519,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Value": 211569,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 520,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Value": 3472278,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 21,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 67,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"95\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515076000\",\"name\":\"foreachRDD @ 17:44:36\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    66
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 66,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"94\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515076000\",\"name\":\"foreachRDD @ 17:44:36\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    65
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            20
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515091266,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"95\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515076000\",\"name\":\"foreachRDD @ 17:44:36\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515076000\">[output operation 0, batch time 17:44:36]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515076000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 21,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 21,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515091281,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 21,
    "Stage Attempt ID": 0,
    "Task Type": "ResultTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 21,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515091281,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515091625,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 525,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 8,
                "Value": 8,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 526,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 8182094,
                "Value": 8182094,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 527,
                "Name": "internal.metrics.executorRunTime",
                "Update": 332,
                "Value": 332,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 528,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 297119854,
                "Value": 297119854,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 529,
                "Name": "internal.metrics.resultSize",
                "Update": 1780,
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 532,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 533,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 534,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 19508002,
                "Value": 19508002,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 536,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 537,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Update": 1,
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 538,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 539,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 540,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Update": 3004802,
                "Value": 3004802,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 541,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 542,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Update": 211569,
                "Value": 211569,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 548,
                "Name": "internal.metrics.output.bytesWritten",
                "Update": 4969668,
                "Value": 4969668,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 549,
                "Name": "internal.metrics.output.recordsWritten",
                "Update": 211569,
                "Value": 211569,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 8,
        "Executor Deserialize CPU Time": 8182094,
        "Executor Run Time": 332,
        "Executor CPU Time": 297119854,
        "Peak Execution Memory": 19508002,
        "Result Size": 1780,
        "JVM GC Time": 0,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 1,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 3004802,
            "Total Records Read": 211569
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 0,
            "Shuffle Write Time": 0,
            "Shuffle Records Written": 0
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 4969668,
            "Records Written": 211569
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 21,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 67,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"95\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515076000\",\"name\":\"foreachRDD @ 17:44:36\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    66
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 66,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"94\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515076000\",\"name\":\"foreachRDD @ 17:44:36\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    65
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            20
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515091266,
        "Completion Time": 1680515091627,
        "Accumulables": [
            {
                "ID": 525,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 8,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 526,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 8182094,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 527,
                "Name": "internal.metrics.executorRunTime",
                "Value": 332,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 528,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 297119854,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 529,
                "Name": "internal.metrics.resultSize",
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 532,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 533,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 534,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 19508002,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 536,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 537,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 538,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 539,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 540,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Value": 3004802,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 541,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 542,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Value": 211569,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 548,
                "Name": "internal.metrics.output.bytesWritten",
                "Value": 4969668,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 549,
                "Name": "internal.metrics.output.recordsWritten",
                "Value": 211569,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerJobEnd",
    "Job ID": 10,
    "Completion Time": 1680515091627,
    "Job Result": {
        "Result": "JobSucceeded"
    }
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 21
}
{
    "Event": "SparkListenerJobStart",
    "Job ID": 11,
    "Submission Time": 1680515091711,
    "Stage Infos": [
        {
            "Stage ID": 22,
            "Stage Attempt ID": 0,
            "Stage Name": "map at main.scala:46",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 71,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"102\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515077000\",\"name\":\"foreachRDD @ 17:44:37\"}}",
                    "Callsite": "map at main.scala:46",
                    "Parent IDs": [
                        70
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 70,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"101\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515077000\",\"name\":\"foreachRDD @ 17:44:37\"}}",
                    "Callsite": "flatMap at main.scala:41",
                    "Parent IDs": [
                        27
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 27,
                    "Name": "KafkaRDD",
                    "Scope": "{\"id\":\"0_1680515077000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:37\"}",
                    "Callsite": "createDirectStream at main.scala:31",
                    "Parent IDs": [],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [],
            "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
            "Accumulables": [],
            "Resource Profile Id": 0
        },
        {
            "Stage ID": 23,
            "Stage Attempt ID": 0,
            "Stage Name": "runJob at SparkHadoopWriter.scala:83",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 73,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"104\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515077000\",\"name\":\"foreachRDD @ 17:44:37\"}}",
                    "Callsite": "saveAsTextFile at main.scala:48",
                    "Parent IDs": [
                        72
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 72,
                    "Name": "ShuffledRDD",
                    "Scope": "{\"id\":\"103\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515077000\",\"name\":\"foreachRDD @ 17:44:37\"}}",
                    "Callsite": "reduceByKey at main.scala:46",
                    "Parent IDs": [
                        71
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [
                22
            ],
            "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
            "Accumulables": [],
            "Resource Profile Id": 0
        }
    ],
    "Stage IDs": [
        22,
        23
    ],
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"104\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515077000\",\"name\":\"foreachRDD @ 17:44:37\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515077000\">[output operation 0, batch time 17:44:37]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515077000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 22,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 71,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"102\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515077000\",\"name\":\"foreachRDD @ 17:44:37\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    70
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 70,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"101\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515077000\",\"name\":\"foreachRDD @ 17:44:37\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    27
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 27,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515077000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:37\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515091713,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"104\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515077000\",\"name\":\"foreachRDD @ 17:44:37\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515077000\">[output operation 0, batch time 17:44:37]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515077000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 22,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 22,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515091717,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 22,
    "Stage Attempt ID": 0,
    "Task Type": "ShuffleMapTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 22,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515091717,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515093485,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 550,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 3,
                "Value": 3,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 551,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 3743772,
                "Value": 3743772,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 552,
                "Name": "internal.metrics.executorRunTime",
                "Update": 1760,
                "Value": 1760,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 553,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 1561213813,
                "Value": 1561213813,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 554,
                "Name": "internal.metrics.resultSize",
                "Update": 1295,
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 555,
                "Name": "internal.metrics.jvmGCTime",
                "Update": 60,
                "Value": 60,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 557,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 558,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 559,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 26003444,
                "Value": 26003444,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 568,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Update": 2950579,
                "Value": 2950579,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 569,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Update": 219708,
                "Value": 219708,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 570,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Update": 3441518,
                "Value": 3441518,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 936378368,
        "JVMOffHeapMemory": 129262376,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 175825,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 175825,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 35660479,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 29,
        "MinorGCTime": 631,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 631
    },
    "Task Metrics": {
        "Executor Deserialize Time": 3,
        "Executor Deserialize CPU Time": 3743772,
        "Executor Run Time": 1760,
        "Executor CPU Time": 1561213813,
        "Peak Execution Memory": 26003444,
        "Result Size": 1295,
        "JVM GC Time": 60,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 0,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 0,
            "Total Records Read": 0
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 2950579,
            "Shuffle Write Time": 3441518,
            "Shuffle Records Written": 219708
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 0,
            "Records Written": 0
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 22,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 71,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"102\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515077000\",\"name\":\"foreachRDD @ 17:44:37\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    70
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 70,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"101\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515077000\",\"name\":\"foreachRDD @ 17:44:37\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    27
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 27,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515077000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:37\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515091713,
        "Completion Time": 1680515093485,
        "Accumulables": [
            {
                "ID": 550,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 3,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 551,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 3743772,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 552,
                "Name": "internal.metrics.executorRunTime",
                "Value": 1760,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 553,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 1561213813,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 554,
                "Name": "internal.metrics.resultSize",
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 555,
                "Name": "internal.metrics.jvmGCTime",
                "Value": 60,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 557,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 558,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 559,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 26003444,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 568,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Value": 2950579,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 569,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Value": 219708,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 570,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Value": 3441518,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 23,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 73,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"104\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515077000\",\"name\":\"foreachRDD @ 17:44:37\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    72
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 72,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"103\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515077000\",\"name\":\"foreachRDD @ 17:44:37\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    71
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            22
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515093486,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"104\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515077000\",\"name\":\"foreachRDD @ 17:44:37\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515077000\">[output operation 0, batch time 17:44:37]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515077000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 23,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 23,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515093504,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 23,
    "Stage Attempt ID": 0,
    "Task Type": "ResultTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 23,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515093504,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515093871,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 575,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 12,
                "Value": 12,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 576,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 12309167,
                "Value": 12309167,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 577,
                "Name": "internal.metrics.executorRunTime",
                "Update": 350,
                "Value": 350,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 578,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 315642544,
                "Value": 315642544,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 579,
                "Name": "internal.metrics.resultSize",
                "Update": 1780,
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 582,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 583,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 584,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 14195962,
                "Value": 14195962,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 586,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 587,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Update": 1,
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 588,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 589,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 590,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Update": 2950579,
                "Value": 2950579,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 591,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 592,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Update": 219708,
                "Value": 219708,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 598,
                "Name": "internal.metrics.output.bytesWritten",
                "Update": 4807186,
                "Value": 4807186,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 599,
                "Name": "internal.metrics.output.recordsWritten",
                "Update": 219708,
                "Value": 219708,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 12,
        "Executor Deserialize CPU Time": 12309167,
        "Executor Run Time": 350,
        "Executor CPU Time": 315642544,
        "Peak Execution Memory": 14195962,
        "Result Size": 1780,
        "JVM GC Time": 0,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 1,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 2950579,
            "Total Records Read": 219708
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 0,
            "Shuffle Write Time": 0,
            "Shuffle Records Written": 0
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 4807186,
            "Records Written": 219708
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 23,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 73,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"104\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515077000\",\"name\":\"foreachRDD @ 17:44:37\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    72
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 72,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"103\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515077000\",\"name\":\"foreachRDD @ 17:44:37\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    71
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            22
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515093486,
        "Completion Time": 1680515093873,
        "Accumulables": [
            {
                "ID": 575,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 12,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 576,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 12309167,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 577,
                "Name": "internal.metrics.executorRunTime",
                "Value": 350,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 578,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 315642544,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 579,
                "Name": "internal.metrics.resultSize",
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 582,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 583,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 584,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 14195962,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 586,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 587,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 588,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 589,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 590,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Value": 2950579,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 591,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 592,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Value": 219708,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 598,
                "Name": "internal.metrics.output.bytesWritten",
                "Value": 4807186,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 599,
                "Name": "internal.metrics.output.recordsWritten",
                "Value": 219708,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerJobEnd",
    "Job ID": 11,
    "Completion Time": 1680515093873,
    "Job Result": {
        "Result": "JobSucceeded"
    }
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 26
}
{
    "Event": "SparkListenerJobStart",
    "Job ID": 12,
    "Submission Time": 1680515093940,
    "Stage Infos": [
        {
            "Stage ID": 24,
            "Stage Attempt ID": 0,
            "Stage Name": "map at main.scala:46",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 77,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"111\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515078000\",\"name\":\"foreachRDD @ 17:44:38\"}}",
                    "Callsite": "map at main.scala:46",
                    "Parent IDs": [
                        76
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 76,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"110\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515078000\",\"name\":\"foreachRDD @ 17:44:38\"}}",
                    "Callsite": "flatMap at main.scala:41",
                    "Parent IDs": [
                        32
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 32,
                    "Name": "KafkaRDD",
                    "Scope": "{\"id\":\"0_1680515078000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:38\"}",
                    "Callsite": "createDirectStream at main.scala:31",
                    "Parent IDs": [],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [],
            "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
            "Accumulables": [],
            "Resource Profile Id": 0
        },
        {
            "Stage ID": 25,
            "Stage Attempt ID": 0,
            "Stage Name": "runJob at SparkHadoopWriter.scala:83",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 79,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"113\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515078000\",\"name\":\"foreachRDD @ 17:44:38\"}}",
                    "Callsite": "saveAsTextFile at main.scala:48",
                    "Parent IDs": [
                        78
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 78,
                    "Name": "ShuffledRDD",
                    "Scope": "{\"id\":\"112\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515078000\",\"name\":\"foreachRDD @ 17:44:38\"}}",
                    "Callsite": "reduceByKey at main.scala:46",
                    "Parent IDs": [
                        77
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [
                24
            ],
            "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
            "Accumulables": [],
            "Resource Profile Id": 0
        }
    ],
    "Stage IDs": [
        24,
        25
    ],
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"113\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515078000\",\"name\":\"foreachRDD @ 17:44:38\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515078000\">[output operation 0, batch time 17:44:38]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515078000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 24,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 77,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"111\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515078000\",\"name\":\"foreachRDD @ 17:44:38\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    76
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 76,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"110\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515078000\",\"name\":\"foreachRDD @ 17:44:38\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    32
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 32,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515078000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:38\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515093942,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"113\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515078000\",\"name\":\"foreachRDD @ 17:44:38\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515078000\">[output operation 0, batch time 17:44:38]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515078000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 24,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 24,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515093948,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 24,
    "Stage Attempt ID": 0,
    "Task Type": "ShuffleMapTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 24,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515093948,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515095783,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 600,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 4,
                "Value": 4,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 601,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 4727635,
                "Value": 4727635,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 602,
                "Name": "internal.metrics.executorRunTime",
                "Update": 1826,
                "Value": 1826,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 603,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 1628825004,
                "Value": 1628825004,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 604,
                "Name": "internal.metrics.resultSize",
                "Update": 1295,
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 605,
                "Name": "internal.metrics.jvmGCTime",
                "Update": 55,
                "Value": 55,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 607,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 608,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 609,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 21717380,
                "Value": 21717380,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 618,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Update": 3284556,
                "Value": 3284556,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 619,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Update": 236117,
                "Value": 236117,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 620,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Update": 3992390,
                "Value": 3992390,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 4,
        "Executor Deserialize CPU Time": 4727635,
        "Executor Run Time": 1826,
        "Executor CPU Time": 1628825004,
        "Peak Execution Memory": 21717380,
        "Result Size": 1295,
        "JVM GC Time": 55,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 0,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 0,
            "Total Records Read": 0
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 3284556,
            "Shuffle Write Time": 3992390,
            "Shuffle Records Written": 236117
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 0,
            "Records Written": 0
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 24,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 77,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"111\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515078000\",\"name\":\"foreachRDD @ 17:44:38\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    76
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 76,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"110\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515078000\",\"name\":\"foreachRDD @ 17:44:38\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    32
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 32,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515078000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:38\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515093942,
        "Completion Time": 1680515095783,
        "Accumulables": [
            {
                "ID": 600,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 4,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 601,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 4727635,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 602,
                "Name": "internal.metrics.executorRunTime",
                "Value": 1826,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 603,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 1628825004,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 604,
                "Name": "internal.metrics.resultSize",
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 605,
                "Name": "internal.metrics.jvmGCTime",
                "Value": 55,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 607,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 608,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 609,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 21717380,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 618,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Value": 3284556,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 619,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Value": 236117,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 620,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Value": 3992390,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 25,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 79,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"113\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515078000\",\"name\":\"foreachRDD @ 17:44:38\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    78
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 78,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"112\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515078000\",\"name\":\"foreachRDD @ 17:44:38\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    77
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            24
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515095784,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"113\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515078000\",\"name\":\"foreachRDD @ 17:44:38\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515078000\">[output operation 0, batch time 17:44:38]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515078000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 25,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 25,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515095797,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 25,
    "Stage Attempt ID": 0,
    "Task Type": "ResultTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 25,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515095797,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515096178,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 625,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 13,
                "Value": 13,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 626,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 13383564,
                "Value": 13383564,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 627,
                "Name": "internal.metrics.executorRunTime",
                "Update": 363,
                "Value": 363,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 628,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 327813599,
                "Value": 327813599,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 629,
                "Name": "internal.metrics.resultSize",
                "Update": 1780,
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 632,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 633,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 634,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 22463930,
                "Value": 22463930,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 636,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 637,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Update": 1,
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 638,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 639,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 640,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Update": 3284556,
                "Value": 3284556,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 641,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 642,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Update": 236117,
                "Value": 236117,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 648,
                "Name": "internal.metrics.output.bytesWritten",
                "Update": 5276338,
                "Value": 5276338,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 649,
                "Name": "internal.metrics.output.recordsWritten",
                "Update": 236117,
                "Value": 236117,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 13,
        "Executor Deserialize CPU Time": 13383564,
        "Executor Run Time": 363,
        "Executor CPU Time": 327813599,
        "Peak Execution Memory": 22463930,
        "Result Size": 1780,
        "JVM GC Time": 0,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 1,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 3284556,
            "Total Records Read": 236117
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 0,
            "Shuffle Write Time": 0,
            "Shuffle Records Written": 0
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 5276338,
            "Records Written": 236117
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 25,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 79,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"113\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515078000\",\"name\":\"foreachRDD @ 17:44:38\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    78
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 78,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"112\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515078000\",\"name\":\"foreachRDD @ 17:44:38\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    77
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            24
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515095784,
        "Completion Time": 1680515096178,
        "Accumulables": [
            {
                "ID": 625,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 13,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 626,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 13383564,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 627,
                "Name": "internal.metrics.executorRunTime",
                "Value": 363,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 628,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 327813599,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 629,
                "Name": "internal.metrics.resultSize",
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 632,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 633,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 634,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 22463930,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 636,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 637,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 638,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 639,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 640,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Value": 3284556,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 641,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 642,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Value": 236117,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 648,
                "Name": "internal.metrics.output.bytesWritten",
                "Value": 5276338,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 649,
                "Name": "internal.metrics.output.recordsWritten",
                "Value": 236117,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerJobEnd",
    "Job ID": 12,
    "Completion Time": 1680515096179,
    "Job Result": {
        "Result": "JobSucceeded"
    }
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 27
}
{
    "Event": "SparkListenerJobStart",
    "Job ID": 13,
    "Submission Time": 1680515096261,
    "Stage Infos": [
        {
            "Stage ID": 26,
            "Stage Attempt ID": 0,
            "Stage Name": "map at main.scala:46",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 84,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"120\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515079000\",\"name\":\"foreachRDD @ 17:44:39\"}}",
                    "Callsite": "map at main.scala:46",
                    "Parent IDs": [
                        83
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 83,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"119\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515079000\",\"name\":\"foreachRDD @ 17:44:39\"}}",
                    "Callsite": "flatMap at main.scala:41",
                    "Parent IDs": [
                        33
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 33,
                    "Name": "KafkaRDD",
                    "Scope": "{\"id\":\"0_1680515079000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:39\"}",
                    "Callsite": "createDirectStream at main.scala:31",
                    "Parent IDs": [],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [],
            "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
            "Accumulables": [],
            "Resource Profile Id": 0
        },
        {
            "Stage ID": 27,
            "Stage Attempt ID": 0,
            "Stage Name": "runJob at SparkHadoopWriter.scala:83",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 86,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"122\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515079000\",\"name\":\"foreachRDD @ 17:44:39\"}}",
                    "Callsite": "saveAsTextFile at main.scala:48",
                    "Parent IDs": [
                        85
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 85,
                    "Name": "ShuffledRDD",
                    "Scope": "{\"id\":\"121\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515079000\",\"name\":\"foreachRDD @ 17:44:39\"}}",
                    "Callsite": "reduceByKey at main.scala:46",
                    "Parent IDs": [
                        84
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [
                26
            ],
            "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
            "Accumulables": [],
            "Resource Profile Id": 0
        }
    ],
    "Stage IDs": [
        26,
        27
    ],
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"122\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515079000\",\"name\":\"foreachRDD @ 17:44:39\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515079000\">[output operation 0, batch time 17:44:39]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515079000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 26,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 84,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"120\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515079000\",\"name\":\"foreachRDD @ 17:44:39\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    83
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 83,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"119\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515079000\",\"name\":\"foreachRDD @ 17:44:39\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    33
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 33,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515079000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:39\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515096262,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"122\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515079000\",\"name\":\"foreachRDD @ 17:44:39\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515079000\">[output operation 0, batch time 17:44:39]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515079000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 26,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 26,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515096267,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 26,
    "Stage Attempt ID": 0,
    "Task Type": "ShuffleMapTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 26,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515096267,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515098434,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 650,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 4,
                "Value": 4,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 651,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 4424272,
                "Value": 4424272,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 652,
                "Name": "internal.metrics.executorRunTime",
                "Update": 2160,
                "Value": 2160,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 653,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 1983690302,
                "Value": 1983690302,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 654,
                "Name": "internal.metrics.resultSize",
                "Update": 1295,
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 655,
                "Name": "internal.metrics.jvmGCTime",
                "Update": 72,
                "Value": 72,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 657,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 658,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 659,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 21844708,
                "Value": 21844708,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 668,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Update": 2865462,
                "Value": 2865462,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 669,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Update": 234824,
                "Value": 234824,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 670,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Update": 3381924,
                "Value": 3381924,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 4,
        "Executor Deserialize CPU Time": 4424272,
        "Executor Run Time": 2160,
        "Executor CPU Time": 1983690302,
        "Peak Execution Memory": 21844708,
        "Result Size": 1295,
        "JVM GC Time": 72,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 0,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 0,
            "Total Records Read": 0
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 2865462,
            "Shuffle Write Time": 3381924,
            "Shuffle Records Written": 234824
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 0,
            "Records Written": 0
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 26,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 84,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"120\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515079000\",\"name\":\"foreachRDD @ 17:44:39\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    83
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 83,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"119\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515079000\",\"name\":\"foreachRDD @ 17:44:39\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    33
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 33,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515079000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:39\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515096262,
        "Completion Time": 1680515098435,
        "Accumulables": [
            {
                "ID": 650,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 4,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 651,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 4424272,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 652,
                "Name": "internal.metrics.executorRunTime",
                "Value": 2160,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 653,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 1983690302,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 654,
                "Name": "internal.metrics.resultSize",
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 655,
                "Name": "internal.metrics.jvmGCTime",
                "Value": 72,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 657,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 658,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 659,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 21844708,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 668,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Value": 2865462,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 669,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Value": 234824,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 670,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Value": 3381924,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 27,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 86,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"122\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515079000\",\"name\":\"foreachRDD @ 17:44:39\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    85
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 85,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"121\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515079000\",\"name\":\"foreachRDD @ 17:44:39\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    84
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            26
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515098436,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"122\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515079000\",\"name\":\"foreachRDD @ 17:44:39\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515079000\">[output operation 0, batch time 17:44:39]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515079000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 27,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 27,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515098469,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 27,
    "Stage Attempt ID": 0,
    "Task Type": "ResultTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 27,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515098469,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515098823,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 675,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 8,
                "Value": 8,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 676,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 8262138,
                "Value": 8262138,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 677,
                "Name": "internal.metrics.executorRunTime",
                "Update": 343,
                "Value": 343,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 678,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 311878435,
                "Value": 311878435,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 679,
                "Name": "internal.metrics.resultSize",
                "Update": 1780,
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 682,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 683,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 684,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 15158493,
                "Value": 15158493,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 686,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 687,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Update": 1,
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 688,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 689,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 690,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Update": 2865462,
                "Value": 2865462,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 691,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 692,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Update": 234824,
                "Value": 234824,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 698,
                "Name": "internal.metrics.output.bytesWritten",
                "Update": 4536393,
                "Value": 4536393,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 699,
                "Name": "internal.metrics.output.recordsWritten",
                "Update": 234824,
                "Value": 234824,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 8,
        "Executor Deserialize CPU Time": 8262138,
        "Executor Run Time": 343,
        "Executor CPU Time": 311878435,
        "Peak Execution Memory": 15158493,
        "Result Size": 1780,
        "JVM GC Time": 0,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 1,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 2865462,
            "Total Records Read": 234824
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 0,
            "Shuffle Write Time": 0,
            "Shuffle Records Written": 0
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 4536393,
            "Records Written": 234824
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 27,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 86,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"122\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515079000\",\"name\":\"foreachRDD @ 17:44:39\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    85
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 85,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"121\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515079000\",\"name\":\"foreachRDD @ 17:44:39\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    84
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            26
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515098436,
        "Completion Time": 1680515098825,
        "Accumulables": [
            {
                "ID": 675,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 8,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 676,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 8262138,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 677,
                "Name": "internal.metrics.executorRunTime",
                "Value": 343,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 678,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 311878435,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 679,
                "Name": "internal.metrics.resultSize",
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 682,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 683,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 684,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 15158493,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 686,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 687,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 688,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 689,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 690,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Value": 2865462,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 691,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 692,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Value": 234824,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 698,
                "Name": "internal.metrics.output.bytesWritten",
                "Value": 4536393,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 699,
                "Name": "internal.metrics.output.recordsWritten",
                "Value": 234824,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerJobEnd",
    "Job ID": 13,
    "Completion Time": 1680515098826,
    "Job Result": {
        "Result": "JobSucceeded"
    }
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 32
}
{
    "Event": "SparkListenerJobStart",
    "Job ID": 14,
    "Submission Time": 1680515098904,
    "Stage Infos": [
        {
            "Stage ID": 28,
            "Stage Attempt ID": 0,
            "Stage Name": "map at main.scala:46",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 90,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"129\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515080000\",\"name\":\"foreachRDD @ 17:44:40\"}}",
                    "Callsite": "map at main.scala:46",
                    "Parent IDs": [
                        89
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 38,
                    "Name": "KafkaRDD",
                    "Scope": "{\"id\":\"0_1680515080000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:40\"}",
                    "Callsite": "createDirectStream at main.scala:31",
                    "Parent IDs": [],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 89,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"128\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515080000\",\"name\":\"foreachRDD @ 17:44:40\"}}",
                    "Callsite": "flatMap at main.scala:41",
                    "Parent IDs": [
                        38
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [],
            "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
            "Accumulables": [],
            "Resource Profile Id": 0
        },
        {
            "Stage ID": 29,
            "Stage Attempt ID": 0,
            "Stage Name": "runJob at SparkHadoopWriter.scala:83",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 92,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"131\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515080000\",\"name\":\"foreachRDD @ 17:44:40\"}}",
                    "Callsite": "saveAsTextFile at main.scala:48",
                    "Parent IDs": [
                        91
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 91,
                    "Name": "ShuffledRDD",
                    "Scope": "{\"id\":\"130\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515080000\",\"name\":\"foreachRDD @ 17:44:40\"}}",
                    "Callsite": "reduceByKey at main.scala:46",
                    "Parent IDs": [
                        90
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [
                28
            ],
            "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
            "Accumulables": [],
            "Resource Profile Id": 0
        }
    ],
    "Stage IDs": [
        28,
        29
    ],
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"131\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515080000\",\"name\":\"foreachRDD @ 17:44:40\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515080000\">[output operation 0, batch time 17:44:40]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515080000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 28,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 90,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"129\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515080000\",\"name\":\"foreachRDD @ 17:44:40\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    89
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 38,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515080000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:40\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 89,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"128\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515080000\",\"name\":\"foreachRDD @ 17:44:40\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    38
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515098912,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"131\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515080000\",\"name\":\"foreachRDD @ 17:44:40\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515080000\">[output operation 0, batch time 17:44:40]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515080000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 28,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 28,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515098921,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 28,
    "Stage Attempt ID": 0,
    "Task Type": "ShuffleMapTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 28,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515098921,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515100682,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 700,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 3,
                "Value": 3,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 701,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 3640620,
                "Value": 3640620,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 702,
                "Name": "internal.metrics.executorRunTime",
                "Update": 1754,
                "Value": 1754,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 703,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 1584929101,
                "Value": 1584929101,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 704,
                "Name": "internal.metrics.resultSize",
                "Update": 1295,
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 705,
                "Name": "internal.metrics.jvmGCTime",
                "Update": 20,
                "Value": 20,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 707,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 708,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 709,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 28352400,
                "Value": 28352400,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 718,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Update": 2951471,
                "Value": 2951471,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 719,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Update": 241934,
                "Value": 241934,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 720,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Update": 3317372,
                "Value": 3317372,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 3,
        "Executor Deserialize CPU Time": 3640620,
        "Executor Run Time": 1754,
        "Executor CPU Time": 1584929101,
        "Peak Execution Memory": 28352400,
        "Result Size": 1295,
        "JVM GC Time": 20,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 0,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 0,
            "Total Records Read": 0
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 2951471,
            "Shuffle Write Time": 3317372,
            "Shuffle Records Written": 241934
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 0,
            "Records Written": 0
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 28,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 90,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"129\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515080000\",\"name\":\"foreachRDD @ 17:44:40\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    89
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 38,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515080000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:40\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 89,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"128\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515080000\",\"name\":\"foreachRDD @ 17:44:40\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    38
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515098912,
        "Completion Time": 1680515100683,
        "Accumulables": [
            {
                "ID": 700,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 3,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 701,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 3640620,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 702,
                "Name": "internal.metrics.executorRunTime",
                "Value": 1754,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 703,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 1584929101,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 704,
                "Name": "internal.metrics.resultSize",
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 705,
                "Name": "internal.metrics.jvmGCTime",
                "Value": 20,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 707,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 708,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 709,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 28352400,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 718,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Value": 2951471,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 719,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Value": 241934,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 720,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Value": 3317372,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 29,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 92,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"131\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515080000\",\"name\":\"foreachRDD @ 17:44:40\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    91
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 91,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"130\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515080000\",\"name\":\"foreachRDD @ 17:44:40\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    90
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            28
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515100684,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"131\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515080000\",\"name\":\"foreachRDD @ 17:44:40\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515080000\">[output operation 0, batch time 17:44:40]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515080000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 29,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 29,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515100724,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 29,
    "Stage Attempt ID": 0,
    "Task Type": "ResultTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 29,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515100724,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515101122,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 725,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 6,
                "Value": 6,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 726,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 5977821,
                "Value": 5977821,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 727,
                "Name": "internal.metrics.executorRunTime",
                "Update": 388,
                "Value": 388,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 728,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 330343776,
                "Value": 330343776,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 729,
                "Name": "internal.metrics.resultSize",
                "Update": 1823,
                "Value": 1823,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 730,
                "Name": "internal.metrics.jvmGCTime",
                "Update": 25,
                "Value": 25,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 732,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 733,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 734,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 18345067,
                "Value": 18345067,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 736,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 737,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Update": 1,
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 738,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 739,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 740,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Update": 2951471,
                "Value": 2951471,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 741,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 742,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Update": 241934,
                "Value": 241934,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 748,
                "Name": "internal.metrics.output.bytesWritten",
                "Update": 4653449,
                "Value": 4653449,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 749,
                "Name": "internal.metrics.output.recordsWritten",
                "Update": 241934,
                "Value": 241934,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 6,
        "Executor Deserialize CPU Time": 5977821,
        "Executor Run Time": 388,
        "Executor CPU Time": 330343776,
        "Peak Execution Memory": 18345067,
        "Result Size": 1823,
        "JVM GC Time": 25,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 1,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 2951471,
            "Total Records Read": 241934
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 0,
            "Shuffle Write Time": 0,
            "Shuffle Records Written": 0
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 4653449,
            "Records Written": 241934
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 29,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 92,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"131\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515080000\",\"name\":\"foreachRDD @ 17:44:40\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    91
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 91,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"130\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515080000\",\"name\":\"foreachRDD @ 17:44:40\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    90
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            28
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515100684,
        "Completion Time": 1680515101123,
        "Accumulables": [
            {
                "ID": 725,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 6,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 726,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 5977821,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 727,
                "Name": "internal.metrics.executorRunTime",
                "Value": 388,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 728,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 330343776,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 729,
                "Name": "internal.metrics.resultSize",
                "Value": 1823,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 730,
                "Name": "internal.metrics.jvmGCTime",
                "Value": 25,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 732,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 733,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 734,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 18345067,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 736,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 737,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 738,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 739,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 740,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Value": 2951471,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 741,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 742,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Value": 241934,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 748,
                "Name": "internal.metrics.output.bytesWritten",
                "Value": 4653449,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 749,
                "Name": "internal.metrics.output.recordsWritten",
                "Value": 241934,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerJobEnd",
    "Job ID": 14,
    "Completion Time": 1680515101124,
    "Job Result": {
        "Result": "JobSucceeded"
    }
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 33
}
{
    "Event": "SparkListenerJobStart",
    "Job ID": 15,
    "Submission Time": 1680515101204,
    "Stage Infos": [
        {
            "Stage ID": 30,
            "Stage Attempt ID": 0,
            "Stage Name": "map at main.scala:46",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 97,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"138\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515081000\",\"name\":\"foreachRDD @ 17:44:41\"}}",
                    "Callsite": "map at main.scala:46",
                    "Parent IDs": [
                        96
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 39,
                    "Name": "KafkaRDD",
                    "Scope": "{\"id\":\"0_1680515081000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:41\"}",
                    "Callsite": "createDirectStream at main.scala:31",
                    "Parent IDs": [],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 96,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"137\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515081000\",\"name\":\"foreachRDD @ 17:44:41\"}}",
                    "Callsite": "flatMap at main.scala:41",
                    "Parent IDs": [
                        39
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [],
            "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
            "Accumulables": [],
            "Resource Profile Id": 0
        },
        {
            "Stage ID": 31,
            "Stage Attempt ID": 0,
            "Stage Name": "runJob at SparkHadoopWriter.scala:83",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 99,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"140\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515081000\",\"name\":\"foreachRDD @ 17:44:41\"}}",
                    "Callsite": "saveAsTextFile at main.scala:48",
                    "Parent IDs": [
                        98
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 98,
                    "Name": "ShuffledRDD",
                    "Scope": "{\"id\":\"139\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515081000\",\"name\":\"foreachRDD @ 17:44:41\"}}",
                    "Callsite": "reduceByKey at main.scala:46",
                    "Parent IDs": [
                        97
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [
                30
            ],
            "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
            "Accumulables": [],
            "Resource Profile Id": 0
        }
    ],
    "Stage IDs": [
        30,
        31
    ],
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"140\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515081000\",\"name\":\"foreachRDD @ 17:44:41\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515081000\">[output operation 0, batch time 17:44:41]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515081000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 30,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 97,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"138\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515081000\",\"name\":\"foreachRDD @ 17:44:41\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    96
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 39,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515081000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:41\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 96,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"137\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515081000\",\"name\":\"foreachRDD @ 17:44:41\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    39
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515101207,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"140\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515081000\",\"name\":\"foreachRDD @ 17:44:41\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515081000\">[output operation 0, batch time 17:44:41]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515081000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 30,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 30,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515101217,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 30,
    "Stage Attempt ID": 0,
    "Task Type": "ShuffleMapTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 30,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515101217,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515102998,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 750,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 3,
                "Value": 3,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 751,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 3672736,
                "Value": 3672736,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 752,
                "Name": "internal.metrics.executorRunTime",
                "Update": 1773,
                "Value": 1773,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 753,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 1590834661,
                "Value": 1590834661,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 754,
                "Name": "internal.metrics.resultSize",
                "Update": 1295,
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 755,
                "Name": "internal.metrics.jvmGCTime",
                "Update": 30,
                "Value": 30,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 757,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 758,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 759,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 17280869,
                "Value": 17280869,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 768,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Update": 3249258,
                "Value": 3249258,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 769,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Update": 241713,
                "Value": 241713,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 770,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Update": 4006226,
                "Value": 4006226,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 654863328,
        "JVMOffHeapMemory": 130192216,
        "OnHeapExecutionMemory": 15728690,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 165256,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 15893946,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 35660479,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 40,
        "MinorGCTime": 863,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 863
    },
    "Task Metrics": {
        "Executor Deserialize Time": 3,
        "Executor Deserialize CPU Time": 3672736,
        "Executor Run Time": 1773,
        "Executor CPU Time": 1590834661,
        "Peak Execution Memory": 17280869,
        "Result Size": 1295,
        "JVM GC Time": 30,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 0,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 0,
            "Total Records Read": 0
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 3249258,
            "Shuffle Write Time": 4006226,
            "Shuffle Records Written": 241713
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 0,
            "Records Written": 0
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 30,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 97,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"138\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515081000\",\"name\":\"foreachRDD @ 17:44:41\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    96
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 39,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515081000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:41\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 96,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"137\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515081000\",\"name\":\"foreachRDD @ 17:44:41\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    39
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515101207,
        "Completion Time": 1680515102998,
        "Accumulables": [
            {
                "ID": 750,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 3,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 751,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 3672736,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 752,
                "Name": "internal.metrics.executorRunTime",
                "Value": 1773,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 753,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 1590834661,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 754,
                "Name": "internal.metrics.resultSize",
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 755,
                "Name": "internal.metrics.jvmGCTime",
                "Value": 30,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 757,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 758,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 759,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 17280869,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 768,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Value": 3249258,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 769,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Value": 241713,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 770,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Value": 4006226,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 31,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 99,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"140\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515081000\",\"name\":\"foreachRDD @ 17:44:41\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    98
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 98,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"139\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515081000\",\"name\":\"foreachRDD @ 17:44:41\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    97
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            30
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515102999,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"140\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515081000\",\"name\":\"foreachRDD @ 17:44:41\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515081000\">[output operation 0, batch time 17:44:41]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515081000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 31,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 31,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515103037,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 31,
    "Stage Attempt ID": 0,
    "Task Type": "ResultTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 31,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515103037,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515103409,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 775,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 9,
                "Value": 9,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 776,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 9808645,
                "Value": 9808645,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 777,
                "Name": "internal.metrics.executorRunTime",
                "Update": 356,
                "Value": 356,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 778,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 323631233,
                "Value": 323631233,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 779,
                "Name": "internal.metrics.resultSize",
                "Update": 1780,
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 782,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 783,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 784,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 17759012,
                "Value": 17759012,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 786,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 787,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Update": 1,
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 788,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 789,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 790,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Update": 3249258,
                "Value": 3249258,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 791,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 792,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Update": 241713,
                "Value": 241713,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 798,
                "Name": "internal.metrics.output.bytesWritten",
                "Update": 5141573,
                "Value": 5141573,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 799,
                "Name": "internal.metrics.output.recordsWritten",
                "Update": 241713,
                "Value": 241713,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 9,
        "Executor Deserialize CPU Time": 9808645,
        "Executor Run Time": 356,
        "Executor CPU Time": 323631233,
        "Peak Execution Memory": 17759012,
        "Result Size": 1780,
        "JVM GC Time": 0,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 1,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 3249258,
            "Total Records Read": 241713
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 0,
            "Shuffle Write Time": 0,
            "Shuffle Records Written": 0
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 5141573,
            "Records Written": 241713
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 31,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 99,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"140\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515081000\",\"name\":\"foreachRDD @ 17:44:41\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    98
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 98,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"139\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515081000\",\"name\":\"foreachRDD @ 17:44:41\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    97
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            30
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515102999,
        "Completion Time": 1680515103410,
        "Accumulables": [
            {
                "ID": 775,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 9,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 776,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 9808645,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 777,
                "Name": "internal.metrics.executorRunTime",
                "Value": 356,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 778,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 323631233,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 779,
                "Name": "internal.metrics.resultSize",
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 782,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 783,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 784,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 17759012,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 786,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 787,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 788,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 789,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 790,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Value": 3249258,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 791,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 792,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Value": 241713,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 798,
                "Name": "internal.metrics.output.bytesWritten",
                "Value": 5141573,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 799,
                "Name": "internal.metrics.output.recordsWritten",
                "Value": 241713,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerJobEnd",
    "Job ID": 15,
    "Completion Time": 1680515103411,
    "Job Result": {
        "Result": "JobSucceeded"
    }
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 38
}
{
    "Event": "SparkListenerJobStart",
    "Job ID": 16,
    "Submission Time": 1680515103482,
    "Stage Infos": [
        {
            "Stage ID": 32,
            "Stage Attempt ID": 0,
            "Stage Name": "map at main.scala:46",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 103,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"147\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515082000\",\"name\":\"foreachRDD @ 17:44:42\"}}",
                    "Callsite": "map at main.scala:46",
                    "Parent IDs": [
                        102
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 102,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"146\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515082000\",\"name\":\"foreachRDD @ 17:44:42\"}}",
                    "Callsite": "flatMap at main.scala:41",
                    "Parent IDs": [
                        44
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 44,
                    "Name": "KafkaRDD",
                    "Scope": "{\"id\":\"0_1680515082000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:42\"}",
                    "Callsite": "createDirectStream at main.scala:31",
                    "Parent IDs": [],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [],
            "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
            "Accumulables": [],
            "Resource Profile Id": 0
        },
        {
            "Stage ID": 33,
            "Stage Attempt ID": 0,
            "Stage Name": "runJob at SparkHadoopWriter.scala:83",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 105,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"149\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515082000\",\"name\":\"foreachRDD @ 17:44:42\"}}",
                    "Callsite": "saveAsTextFile at main.scala:48",
                    "Parent IDs": [
                        104
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 104,
                    "Name": "ShuffledRDD",
                    "Scope": "{\"id\":\"148\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515082000\",\"name\":\"foreachRDD @ 17:44:42\"}}",
                    "Callsite": "reduceByKey at main.scala:46",
                    "Parent IDs": [
                        103
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [
                32
            ],
            "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
            "Accumulables": [],
            "Resource Profile Id": 0
        }
    ],
    "Stage IDs": [
        32,
        33
    ],
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"149\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515082000\",\"name\":\"foreachRDD @ 17:44:42\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515082000\">[output operation 0, batch time 17:44:42]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515082000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 32,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 103,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"147\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515082000\",\"name\":\"foreachRDD @ 17:44:42\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    102
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 102,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"146\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515082000\",\"name\":\"foreachRDD @ 17:44:42\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    44
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 44,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515082000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:42\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515103486,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"149\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515082000\",\"name\":\"foreachRDD @ 17:44:42\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515082000\">[output operation 0, batch time 17:44:42]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515082000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 32,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 32,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515103494,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 32,
    "Stage Attempt ID": 0,
    "Task Type": "ShuffleMapTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 32,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515103494,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515105277,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 800,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 3,
                "Value": 3,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 801,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 3908010,
                "Value": 3908010,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 802,
                "Name": "internal.metrics.executorRunTime",
                "Update": 1775,
                "Value": 1775,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 803,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 1559868425,
                "Value": 1559868425,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 804,
                "Name": "internal.metrics.resultSize",
                "Update": 1295,
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 805,
                "Name": "internal.metrics.jvmGCTime",
                "Update": 72,
                "Value": 72,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 807,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 808,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 809,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 34703679,
                "Value": 34703679,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 818,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Update": 3130534,
                "Value": 3130534,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 819,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Update": 244942,
                "Value": 244942,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 820,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Update": 3431740,
                "Value": 3431740,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 3,
        "Executor Deserialize CPU Time": 3908010,
        "Executor Run Time": 1775,
        "Executor CPU Time": 1559868425,
        "Peak Execution Memory": 34703679,
        "Result Size": 1295,
        "JVM GC Time": 72,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 0,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 0,
            "Total Records Read": 0
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 3130534,
            "Shuffle Write Time": 3431740,
            "Shuffle Records Written": 244942
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 0,
            "Records Written": 0
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 32,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 103,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"147\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515082000\",\"name\":\"foreachRDD @ 17:44:42\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    102
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 102,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"146\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515082000\",\"name\":\"foreachRDD @ 17:44:42\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    44
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 44,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515082000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:42\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515103486,
        "Completion Time": 1680515105278,
        "Accumulables": [
            {
                "ID": 800,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 3,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 801,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 3908010,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 802,
                "Name": "internal.metrics.executorRunTime",
                "Value": 1775,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 803,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 1559868425,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 804,
                "Name": "internal.metrics.resultSize",
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 805,
                "Name": "internal.metrics.jvmGCTime",
                "Value": 72,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 807,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 808,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 809,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 34703679,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 818,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Value": 3130534,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 819,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Value": 244942,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 820,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Value": 3431740,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 33,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 105,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"149\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515082000\",\"name\":\"foreachRDD @ 17:44:42\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    104
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 104,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"148\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515082000\",\"name\":\"foreachRDD @ 17:44:42\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    103
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            32
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515105279,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"149\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515082000\",\"name\":\"foreachRDD @ 17:44:42\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515082000\">[output operation 0, batch time 17:44:42]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515082000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 33,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 33,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515105314,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 33,
    "Stage Attempt ID": 0,
    "Task Type": "ResultTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 33,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515105314,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515105676,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 825,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 5,
                "Value": 5,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 826,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 5704801,
                "Value": 5704801,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 827,
                "Name": "internal.metrics.executorRunTime",
                "Update": 353,
                "Value": 353,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 828,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 320254797,
                "Value": 320254797,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 829,
                "Name": "internal.metrics.resultSize",
                "Update": 1780,
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 832,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 833,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 834,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 21252004,
                "Value": 21252004,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 836,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 837,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Update": 1,
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 838,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 839,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 840,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Update": 3130534,
                "Value": 3130534,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 841,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 842,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Update": 244942,
                "Value": 244942,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 848,
                "Name": "internal.metrics.output.bytesWritten",
                "Update": 4879424,
                "Value": 4879424,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 849,
                "Name": "internal.metrics.output.recordsWritten",
                "Update": 244942,
                "Value": 244942,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 5,
        "Executor Deserialize CPU Time": 5704801,
        "Executor Run Time": 353,
        "Executor CPU Time": 320254797,
        "Peak Execution Memory": 21252004,
        "Result Size": 1780,
        "JVM GC Time": 0,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 1,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 3130534,
            "Total Records Read": 244942
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 0,
            "Shuffle Write Time": 0,
            "Shuffle Records Written": 0
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 4879424,
            "Records Written": 244942
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 33,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 105,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"149\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515082000\",\"name\":\"foreachRDD @ 17:44:42\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    104
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 104,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"148\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515082000\",\"name\":\"foreachRDD @ 17:44:42\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    103
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            32
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515105279,
        "Completion Time": 1680515105678,
        "Accumulables": [
            {
                "ID": 825,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 5,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 826,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 5704801,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 827,
                "Name": "internal.metrics.executorRunTime",
                "Value": 353,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 828,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 320254797,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 829,
                "Name": "internal.metrics.resultSize",
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 832,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 833,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 834,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 21252004,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 836,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 837,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 838,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 839,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 840,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Value": 3130534,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 841,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 842,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Value": 244942,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 848,
                "Name": "internal.metrics.output.bytesWritten",
                "Value": 4879424,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 849,
                "Name": "internal.metrics.output.recordsWritten",
                "Value": 244942,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerJobEnd",
    "Job ID": 16,
    "Completion Time": 1680515105678,
    "Job Result": {
        "Result": "JobSucceeded"
    }
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 39
}
{
    "Event": "SparkListenerJobStart",
    "Job ID": 17,
    "Submission Time": 1680515105761,
    "Stage Infos": [
        {
            "Stage ID": 34,
            "Stage Attempt ID": 0,
            "Stage Name": "map at main.scala:46",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 109,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"156\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515083000\",\"name\":\"foreachRDD @ 17:44:43\"}}",
                    "Callsite": "map at main.scala:46",
                    "Parent IDs": [
                        108
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 45,
                    "Name": "KafkaRDD",
                    "Scope": "{\"id\":\"0_1680515083000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:43\"}",
                    "Callsite": "createDirectStream at main.scala:31",
                    "Parent IDs": [],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 108,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"155\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515083000\",\"name\":\"foreachRDD @ 17:44:43\"}}",
                    "Callsite": "flatMap at main.scala:41",
                    "Parent IDs": [
                        45
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [],
            "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
            "Accumulables": [],
            "Resource Profile Id": 0
        },
        {
            "Stage ID": 35,
            "Stage Attempt ID": 0,
            "Stage Name": "runJob at SparkHadoopWriter.scala:83",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 111,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"158\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515083000\",\"name\":\"foreachRDD @ 17:44:43\"}}",
                    "Callsite": "saveAsTextFile at main.scala:48",
                    "Parent IDs": [
                        110
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 110,
                    "Name": "ShuffledRDD",
                    "Scope": "{\"id\":\"157\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515083000\",\"name\":\"foreachRDD @ 17:44:43\"}}",
                    "Callsite": "reduceByKey at main.scala:46",
                    "Parent IDs": [
                        109
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [
                34
            ],
            "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
            "Accumulables": [],
            "Resource Profile Id": 0
        }
    ],
    "Stage IDs": [
        34,
        35
    ],
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"158\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515083000\",\"name\":\"foreachRDD @ 17:44:43\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515083000\">[output operation 0, batch time 17:44:43]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515083000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 34,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 109,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"156\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515083000\",\"name\":\"foreachRDD @ 17:44:43\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    108
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 45,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515083000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:43\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 108,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"155\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515083000\",\"name\":\"foreachRDD @ 17:44:43\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    45
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515105764,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"158\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515083000\",\"name\":\"foreachRDD @ 17:44:43\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515083000\">[output operation 0, batch time 17:44:43]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515083000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 34,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 34,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515105772,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 34,
    "Stage Attempt ID": 0,
    "Task Type": "ShuffleMapTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 34,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515105772,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515107551,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 850,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 1,
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 851,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 1874724,
                "Value": 1874724,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 852,
                "Name": "internal.metrics.executorRunTime",
                "Update": 1774,
                "Value": 1774,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 853,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 1594245126,
                "Value": 1594245126,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 854,
                "Name": "internal.metrics.resultSize",
                "Update": 1295,
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 855,
                "Name": "internal.metrics.jvmGCTime",
                "Update": 38,
                "Value": 38,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 857,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 858,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 859,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 27748269,
                "Value": 27748269,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 868,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Update": 3220621,
                "Value": 3220621,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 869,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Update": 253618,
                "Value": 253618,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 870,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Update": 3608863,
                "Value": 3608863,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 1,
        "Executor Deserialize CPU Time": 1874724,
        "Executor Run Time": 1774,
        "Executor CPU Time": 1594245126,
        "Peak Execution Memory": 27748269,
        "Result Size": 1295,
        "JVM GC Time": 38,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 0,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 0,
            "Total Records Read": 0
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 3220621,
            "Shuffle Write Time": 3608863,
            "Shuffle Records Written": 253618
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 0,
            "Records Written": 0
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 34,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 109,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"156\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515083000\",\"name\":\"foreachRDD @ 17:44:43\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    108
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 45,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515083000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:43\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 108,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"155\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515083000\",\"name\":\"foreachRDD @ 17:44:43\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    45
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515105764,
        "Completion Time": 1680515107552,
        "Accumulables": [
            {
                "ID": 850,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 851,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 1874724,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 852,
                "Name": "internal.metrics.executorRunTime",
                "Value": 1774,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 853,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 1594245126,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 854,
                "Name": "internal.metrics.resultSize",
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 855,
                "Name": "internal.metrics.jvmGCTime",
                "Value": 38,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 857,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 858,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 859,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 27748269,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 868,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Value": 3220621,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 869,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Value": 253618,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 870,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Value": 3608863,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 35,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 111,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"158\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515083000\",\"name\":\"foreachRDD @ 17:44:43\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    110
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 110,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"157\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515083000\",\"name\":\"foreachRDD @ 17:44:43\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    109
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            34
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515107554,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"158\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515083000\",\"name\":\"foreachRDD @ 17:44:43\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515083000\">[output operation 0, batch time 17:44:43]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515083000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 35,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 35,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515107593,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 35,
    "Stage Attempt ID": 0,
    "Task Type": "ResultTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 35,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515107593,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515108017,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 875,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 12,
                "Value": 12,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 876,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 12329352,
                "Value": 12329352,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 877,
                "Name": "internal.metrics.executorRunTime",
                "Update": 407,
                "Value": 407,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 878,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 349999631,
                "Value": 349999631,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 879,
                "Name": "internal.metrics.resultSize",
                "Update": 1823,
                "Value": 1823,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 880,
                "Name": "internal.metrics.jvmGCTime",
                "Update": 21,
                "Value": 21,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 882,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 883,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 884,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 15430427,
                "Value": 15430427,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 886,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 887,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Update": 1,
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 888,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 889,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 890,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Update": 3220621,
                "Value": 3220621,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 891,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 892,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Update": 253618,
                "Value": 253618,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 898,
                "Name": "internal.metrics.output.bytesWritten",
                "Update": 5068369,
                "Value": 5068369,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 899,
                "Name": "internal.metrics.output.recordsWritten",
                "Update": 253618,
                "Value": 253618,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 12,
        "Executor Deserialize CPU Time": 12329352,
        "Executor Run Time": 407,
        "Executor CPU Time": 349999631,
        "Peak Execution Memory": 15430427,
        "Result Size": 1823,
        "JVM GC Time": 21,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 1,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 3220621,
            "Total Records Read": 253618
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 0,
            "Shuffle Write Time": 0,
            "Shuffle Records Written": 0
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 5068369,
            "Records Written": 253618
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 35,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 111,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"158\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515083000\",\"name\":\"foreachRDD @ 17:44:43\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    110
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 110,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"157\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515083000\",\"name\":\"foreachRDD @ 17:44:43\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    109
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            34
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515107554,
        "Completion Time": 1680515108018,
        "Accumulables": [
            {
                "ID": 875,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 12,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 876,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 12329352,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 877,
                "Name": "internal.metrics.executorRunTime",
                "Value": 407,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 878,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 349999631,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 879,
                "Name": "internal.metrics.resultSize",
                "Value": 1823,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 880,
                "Name": "internal.metrics.jvmGCTime",
                "Value": 21,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 882,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 883,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 884,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 15430427,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 886,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 887,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 888,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 889,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 890,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Value": 3220621,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 891,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 892,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Value": 253618,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 898,
                "Name": "internal.metrics.output.bytesWritten",
                "Value": 5068369,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 899,
                "Name": "internal.metrics.output.recordsWritten",
                "Value": 253618,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerJobEnd",
    "Job ID": 17,
    "Completion Time": 1680515108018,
    "Job Result": {
        "Result": "JobSucceeded"
    }
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 44
}
{
    "Event": "SparkListenerJobStart",
    "Job ID": 18,
    "Submission Time": 1680515108093,
    "Stage Infos": [
        {
            "Stage ID": 36,
            "Stage Attempt ID": 0,
            "Stage Name": "map at main.scala:46",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 116,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"165\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515084000\",\"name\":\"foreachRDD @ 17:44:44\"}}",
                    "Callsite": "map at main.scala:46",
                    "Parent IDs": [
                        115
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 50,
                    "Name": "KafkaRDD",
                    "Scope": "{\"id\":\"0_1680515084000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:44\"}",
                    "Callsite": "createDirectStream at main.scala:31",
                    "Parent IDs": [],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 115,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"164\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515084000\",\"name\":\"foreachRDD @ 17:44:44\"}}",
                    "Callsite": "flatMap at main.scala:41",
                    "Parent IDs": [
                        50
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [],
            "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
            "Accumulables": [],
            "Resource Profile Id": 0
        },
        {
            "Stage ID": 37,
            "Stage Attempt ID": 0,
            "Stage Name": "runJob at SparkHadoopWriter.scala:83",
            "Number of Tasks": 1,
            "RDD Info": [
                {
                    "RDD ID": 118,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"167\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515084000\",\"name\":\"foreachRDD @ 17:44:44\"}}",
                    "Callsite": "saveAsTextFile at main.scala:48",
                    "Parent IDs": [
                        117
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 117,
                    "Name": "ShuffledRDD",
                    "Scope": "{\"id\":\"166\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515084000\",\"name\":\"foreachRDD @ 17:44:44\"}}",
                    "Callsite": "reduceByKey at main.scala:46",
                    "Parent IDs": [
                        116
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 1,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [
                36
            ],
            "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
            "Accumulables": [],
            "Resource Profile Id": 0
        }
    ],
    "Stage IDs": [
        36,
        37
    ],
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"167\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515084000\",\"name\":\"foreachRDD @ 17:44:44\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515084000\">[output operation 0, batch time 17:44:44]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515084000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 36,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 116,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"165\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515084000\",\"name\":\"foreachRDD @ 17:44:44\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    115
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 50,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515084000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:44\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 115,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"164\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515084000\",\"name\":\"foreachRDD @ 17:44:44\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    50
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515108095,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"167\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515084000\",\"name\":\"foreachRDD @ 17:44:44\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515084000\">[output operation 0, batch time 17:44:44]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515084000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 36,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 36,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515108106,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 36,
    "Stage Attempt ID": 0,
    "Task Type": "ShuffleMapTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 36,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515108106,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515109188,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 900,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 3,
                "Value": 3,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 901,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 3909875,
                "Value": 3909875,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 902,
                "Name": "internal.metrics.executorRunTime",
                "Update": 1076,
                "Value": 1076,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 903,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 962398465,
                "Value": 962398465,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 904,
                "Name": "internal.metrics.resultSize",
                "Update": 1295,
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 905,
                "Name": "internal.metrics.jvmGCTime",
                "Update": 21,
                "Value": 21,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 907,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 908,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 909,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 15057040,
                "Value": 15057040,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 918,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Update": 1980672,
                "Value": 1980672,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 919,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Update": 172048,
                "Value": 172048,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 920,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Update": 2279671,
                "Value": 2279671,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 3,
        "Executor Deserialize CPU Time": 3909875,
        "Executor Run Time": 1076,
        "Executor CPU Time": 962398465,
        "Peak Execution Memory": 15057040,
        "Result Size": 1295,
        "JVM GC Time": 21,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 0,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 0,
            "Total Records Read": 0
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 1980672,
            "Shuffle Write Time": 2279671,
            "Shuffle Records Written": 172048
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 0,
            "Records Written": 0
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 36,
        "Stage Attempt ID": 0,
        "Stage Name": "map at main.scala:46",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 116,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"165\",\"name\":\"map\",\"parent\":{\"id\":\"1_1680515084000\",\"name\":\"foreachRDD @ 17:44:44\"}}",
                "Callsite": "map at main.scala:46",
                "Parent IDs": [
                    115
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 50,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1680515084000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 17:44:44\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 115,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"164\",\"name\":\"flatMap\",\"parent\":{\"id\":\"1_1680515084000\",\"name\":\"foreachRDD @ 17:44:44\"}}",
                "Callsite": "flatMap at main.scala:41",
                "Parent IDs": [
                    50
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.map(RDD.scala:413)\nname.spade5.Main$.$anonfun$main$1(main.scala:46)\nname.spade5.Main$.$anonfun$main$1$adapted(main.scala:34)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:629)\norg.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:629)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.Try$.apply(Try.scala:210)\norg.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\njava.base/java.lang.Thread.run(Thread.java:834)",
        "Submission Time": 1680515108095,
        "Completion Time": 1680515109189,
        "Accumulables": [
            {
                "ID": 900,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 3,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 901,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 3909875,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 902,
                "Name": "internal.metrics.executorRunTime",
                "Value": 1076,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 903,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 962398465,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 904,
                "Name": "internal.metrics.resultSize",
                "Value": 1295,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 905,
                "Name": "internal.metrics.jvmGCTime",
                "Value": 21,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 907,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 908,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 909,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 15057040,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 918,
                "Name": "internal.metrics.shuffle.write.bytesWritten",
                "Value": 1980672,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 919,
                "Name": "internal.metrics.shuffle.write.recordsWritten",
                "Value": 172048,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 920,
                "Name": "internal.metrics.shuffle.write.writeTime",
                "Value": 2279671,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 37,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 118,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"167\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515084000\",\"name\":\"foreachRDD @ 17:44:44\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    117
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 117,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"166\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515084000\",\"name\":\"foreachRDD @ 17:44:44\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    116
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            36
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515109190,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "spark.rdd.scope": "{\"id\":\"167\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515084000\",\"name\":\"foreachRDD @ 17:44:44\"}}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1680515084000\">[output operation 0, batch time 17:44:44]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1680515084000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 37,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 37,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515109229,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 37,
    "Stage Attempt ID": 0,
    "Task Type": "ResultTask",
    "Task End Reason": {
        "Reason": "Success"
    },
    "Task Info": {
        "Task ID": 37,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1680515109229,
        "Executor ID": "driver",
        "Host": "node86",
        "Locality": "NODE_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1680515109501,
        "Failed": false,
        "Killed": false,
        "Accumulables": [
            {
                "ID": 925,
                "Name": "internal.metrics.executorDeserializeTime",
                "Update": 5,
                "Value": 5,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 926,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Update": 5224591,
                "Value": 5224591,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 927,
                "Name": "internal.metrics.executorRunTime",
                "Update": 264,
                "Value": 264,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 928,
                "Name": "internal.metrics.executorCpuTime",
                "Update": 232841576,
                "Value": 232841576,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 929,
                "Name": "internal.metrics.resultSize",
                "Update": 1780,
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 932,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 933,
                "Name": "internal.metrics.diskBytesSpilled",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 934,
                "Name": "internal.metrics.peakExecutionMemory",
                "Update": 11508266,
                "Value": 11508266,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 936,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 937,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Update": 1,
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 938,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 939,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 940,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Update": 1980672,
                "Value": 1980672,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 941,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Update": 0,
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 942,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Update": 172048,
                "Value": 172048,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 948,
                "Name": "internal.metrics.output.bytesWritten",
                "Update": 3100011,
                "Value": 3100011,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 949,
                "Name": "internal.metrics.output.recordsWritten",
                "Update": 172048,
                "Value": 172048,
                "Internal": true,
                "Count Failed Values": true
            }
        ]
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 5,
        "Executor Deserialize CPU Time": 5224591,
        "Executor Run Time": 264,
        "Executor CPU Time": 232841576,
        "Peak Execution Memory": 11508266,
        "Result Size": 1780,
        "JVM GC Time": 0,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 1,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 1980672,
            "Total Records Read": 172048
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 0,
            "Shuffle Write Time": 0,
            "Shuffle Records Written": 0
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 3100011,
            "Records Written": 172048
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 37,
        "Stage Attempt ID": 0,
        "Stage Name": "runJob at SparkHadoopWriter.scala:83",
        "Number of Tasks": 1,
        "RDD Info": [
            {
                "RDD ID": 118,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"167\",\"name\":\"saveAsTextFile\",\"parent\":{\"id\":\"1_1680515084000\",\"name\":\"foreachRDD @ 17:44:44\"}}",
                "Callsite": "saveAsTextFile at main.scala:48",
                "Parent IDs": [
                    117
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 117,
                "Name": "ShuffledRDD",
                "Scope": "{\"id\":\"166\",\"name\":\"reduceByKey\",\"parent\":{\"id\":\"1_1680515084000\",\"name\":\"foreachRDD @ 17:44:44\"}}",
                "Callsite": "reduceByKey at main.scala:46",
                "Parent IDs": [
                    116
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "UNORDERED",
                "Number of Partitions": 1,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [
            36
        ],
        "Details": "org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\norg.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\norg.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\nscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:406)\norg.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)",
        "Submission Time": 1680515109190,
        "Completion Time": 1680515109503,
        "Accumulables": [
            {
                "ID": 925,
                "Name": "internal.metrics.executorDeserializeTime",
                "Value": 5,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 926,
                "Name": "internal.metrics.executorDeserializeCpuTime",
                "Value": 5224591,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 927,
                "Name": "internal.metrics.executorRunTime",
                "Value": 264,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 928,
                "Name": "internal.metrics.executorCpuTime",
                "Value": 232841576,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 929,
                "Name": "internal.metrics.resultSize",
                "Value": 1780,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 932,
                "Name": "internal.metrics.memoryBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 933,
                "Name": "internal.metrics.diskBytesSpilled",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 934,
                "Name": "internal.metrics.peakExecutionMemory",
                "Value": 11508266,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 936,
                "Name": "internal.metrics.shuffle.read.remoteBlocksFetched",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 937,
                "Name": "internal.metrics.shuffle.read.localBlocksFetched",
                "Value": 1,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 938,
                "Name": "internal.metrics.shuffle.read.remoteBytesRead",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 939,
                "Name": "internal.metrics.shuffle.read.remoteBytesReadToDisk",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 940,
                "Name": "internal.metrics.shuffle.read.localBytesRead",
                "Value": 1980672,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 941,
                "Name": "internal.metrics.shuffle.read.fetchWaitTime",
                "Value": 0,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 942,
                "Name": "internal.metrics.shuffle.read.recordsRead",
                "Value": 172048,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 948,
                "Name": "internal.metrics.output.bytesWritten",
                "Value": 3100011,
                "Internal": true,
                "Count Failed Values": true
            },
            {
                "ID": 949,
                "Name": "internal.metrics.output.recordsWritten",
                "Value": 172048,
                "Internal": true,
                "Count Failed Values": true
            }
        ],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerJobEnd",
    "Job ID": 18,
    "Completion Time": 1680515109503,
    "Job Result": {
        "Result": "JobSucceeded"
    }
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 45
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 50
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 51
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 56
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 57
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 62
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 63
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 68
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 69
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 74
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 75
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 80
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 81
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 82
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 87
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 88
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 93
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 94
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 95
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 100
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 101
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 106
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 107
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 112
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 113
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 114
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 119
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 120
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 121
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 122
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 123
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 124
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 125
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 126
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 127
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 128
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 129
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 130
}
{
    "Event": "SparkListenerUnpersistRDD",
    "RDD ID": 131
}
{
    "Event": "SparkListenerApplicationEnd",
    "Timestamp": 1680515122733
}