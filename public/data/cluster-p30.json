{
    "Event": "SparkListenerLogStart",
    "Spark Version": "3.3.2"
}
{
    "Event": "SparkListenerResourceProfileAdded",
    "Resource Profile Id": 0,
    "Executor Resource Requests": {
        "cores": {
            "Resource Name": "cores",
            "Amount": 1,
            "Discovery Script": "",
            "Vendor": ""
        },
        "memory": {
            "Resource Name": "memory",
            "Amount": 1024,
            "Discovery Script": "",
            "Vendor": ""
        },
        "offHeap": {
            "Resource Name": "offHeap",
            "Amount": 0,
            "Discovery Script": "",
            "Vendor": ""
        }
    },
    "Task Resource Requests": {
        "cpus": {
            "Resource Name": "cpus",
            "Amount": 1.0
        }
    }
}
{
    "Event": "SparkListenerBlockManagerAdded",
    "Block Manager ID": {
        "Executor ID": "driver",
        "Host": "node85",
        "Port": 51116
    },
    "Maximum Memory": 455501414,
    "Timestamp": 1681460002653,
    "Maximum Onheap Memory": 455501414,
    "Maximum Offheap Memory": 0
}
{
    "Event": "SparkListenerEnvironmentUpdate",
    "JVM Information": {
        "Java Home": "/home/chenhao/libs/jdk11",
        "Java Version": "11.0.16.1 (Oracle Corporation)",
        "Scala Version": "version 2.13.8"
    },
    "Spark Properties": {
        "spark.eventLog.enabled": "true",
        "spark.driver.port": "46111",
        "spark.jars": "file:/home/chenhao/workspace/SparkTemplate-1.3-jar-with-dependencies.jar",
        "spark.driver.supervise": "false",
        "spark.app.name": "WordCount",
        "spark.submit.pyFiles": "",
        "spark.app.submitTime": "1681459654249",
        "spark.submit.deployMode": "cluster",
        "spark.master": "spark://node85:7077",
        "spark.eventLog.dir": "/home/chenhao/logs/events",
        "spark.executor.cores": "1",
        "spark.app.id": "app-20230414161322-0013",
        "spark.executor.extraJavaOptions": "-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED",
        "spark.driver.host": "node85",
        "spark.scheduler.mode": "FIFO",
        "spark.rpc.askTimeout": "10s",
        "spark.streaming.kafka.maxRatePerPartition": "30000",
        "spark.app.startTime": "1681460001778",
        "spark.executor.id": "driver",
        "spark.driver.extraJavaOptions": "-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED",
        "spark.app.initial.jar.urls": "spark://node85:46111/jars/SparkTemplate-1.3-jar-with-dependencies.jar"
    },
    "Hadoop Properties": {
        "hadoop.service.shutdown.timeout": "30s",
        "yarn.resourcemanager.amlauncher.thread-count": "50",
        "yarn.nodemanager.numa-awareness.numactl.cmd": "/usr/bin/numactl",
        "fs.viewfs.overload.scheme.target.o3fs.impl": "org.apache.hadoop.fs.ozone.OzoneFileSystem",
        "yarn.timeline-service.timeline-client.number-of-async-entities-to-merge": "10",
        "hadoop.security.kms.client.timeout": "60",
        "yarn.resourcemanager.application-tag-based-placement.enable": "false",
        "yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds.min": "3600",
        "yarn.app.mapreduce.am.job.task.listener.thread-count": "30",
        "yarn.nodemanager.node-attributes.resync-interval-ms": "120000",
        "yarn.nodemanager.container-log-monitor.interval-ms": "60000",
        "fs.viewfs.overload.scheme.target.gs.impl": "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS",
        "fs.s3a.retry.limit": "7",
        "mapreduce.jobhistory.loadedjobs.cache.size": "5",
        "yarn.sharedcache.enabled": "false",
        "fs.s3a.connection.maximum": "96",
        "fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem",
        "yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms": "1000",
        "hadoop.http.authentication.kerberos.principal": "HTTP/_HOST@LOCALHOST",
        "mapreduce.jobhistory.loadedjob.tasks.max": "-1",
        "mapreduce.framework.name": "local",
        "yarn.sharedcache.uploader.server.thread-count": "50",
        "yarn.nodemanager.linux-container-executor.nonsecure-mode.user-pattern": "^[_.A-Za-z0-9][-@_.A-Za-z0-9]{0,255}?[$]?$",
        "tfile.fs.output.buffer.size": "262144",
        "hadoop.security.groups.cache.background.reload.threads": "3",
        "yarn.resourcemanager.webapp.cross-origin.enabled": "false",
        "fs.AbstractFileSystem.ftp.impl": "org.apache.hadoop.fs.ftp.FtpFs",
        "hadoop.registry.secure": "false",
        "hadoop.shell.safely.delete.limit.num.files": "100",
        "mapreduce.job.acl-view-job": " ",
        "fs.s3a.s3guard.ddb.background.sleep": "25ms",
        "fs.s3a.s3guard.ddb.table.create": "false",
        "fs.viewfs.overload.scheme.target.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem",
        "mapreduce.shuffle.pathcache.expire-after-access-minutes": "5",
        "mapreduce.input.fileinputformat.split.minsize": "0",
        "yarn.resourcemanager.container.liveness-monitor.interval-ms": "600000",
        "yarn.resourcemanager.client.thread-count": "50",
        "fs.viewfs.overload.scheme.target.http.impl": "org.apache.hadoop.fs.http.HttpFileSystem",
        "yarn.nodemanager.amrmproxy.interceptor-class.pipeline": "org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor",
        "yarn.timeline-service.entity-group-fs-store.leveldb-cache-read-cache-size": "10485760",
        "yarn.nodemanager.admin-env": "MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX",
        "yarn.resourcemanager.node-removal-untracked.timeout-ms": "60000",
        "mapreduce.am.max-attempts": "2",
        "hadoop.security.kms.client.failover.sleep.base.millis": "100",
        "yarn.nodemanager.amrmproxy.enabled": "false",
        "yarn.timeline-service.entity-group-fs-store.with-user-dir": "false",
        "io.seqfile.compress.blocksize": "1000000",
        "yarn.nodemanager.runtime.linux.docker.allowed-container-runtimes": "runc",
        "yarn.resourcemanager.nodemanagers.heartbeat-interval-slowdown-factor": "1.0",
        "yarn.sharedcache.checksum.algo.impl": "org.apache.hadoop.yarn.sharedcache.ChecksumSHA256Impl",
        "mapreduce.reduce.shuffle.fetch.retry.interval-ms": "1000",
        "mapreduce.task.profile.maps": "0-2",
        "yarn.scheduler.include-port-in-node-name": "false",
        "mapreduce.jobhistory.webapp.https.address": "0.0.0.0:19890",
        "yarn.node-labels.fs-store.impl.class": "org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore",
        "hadoop.http.authentication.signature.secret.file": "*********(redacted)",
        "hadoop.jetty.logs.serve.aliases": "true",
        "yarn.sharedcache.webapp.address": "0.0.0.0:8788",
        "fs.s3a.select.input.csv.quote.escape.character": "\\\\",
        "fs.viewfs.overload.scheme.target.swift.impl": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem",
        "hadoop.security.group.mapping.ldap.posix.attr.gid.name": "gidNumber",
        "ipc.client.fallback-to-simple-auth-allowed": "false",
        "yarn.nodemanager.resource.memory.enforced": "true",
        "yarn.resourcemanager.system-metrics-publisher.timeline-server-v1.enable-batch": "false",
        "yarn.client.failover-proxy-provider": "org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider",
        "yarn.nodemanager.collector-service.address": "${yarn.nodemanager.hostname}:8048",
        "fs.trash.checkpoint.interval": "0",
        "mapreduce.job.map.output.collector.class": "org.apache.hadoop.mapred.MapTask$MapOutputBuffer",
        "yarn.resourcemanager.node-ip-cache.expiry-interval-secs": "-1",
        "yarn.resourcemanager.placement-constraints.handler": "disabled",
        "yarn.timeline-service.handler-thread-count": "10",
        "yarn.resourcemanager.max-completed-applications": "1000",
        "yarn.nodemanager.aux-services.manifest.enabled": "false",
        "yarn.resourcemanager.system-metrics-publisher.enabled": "false",
        "yarn.resourcemanager.placement-constraints.algorithm.class": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.algorithm.DefaultPlacementAlgorithm",
        "yarn.resourcemanager.delegation.token.renew-interval": "*********(redacted)",
        "yarn.sharedcache.nm.uploader.replication.factor": "10",
        "hadoop.security.groups.negative-cache.secs": "30",
        "yarn.app.mapreduce.task.container.log.backups": "0",
        "mapreduce.reduce.skip.proc-count.auto-incr": "true",
        "yarn.timeline-service.http-authentication.simple.anonymous.allowed": "true",
        "ha.health-monitor.check-interval.ms": "1000",
        "yarn.nodemanager.runtime.linux.runc.host-pid-namespace.allowed": "false",
        "hadoop.metrics.jvm.use-thread-mxbean": "false",
        "ipc.[port_number].faircallqueue.multiplexer.weights": "8,4,2,1",
        "yarn.acl.reservation-enable": "false",
        "yarn.resourcemanager.store.class": "org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore",
        "yarn.app.mapreduce.am.hard-kill-timeout-ms": "10000",
        "yarn.resourcemanager.nodemanagers.heartbeat-interval-scaling-enable": "false",
        "yarn.resourcemanager.nodemanagers.heartbeat-interval-ms": "1000",
        "yarn.nodemanager.windows-container.cpu-limit.enabled": "false",
        "yarn.scheduler.configuration.leveldb-store.path": "${hadoop.tmp.dir}/yarn/system/confstore",
        "mapreduce.map.skip.proc-count.auto-incr": "true",
        "fs.s3a.committer.name": "file",
        "yarn.webapp.xfs-filter.enabled": "true",
        "yarn.resourcemanager.scheduler.address": "${yarn.resourcemanager.hostname}:8030",
        "yarn.node-labels.enabled": "false",
        "yarn.resourcemanager.webapp.ui-actions.enabled": "true",
        "fs.s3a.etag.checksum.enabled": "false",
        "yarn.nodemanager.container-metrics.enable": "true",
        "ha.health-monitor.rpc.connect.max.retries": "1",
        "yarn.timeline-service.client.fd-clean-interval-secs": "60",
        "hadoop.common.configuration.version": "3.0.0",
        "fs.s3a.s3guard.ddb.table.capacity.read": "0",
        "yarn.nodemanager.remote-app-log-dir-suffix": "logs",
        "yarn.nodemanager.container-log-monitor.dir-size-limit-bytes": "1000000000",
        "yarn.nodemanager.runtime.linux.docker.privileged-containers.allowed": "false",
        "file.blocksize": "67108864",
        "hadoop.http.idle_timeout.ms": "60000",
        "hadoop.registry.zk.retry.ceiling.ms": "60000",
        "yarn.sharedcache.store.in-memory.initial-delay-mins": "10",
        "mapreduce.jobhistory.principal": "jhs/_HOST@REALM.TLD",
        "mapreduce.task.profile.reduces": "0-2",
        "hadoop.zk.num-retries": "1000",
        "fs.viewfs.overload.scheme.target.hdfs.impl": "org.apache.hadoop.hdfs.DistributedFileSystem",
        "seq.io.sort.mb": "100",
        "yarn.scheduler.configuration.max.version": "100",
        "yarn.timeline-service.webapp.https.address": "${yarn.timeline-service.hostname}:8190",
        "mapreduce.task.timeout": "600000",
        "yarn.sharedcache.client-server.thread-count": "50",
        "hadoop.security.groups.shell.command.timeout": "0s",
        "hadoop.security.crypto.cipher.suite": "AES/CTR/NoPadding",
        "yarn.nodemanager.elastic-memory-control.oom-handler": "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DefaultOOMHandler",
        "yarn.resourcemanager.connect.max-wait.ms": "900000",
        "fs.defaultFS": "file:///",
        "yarn.minicluster.use-rpc": "false",
        "io.compression.codec.bzip2.library": "system-native",
        "yarn.webapp.filter-invalid-xml-chars": "false",
        "yarn.nodemanager.runtime.linux.runc.layer-mounts-interval-secs": "600",
        "fs.s3a.select.input.csv.record.delimiter": "\\n",
        "fs.s3a.change.detection.source": "etag",
        "ipc.[port_number].backoff.enable": "false",
        "yarn.nodemanager.distributed-scheduling.enabled": "false",
        "mapreduce.shuffle.connection-keep-alive.timeout": "5",
        "yarn.resourcemanager.webapp.https.address": "${yarn.resourcemanager.hostname}:8090",
        "yarn.webapp.enable-rest-app-submissions": "true",
        "fs.AbstractFileSystem.s3a.impl": "org.apache.hadoop.fs.s3a.S3A",
        "mapreduce.task.combine.progress.records": "10000",
        "yarn.resourcemanager.epoch.range": "0",
        "yarn.resourcemanager.am.max-attempts": "2",
        "yarn.nodemanager.runtime.linux.runc.image-toplevel-dir": "/runc-root",
        "yarn.nodemanager.linux-container-executor.cgroups.hierarchy": "/hadoop-yarn",
        "fs.AbstractFileSystem.wasbs.impl": "org.apache.hadoop.fs.azure.Wasbs",
        "yarn.timeline-service.entity-group-fs-store.cache-store-class": "org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore",
        "yarn.nodemanager.runtime.linux.runc.allowed-container-networks": "host,none,bridge",
        "fs.ftp.transfer.mode": "BLOCK_TRANSFER_MODE",
        "ipc.[port_number].decay-scheduler.decay-factor": "0.5",
        "fs.har.impl.disable.cache": "true",
        "yarn.webapp.ui2.enable": "false",
        "mapreduce.jobhistory.address": "0.0.0.0:10020",
        "yarn.resourcemanager.nm-tokens.master-key-rolling-interval-secs": "*********(redacted)",
        "yarn.is.minicluster": "false",
        "yarn.nodemanager.address": "${yarn.nodemanager.hostname}:0",
        "fs.abfss.impl": "org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem",
        "ipc.server.log.slow.rpc": "false",
        "ipc.server.reuseaddr": "true",
        "yarn.router.webapp.https.address": "0.0.0.0:8091",
        "yarn.nodemanager.webapp.cross-origin.enabled": "false",
        "fs.wasb.impl": "org.apache.hadoop.fs.azure.NativeAzureFileSystem",
        "fs.AbstractFileSystem.abfs.impl": "org.apache.hadoop.fs.azurebfs.Abfs",
        "hadoop.security.credential.clear-text-fallback": "true",
        "yarn.timeline-service.writer.async.queue.capacity": "100",
        "yarn.resourcemanager.fs.state-store.num-retries": "0",
        "yarn.resourcemanager.nodemanager-connect-retries": "10",
        "fs.ftp.timeout": "0",
        "yarn.resourcemanager.node-labels.provider.fetch-interval-ms": "1800000",
        "yarn.resourcemanager.auto-update.containers": "false",
        "yarn.app.mapreduce.am.job.committer.cancel-timeout": "60000",
        "yarn.scheduler.configuration.zk-store.parent-path": "/confstore",
        "yarn.nodemanager.default-container-executor.log-dirs.permissions": "710",
        "yarn.app.attempt.diagnostics.limit.kc": "64",
        "fs.viewfs.overload.scheme.target.swebhdfs.impl": "org.apache.hadoop.hdfs.web.SWebHdfsFileSystem",
        "yarn.client.failover-no-ha-proxy-provider": "org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider",
        "fs.s3a.change.detection.mode": "server",
        "ftp.bytes-per-checksum": "512",
        "yarn.nodemanager.resource.memory-mb": "-1",
        "yarn.timeline-service.writer.flush-interval-seconds": "60",
        "fs.s3a.fast.upload.active.blocks": "4",
        "yarn.resourcemanager.submission-preprocessor.enabled": "false",
        "yarn.nodemanager.collector-service.thread-count": "5",
        "ipc.[port_number].scheduler.impl": "org.apache.hadoop.ipc.DefaultRpcScheduler",
        "fs.azure.secure.mode": "false",
        "mapreduce.jobhistory.joblist.cache.size": "20000",
        "fs.ftp.host": "0.0.0.0",
        "yarn.nodemanager.log-aggregation.num-log-files-per-app": "30",
        "hadoop.security.kms.client.encrypted.key.cache.low-watermark": "0.3f",
        "fs.s3a.committer.magic.enabled": "true",
        "yarn.timeline-service.client.max-retries": "30",
        "dfs.ha.fencing.ssh.connect-timeout": "30000",
        "yarn.log-aggregation-enable": "false",
        "yarn.system-metrics-publisher.enabled": "false",
        "mapreduce.reduce.markreset.buffer.percent": "0.0",
        "fs.AbstractFileSystem.viewfs.impl": "org.apache.hadoop.fs.viewfs.ViewFs",
        "yarn.resourcemanager.nodemanagers.heartbeat-interval-speedup-factor": "1.0",
        "ha.failover-controller.new-active.rpc-timeout.ms": "60000",
        "yarn.nodemanager.container-localizer.java.opts": "-Xmx256m",
        "yarn.app.mapreduce.am.job.committer.commit-window": "10000",
        "hadoop.tags.system": "YARN,HDFS,NAMENODE,DATANODE,REQUIRED,SECURITY,KERBEROS,PERFORMANCE,CLIENT\n      ,SERVER,DEBUG,DEPRECATED,COMMON,OPTIONAL",
        "hadoop.caller.context.signature.max.size": "40",
        "yarn.scheduler.configuration.store.max-logs": "1000",
        "yarn.nodemanager.node-attributes.provider.fetch-interval-ms": "600000",
        "hadoop.http.cross-origin.enabled": "false",
        "hadoop.zk.acl": "world:anyone:rwcda",
        "mapreduce.task.io.sort.factor": "10",
        "yarn.nodemanager.amrmproxy.client.thread-count": "25",
        "mapreduce.jobhistory.datestring.cache.size": "200000",
        "mapreduce.job.acl-modify-job": " ",
        "yarn.nodemanager.windows-container.memory-limit.enabled": "false",
        "yarn.timeline-service.webapp.address": "${yarn.timeline-service.hostname}:8188",
        "yarn.nodemanager.container-manager.thread-count": "20",
        "yarn.minicluster.fixed.ports": "false",
        "yarn.cluster.max-application-priority": "0",
        "yarn.timeline-service.ttl-enable": "true",
        "mapreduce.jobhistory.recovery.store.fs.uri": "${hadoop.tmp.dir}/mapred/history/recoverystore",
        "ipc.[port_number].decay-scheduler.backoff.responsetime.enable": "false",
        "yarn.client.load.resource-types.from-server": "false",
        "ha.zookeeper.session-timeout.ms": "10000",
        "ipc.[port_number].decay-scheduler.metrics.top.user.count": "10",
        "tfile.io.chunk.size": "1048576",
        "fs.s3a.s3guard.ddb.table.capacity.write": "0",
        "yarn.dispatcher.print-events-info.threshold": "5000",
        "mapreduce.job.speculative.slowtaskthreshold": "1.0",
        "io.serializations": "org.apache.hadoop.io.serializer.WritableSerialization, org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization, org.apache.hadoop.io.serializer.avro.AvroReflectSerialization",
        "hadoop.security.kms.client.failover.sleep.max.millis": "2000",
        "hadoop.security.group.mapping.ldap.directory.search.timeout": "10000",
        "fs.swift.impl": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem",
        "yarn.nodemanager.local-cache.max-files-per-directory": "8192",
        "yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin.num-manifests-to-cache": "10",
        "mapreduce.map.sort.spill.percent": "0.80",
        "yarn.timeline-service.entity-group-fs-store.scan-interval-seconds": "60",
        "fs.s3a.select.enabled": "true",
        "mapreduce.ifile.readahead": "true",
        "yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms": "300000",
        "yarn.timeline-service.hbase.coprocessor.jar.hdfs.location": "/hbase/coprocessor/hadoop-yarn-server-timelineservice.jar",
        "hadoop.security.kms.client.encrypted.key.cache.num.refill.threads": "2",
        "yarn.resourcemanager.scheduler.class": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler",
        "hadoop.http.sni.host.check.enabled": "false",
        "fs.client.resolve.topology.enabled": "false",
        "yarn.nodemanager.runtime.linux.allowed-runtimes": "default",
        "io.skip.checksum.errors": "false",
        "yarn.timeline-service.client.best-effort": "false",
        "yarn.node-attribute.fs-store.impl.class": "org.apache.hadoop.yarn.server.resourcemanager.nodelabels.FileSystemNodeAttributeStore",
        "fs.s3a.retry.interval": "500ms",
        "yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled": "*********(redacted)",
        "hadoop.security.group.mapping.ldap.posix.attr.uid.name": "uidNumber",
        "fs.AbstractFileSystem.swebhdfs.impl": "org.apache.hadoop.fs.SWebHdfs",
        "yarn.nodemanager.elastic-memory-control.timeout-sec": "5",
        "yarn.timeline-service.reader.webapp.address": "${yarn.timeline-service.webapp.address}",
        "yarn.resourcemanager.placement-constraints.algorithm.pool-size": "1",
        "yarn.app.mapreduce.am.command-opts": "-Xmx1024m",
        "fs.s3a.metadatastore.fail.on.write.error": "true",
        "mapreduce.cluster.local.dir": "${hadoop.tmp.dir}/mapred/local",
        "io.mapfile.bloom.error.rate": "0.005",
        "yarn.sharedcache.store.class": "org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore",
        "ha.failover-controller.graceful-fence.rpc-timeout.ms": "5000",
        "ftp.replication": "3",
        "fs.getspaceused.jitterMillis": "60000",
        "hadoop.security.uid.cache.secs": "14400",
        "mapreduce.job.maxtaskfailures.per.tracker": "3",
        "fs.s3a.metadatastore.impl": "org.apache.hadoop.fs.s3a.s3guard.NullMetadataStore",
        "yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts": "3",
        "yarn.timeline-service.webapp.xfs-filter.xframe-options": "SAMEORIGIN",
        "fs.s3a.connection.timeout": "200000",
        "yarn.app.mapreduce.am.webapp.https.enabled": "false",
        "mapreduce.job.max.split.locations": "15",
        "yarn.resourcemanager.nm-container-queuing.max-queue-length": "15",
        "yarn.resourcemanager.delegation-token.always-cancel": "*********(redacted)",
        "hadoop.registry.zk.session.timeout.ms": "60000",
        "yarn.federation.cache-ttl.secs": "300",
        "mapreduce.jvm.system-properties-to-log": "os.name,os.version,java.home,java.runtime.version,java.vendor,java.version,java.vm.name,java.class.path,java.io.tmpdir,user.dir,user.name",
        "yarn.resourcemanager.opportunistic-container-allocation.nodes-used": "10",
        "yarn.minicluster.yarn.nodemanager.resource.memory-mb": "4096",
        "yarn.resourcemanager.admin.client.thread-count": "1",
        "yarn.dispatcher.drain-events.timeout": "300000",
        "yarn.timeline-service.entity-group-fs-store.active-dir": "/tmp/entity-file-history/active",
        "mapreduce.shuffle.transfer.buffer.size": "131072",
        "yarn.timeline-service.client.retry-interval-ms": "1000",
        "yarn.timeline-service.flowname.max-size": "0",
        "yarn.http.policy": "HTTP_ONLY",
        "fs.s3a.socket.send.buffer": "8192",
        "fs.AbstractFileSystem.abfss.impl": "org.apache.hadoop.fs.azurebfs.Abfss",
        "yarn.sharedcache.uploader.server.address": "0.0.0.0:8046",
        "yarn.resourcemanager.delegation-token.max-conf-size-bytes": "*********(redacted)",
        "hadoop.http.authentication.token.validity": "*********(redacted)",
        "mapreduce.shuffle.max.connections": "0",
        "mapreduce.job.emit-timeline-data": "false",
        "yarn.nodemanager.resource.system-reserved-memory-mb": "-1",
        "hadoop.kerberos.min.seconds.before.relogin": "60",
        "mapreduce.jobhistory.move.thread-count": "3",
        "ipc.[port_number].decay-scheduler.backoff.responsetime.thresholds": "10s,20s,30s,40s",
        "fs.s3a.buffer.dir": "${hadoop.tmp.dir}/s3a",
        "hadoop.ssl.enabled.protocols": "TLSv1.2",
        "mapreduce.jobhistory.admin.address": "0.0.0.0:10033",
        "yarn.log-aggregation-status.time-out.ms": "600000",
        "fs.s3a.accesspoint.required": "false",
        "yarn.nodemanager.health-checker.interval-ms": "600000",
        "yarn.router.clientrm.interceptor-class.pipeline": "org.apache.hadoop.yarn.server.router.clientrm.DefaultClientRequestInterceptor",
        "yarn.nodemanager.runtime.linux.sandbox-mode.local-dirs.permissions": "read",
        "yarn.resourcemanager.activities-manager.app-activities.max-queue-length": "100",
        "yarn.nodemanager.pmem-check-enabled": "true",
        "yarn.federation.enabled": "false",
        "yarn.resourcemanager.nm-container-queuing.load-comparator": "QUEUE_LENGTH",
        "mapreduce.job.complete.cancel.delegation.tokens": "*********(redacted)",
        "yarn.nodemanager.amrmproxy.ha.enable": "false",
        "fs.AbstractFileSystem.gs.impl": "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS",
        "mapreduce.shuffle.port": "13562",
        "yarn.resourcemanager.max-log-aggregation-diagnostics-in-memory": "10",
        "yarn.resourcemanager.zk-appid-node.split-index": "0",
        "ftp.blocksize": "67108864",
        "yarn.router.rmadmin.interceptor-class.pipeline": "org.apache.hadoop.yarn.server.router.rmadmin.DefaultRMAdminRequestInterceptor",
        "yarn.nodemanager.log-container-debug-info.enabled": "true",
        "yarn.resourcemanager.application-https.policy": "NONE",
        "yarn.client.max-cached-nodemanagers-proxies": "0",
        "yarn.nodemanager.linux-container-executor.cgroups.delete-delay-ms": "20",
        "yarn.nodemanager.delete.debug-delay-sec": "0",
        "yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage": "90.0",
        "mapreduce.app-submission.cross-platform": "false",
        "yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms": "10000",
        "yarn.nodemanager.container-retry-minimum-interval-ms": "1000",
        "hadoop.security.groups.cache.secs": "300",
        "yarn.workflow-id.tag-prefix": "workflowid:",
        "fs.azure.local.sas.key.mode": "false",
        "ipc.maximum.data.length": "134217728",
        "yarn.router.pipeline.cache-max-size": "25",
        "fs.s3a.endpoint": "s3.amazonaws.com",
        "mapreduce.shuffle.max.threads": "0",
        "yarn.resourcemanager.resource-tracker.nm.ip-hostname-check": "false",
        "hadoop.security.authorization": "false",
        "fs.s3a.paging.maximum": "5000",
        "nfs.exports.allowed.hosts": "* rw",
        "mapreduce.jobhistory.http.policy": "HTTP_ONLY",
        "yarn.sharedcache.store.in-memory.check-period-mins": "720",
        "hadoop.security.group.mapping.ldap.ssl": "false",
        "yarn.scheduler.configuration.leveldb-store.compaction-interval-secs": "86400",
        "yarn.timeline-service.writer.class": "org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl",
        "ha.zookeeper.parent-znode": "/hadoop-ha",
        "yarn.resourcemanager.submission-preprocessor.file-refresh-interval-ms": "60000",
        "hadoop.security.group.mapping.ldap.search.filter.group": "(objectClass=group)",
        "yarn.resourcemanager.placement-constraints.scheduler.pool-size": "1",
        "yarn.resourcemanager.activities-manager.cleanup-interval-ms": "5000",
        "yarn.admin.acl": "*",
        "yarn.sharedcache.admin.thread-count": "1",
        "mapreduce.task.local-fs.write-limit.bytes": "-1",
        "fs.adl.oauth2.access.token.provider.type": "*********(redacted)",
        "yarn.nodemanager.resource-plugins.gpu.docker-plugin": "nvidia-docker-v1",
        "fs.s3a.downgrade.syncable.exceptions": "true",
        "yarn.client.application-client-protocol.poll-interval-ms": "200",
        "yarn.nodemanager.log-aggregation.policy.class": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AllContainerLogAggregationPolicy",
        "mapreduce.reduce.shuffle.merge.percent": "0.66",
        "yarn.nodemanager.resourcemanager.minimum.version": "NONE",
        "mapreduce.job.speculative.speculative-cap-running-tasks": "0.1",
        "ipc.[port_number].identity-provider.impl": "org.apache.hadoop.ipc.UserIdentityProvider",
        "yarn.nodemanager.recovery.supervised": "false",
        "mapreduce.reduce.skip.maxgroups": "0",
        "yarn.resourcemanager.ha.automatic-failover.enabled": "true",
        "yarn.nodemanager.container-log-monitor.total-size-limit-bytes": "10000000000",
        "mapreduce.reduce.shuffle.connect.timeout": "180000",
        "yarn.nodemanager.health-checker.scripts": "script",
        "yarn.resourcemanager.address": "${yarn.resourcemanager.hostname}:8032",
        "ipc.client.ping": "true",
        "mapreduce.shuffle.ssl.file.buffer.size": "65536",
        "yarn.resourcemanager.ha.automatic-failover.embedded": "true",
        "fs.s3a.s3guard.consistency.retry.interval": "2s",
        "yarn.resourcemanager.nm-container-queuing.queue-limit-stdev": "1.0f",
        "mapreduce.job.end-notification.max.attempts": "5",
        "yarn.nodemanager.keytab": "/etc/krb5.keytab",
        "mapreduce.jobhistory.keytab": "/etc/security/keytab/jhs.service.keytab",
        "fs.s3a.threads.max": "64",
        "yarn.nodemanager.runtime.linux.docker.image-update": "false",
        "mapreduce.reduce.shuffle.input.buffer.percent": "0.70",
        "fs.viewfs.overload.scheme.target.abfss.impl": "org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem",
        "yarn.dispatcher.cpu-monitor.samples-per-min": "60",
        "hadoop.security.token.service.use_ip": "*********(redacted)",
        "yarn.nodemanager.runtime.linux.docker.allowed-container-networks": "host,none,bridge",
        "yarn.nodemanager.node-labels.resync-interval-ms": "120000",
        "mapreduce.job.end-notification.max.retry.interval": "5000",
        "yarn.nodemanager.containers-launcher.class": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher",
        "fs.s3a.multipart.purge": "false",
        "yarn.scheduler.configuration.store.class": "file",
        "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.DefaultCodec",
        "yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled": "false",
        "ipc.client.bind.wildcard.addr": "false",
        "yarn.resourcemanager.webapp.rest-csrf.enabled": "false",
        "ha.health-monitor.connect-retry-interval.ms": "1000",
        "hadoop.tmp.dir": "/tmp/hadoop-${user.name}",
        "mapreduce.job.maps": "2",
        "mapreduce.jobhistory.webapp.rest-csrf.custom-header": "X-XSRF-Header",
        "yarn.log-aggregation.retain-check-interval-seconds": "-1",
        "yarn.resourcemanager.resource-tracker.client.thread-count": "50",
        "yarn.resourcemanager.ha.automatic-failover.zk-base-path": "/yarn-leader-election",
        "fs.AbstractFileSystem.wasb.impl": "org.apache.hadoop.fs.azure.Wasb",
        "mapreduce.client.submit.file.replication": "10",
        "mapreduce.jobhistory.minicluster.fixed.ports": "false",
        "fs.s3a.multipart.threshold": "128M",
        "yarn.resourcemanager.webapp.xfs-filter.xframe-options": "SAMEORIGIN",
        "yarn.rm.system-metrics-publisher.emit-container-events": "false",
        "yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size": "10000",
        "io.seqfile.local.dir": "${hadoop.tmp.dir}/io/local",
        "fs.s3a.s3guard.ddb.throttle.retry.interval": "100ms",
        "mapreduce.jobhistory.done-dir": "${yarn.app.mapreduce.am.staging-dir}/history/done",
        "ipc.server.purge.interval": "15",
        "ipc.client.idlethreshold": "4000",
        "yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage": "false",
        "mapreduce.reduce.input.buffer.percent": "0.0",
        "yarn.nodemanager.runtime.linux.docker.userremapping-gid-threshold": "1",
        "yarn.nodemanager.webapp.rest-csrf.enabled": "false",
        "yarn.resourcemanager.history-writer.multi-threaded-dispatcher.pool-size": "10",
        "fs.ftp.host.port": "21",
        "ipc.ping.interval": "60000",
        "yarn.resourcemanager.admin.address": "${yarn.resourcemanager.hostname}:8033",
        "file.client-write-packet-size": "65536",
        "ipc.client.kill.max": "10",
        "mapreduce.reduce.speculative": "true",
        "ipc.client.connection.maxidletime": "10000",
        "mapreduce.task.io.sort.mb": "100",
        "yarn.nodemanager.localizer.client.thread-count": "5",
        "io.erasurecode.codec.rs.rawcoders": "rs_native,rs_java",
        "io.erasurecode.codec.rs-legacy.rawcoders": "rs-legacy_java",
        "yarn.nodemanager.localizer.cache.cleanup.interval-ms": "600000",
        "yarn.nodemanager.process-kill-wait.ms": "5000",
        "mapreduce.job.hdfs-servers": "${fs.defaultFS}",
        "fs.s3a.multiobjectdelete.enable": "true",
        "fs.viewfs.overload.scheme.target.wasb.impl": "org.apache.hadoop.fs.azure.NativeAzureFileSystem",
        "hadoop.security.group.mapping.ldap.search.attr.member": "member",
        "hadoop.security.random.device.file.path": "/dev/urandom",
        "hadoop.security.sensitive-config-keys": "*********(redacted)",
        "hadoop.rpc.socket.factory.class.default": "org.apache.hadoop.net.StandardSocketFactory",
        "yarn.intermediate-data-encryption.enable": "false",
        "hadoop.security.key.default.bitlength": "128",
        "mapreduce.job.reducer.unconditional-preempt.delay.sec": "300",
        "yarn.nodemanager.disk-health-checker.interval-ms": "120000",
        "yarn.nodemanager.log.deletion-threads-count": "4",
        "fs.s3a.committer.abort.pending.uploads": "true",
        "yarn.webapp.filter-entity-list-by-user": "false",
        "yarn.resourcemanager.activities-manager.app-activities.ttl-ms": "600000",
        "yarn.sharedcache.admin.address": "0.0.0.0:8047",
        "yarn.resourcemanager.placement-constraints.algorithm.iterator": "SERIAL",
        "hadoop.security.crypto.codec.classes.aes.ctr.nopadding": "org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec, org.apache.hadoop.crypto.JceAesCtrCryptoCodec",
        "mapreduce.job.cache.limit.max-resources-mb": "0",
        "fs.s3a.connection.ssl.enabled": "true",
        "yarn.app.mapreduce.am.webapp.https.client.auth": "false",
        "hadoop.workaround.non.threadsafe.getpwuid": "true",
        "fs.df.interval": "60000",
        "ipc.[port_number].decay-scheduler.thresholds": "13,25,50",
        "yarn.sharedcache.cleaner.resource-sleep-ms": "0",
        "yarn.nodemanager.disk-health-checker.min-healthy-disks": "0.25",
        "hadoop.shell.missing.defaultFs.warning": "false",
        "io.file.buffer.size": "65536",
        "fs.s3a.s3guard.ddb.max.retries": "9",
        "fs.viewfs.overload.scheme.target.file.impl": "org.apache.hadoop.fs.LocalFileSystem",
        "yarn.resourcemanager.connect.retry-interval.ms": "30000",
        "yarn.nodemanager.container.stderr.pattern": "{*stderr*,*STDERR*}",
        "yarn.scheduler.minimum-allocation-mb": "1024",
        "hadoop.http.cross-origin.max-age": "1800",
        "fs.s3a.connection.establish.timeout": "5000",
        "yarn.federation.state-store.class": "org.apache.hadoop.yarn.server.federation.store.impl.MemoryFederationStateStore",
        "mapreduce.reduce.log.level": "INFO",
        "yarn.resourcemanager.placement-constraints.retry-attempts": "3",
        "yarn.nodemanager.vmem-pmem-ratio": "2.1",
        "hadoop.rpc.protection": "authentication",
        "yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size": "10",
        "yarn.app.mapreduce.am.staging-dir": "/tmp/hadoop-yarn/staging",
        "mapreduce.reduce.shuffle.read.timeout": "180000",
        "io.erasurecode.codec.xor.rawcoders": "xor_native,xor_java",
        "fs.s3a.s3guard.consistency.retry.limit": "7",
        "mapreduce.job.running.map.limit": "0",
        "yarn.minicluster.control-resource-monitoring": "false",
        "hadoop.ssl.require.client.cert": "false",
        "hadoop.kerberos.kinit.command": "kinit",
        "adl.http.timeout": "-1",
        "hadoop.security.dns.log-slow-lookups.threshold.ms": "1000",
        "mapreduce.job.ubertask.enable": "false",
        "hadoop.caller.context.enabled": "false",
        "hadoop.security.group.mapping.ldap.num.attempts": "3",
        "ha.health-monitor.rpc-timeout.ms": "45000",
        "yarn.nodemanager.remote-app-log-dir": "/tmp/logs",
        "hadoop.zk.timeout-ms": "10000",
        "fs.s3a.s3guard.cli.prune.age": "86400000",
        "yarn.nodemanager.resource.pcores-vcores-multiplier": "1.0",
        "yarn.nodemanager.runtime.linux.sandbox-mode": "disabled",
        "fs.viewfs.overload.scheme.target.webhdfs.impl": "org.apache.hadoop.hdfs.web.WebHdfsFileSystem",
        "fs.s3a.committer.threads": "8",
        "yarn.nodemanager.delete.thread-count": "4",
        "mapreduce.job.finish-when-all-reducers-done": "true",
        "hadoop.registry.jaas.context": "Client",
        "yarn.timeline-service.leveldb-timeline-store.path": "${hadoop.tmp.dir}/yarn/timeline",
        "io.map.index.interval": "128",
        "mapreduce.jobhistory.webapp.rest-csrf.enabled": "false",
        "fs.s3a.change.detection.version.required": "true",
        "yarn.nodemanager.localizer.fetch.thread-count": "4",
        "yarn.resourcemanager.scheduler.client.thread-count": "50",
        "hadoop.ssl.hostname.verifier": "DEFAULT",
        "yarn.timeline-service.leveldb-state-store.path": "${hadoop.tmp.dir}/yarn/timeline",
        "hadoop.zk.retry-interval-ms": "1000",
        "hadoop.security.crypto.buffer.size": "8192",
        "yarn.nodemanager.node-labels.provider.fetch-interval-ms": "600000",
        "mapreduce.jobhistory.recovery.store.leveldb.path": "${hadoop.tmp.dir}/mapred/history/recoverystore",
        "yarn.client.failover-retries-on-socket-timeouts": "0",
        "fs.s3a.ssl.channel.mode": "default_jsse",
        "yarn.nodemanager.resource.memory.enabled": "false",
        "fs.azure.authorization.caching.enable": "true",
        "hadoop.security.instrumentation.requires.admin": "false",
        "yarn.resourcemanager.nm-container-queuing.max-queue-wait-time-ms": "100",
        "fs.abfs.impl": "org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem",
        "mapreduce.job.counters.max": "120",
        "yarn.timeline-service.store-class": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore",
        "mapreduce.jobhistory.move.interval-ms": "180000",
        "mapreduce.job.classloader": "false",
        "mapreduce.task.profile.map.params": "${mapreduce.task.profile.params}",
        "ipc.client.connect.timeout": "20000",
        "hadoop.security.auth_to_local.mechanism": "hadoop",
        "yarn.resourcemanager.reservation-system.planfollower.time-step": "1000",
        "yarn.resourcemanager.activities-manager.scheduler-activities.ttl-ms": "600000",
        "yarn.nodemanager.runtime.linux.docker.enable-userremapping.allowed": "true",
        "yarn.webapp.api-service.enable": "false",
        "yarn.nodemanager.container.stderr.tail.bytes": "4096",
        "yarn.nodemanager.disk-health-checker.disk-free-space-threshold.enabled": "true",
        "hadoop.security.group.mapping.ldap.read.timeout.ms": "60000",
        "hadoop.security.groups.cache.warn.after.ms": "5000",
        "file.bytes-per-checksum": "512",
        "mapreduce.outputcommitter.factory.scheme.s3a": "org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory",
        "yarn.timeline-service.app-collector.linger-period.ms": "60000",
        "yarn.nm.liveness-monitor.expiry-interval-ms": "600000",
        "yarn.nodemanager.recovery.enabled": "false",
        "mapreduce.job.end-notification.retry.interval": "1000",
        "fs.du.interval": "600000",
        "fs.ftp.impl": "org.apache.hadoop.fs.ftp.FTPFileSystem",
        "hadoop.security.groups.cache.background.reload": "false",
        "yarn.nodemanager.container-monitor.enabled": "true",
        "yarn.nodemanager.elastic-memory-control.enabled": "false",
        "net.topology.script.number.args": "100",
        "mapreduce.task.merge.progress.records": "10000",
        "yarn.nodemanager.container-executor.exit-code-file.timeout-ms": "2000",
        "mapreduce.fileoutputcommitter.algorithm.version": "1",
        "yarn.sharedcache.root-dir": "/sharedcache",
        "fs.s3a.retry.throttle.limit": "20",
        "hadoop.http.authentication.type": "simple",
        "fs.viewfs.overload.scheme.target.oss.impl": "org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem",
        "mapreduce.job.cache.limit.max-resources": "0",
        "mapreduce.task.userlog.limit.kb": "0",
        "ipc.[port_number].weighted-cost.handler": "1",
        "yarn.resourcemanager.scheduler.monitor.enable": "false",
        "ipc.client.connect.max.retries": "10",
        "hadoop.registry.zk.retry.times": "5",
        "yarn.nodemanager.localizer.address": "${yarn.nodemanager.hostname}:8040",
        "yarn.timeline-service.keytab": "/etc/krb5.keytab",
        "mapreduce.reduce.shuffle.fetch.retry.timeout-ms": "30000",
        "yarn.resourcemanager.rm.container-allocation.expiry-interval-ms": "600000",
        "yarn.resourcemanager.work-preserving-recovery.enabled": "true",
        "mapreduce.map.skip.maxrecords": "0",
        "yarn.nodemanager.resource-monitor.interval-ms": "3000",
        "yarn.nodemanager.resource-plugins.gpu.allowed-gpu-devices": "auto",
        "mapreduce.job.sharedcache.mode": "disabled",
        "mapreduce.map.cpu.vcores": "1",
        "mapreduce.job.reducer.preempt.delay.sec": "0",
        "hadoop.util.hash.type": "murmur",
        "yarn.nodemanager.webapp.rest-csrf.custom-header": "X-XSRF-Header",
        "mapreduce.shuffle.listen.queue.size": "128",
        "yarn.scheduler.configuration.mutation.acl-policy.class": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.DefaultConfigurationMutationACLPolicy",
        "yarn.log-aggregation.file-formats": "TFile",
        "yarn.timeline-service.client.fd-retain-secs": "300",
        "fs.s3a.select.output.csv.field.delimiter": ",",
        "yarn.nodemanager.health-checker.timeout-ms": "1200000",
        "hadoop.user.group.static.mapping.overrides": "dr.who=;",
        "fs.azure.sas.expiry.period": "90d",
        "fs.s3a.select.output.csv.record.delimiter": "\\n",
        "mapreduce.jobhistory.recovery.store.class": "org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService",
        "fs.viewfs.overload.scheme.target.https.impl": "org.apache.hadoop.fs.http.HttpsFileSystem",
        "fs.s3a.s3guard.ddb.table.sse.enabled": "false",
        "yarn.resourcemanager.fail-fast": "${yarn.fail-fast}",
        "yarn.resourcemanager.proxy-user-privileges.enabled": "false",
        "yarn.router.webapp.interceptor-class.pipeline": "org.apache.hadoop.yarn.server.router.webapp.DefaultRequestInterceptorREST",
        "yarn.nodemanager.resource.memory.cgroups.soft-limit-percentage": "90.0",
        "yarn.nodemanager.disk-validator": "basic",
        "yarn.nodemanager.linux-container-executor.cgroups.delete-timeout-ms": "1000",
        "fs.AbstractFileSystem.file.impl": "org.apache.hadoop.fs.local.LocalFs",
        "yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds": "-1",
        "mapreduce.jobhistory.cleaner.interval-ms": "86400000",
        "yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs": "*********(redacted)",
        "hadoop.ssl.server.conf": "ssl-server.xml",
        "fs.s3a.retry.throttle.interval": "100ms",
        "mapreduce.client.completion.pollinterval": "5000",
        "hadoop.ssl.keystores.factory.class": "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory",
        "yarn.timeline-service.entity-group-fs-store.done-dir": "/tmp/entity-file-history/done/",
        "yarn.resourcemanager.fs.state-store.uri": "${hadoop.tmp.dir}/yarn/system/rmstore",
        "mapreduce.jobhistory.always-scan-user-dir": "false",
        "yarn.app.mapreduce.client.job.max-retries": "3",
        "fs.viewfs.overload.scheme.target.ftp.impl": "org.apache.hadoop.fs.ftp.FTPFileSystem",
        "mapreduce.reduce.shuffle.retry-delay.max.ms": "60000",
        "hadoop.security.group.mapping.ldap.connection.timeout.ms": "60000",
        "mapreduce.task.profile.params": "-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s",
        "yarn.app.mapreduce.shuffle.log.backups": "0",
        "yarn.nodemanager.container-diagnostics-maximum-size": "10000",
        "hadoop.registry.zk.retry.interval.ms": "1000",
        "hadoop.registry.zk.quorum": "localhost:2181",
        "yarn.nodemanager.runtime.linux.runc.allowed-container-runtimes": "runc",
        "mapreduce.output.fileoutputformat.compress": "false",
        "fs.s3a.assumed.role.session.duration": "30m",
        "hadoop.security.group.mapping.ldap.conversion.rule": "none",
        "seq.io.sort.factor": "100",
        "fs.viewfs.overload.scheme.target.ofs.impl": "org.apache.hadoop.fs.ozone.RootedOzoneFileSystem",
        "yarn.sharedcache.cleaner.initial-delay-mins": "10",
        "yarn.app.mapreduce.am.resource.cpu-vcores": "1",
        "yarn.timeline-service.enabled": "false",
        "yarn.nodemanager.runtime.linux.docker.capabilities": "CHOWN,DAC_OVERRIDE,FSETID,FOWNER,MKNOD,NET_RAW,SETGID,SETUID,SETFCAP,SETPCAP,NET_BIND_SERVICE,SYS_CHROOT,KILL,AUDIT_WRITE",
        "yarn.acl.enable": "false",
        "hadoop.prometheus.endpoint.enabled": "false",
        "hadoop.security.group.mapping.ldap.num.attempts.before.failover": "3",
        "mapreduce.task.profile": "false",
        "fs.s3a.metadatastore.metadata.ttl": "15m",
        "yarn.nodemanager.opportunistic-containers-use-pause-for-preemption": "false",
        "yarn.nodemanager.resource.percentage-physical-cpu-limit": "100",
        "yarn.resourcemanager.opportunistic.max.container-allocation.per.am.heartbeat": "-1",
        "ipc.[port_number].cost-provider.impl": "org.apache.hadoop.ipc.DefaultCostProvider",
        "yarn.nodemanager.resource.memory.cgroups.swappiness": "0",
        "yarn.timeline-service.address": "${yarn.timeline-service.hostname}:10200",
        "ha.failover-controller.graceful-fence.connection.retries": "1",
        "yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user": "nobody",
        "yarn.timeline-service.reader.class": "org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineReaderImpl",
        "yarn.resourcemanager.configuration.provider-class": "org.apache.hadoop.yarn.LocalConfigurationProvider",
        "yarn.nodemanager.runtime.linux.docker.userremapping-uid-threshold": "1",
        "yarn.resourcemanager.configuration.file-system-based-store": "/yarn/conf",
        "mapreduce.job.cache.limit.max-single-resource-mb": "0",
        "yarn.nodemanager.runtime.linux.docker.stop.grace-period": "10",
        "yarn.resourcemanager.resource-profiles.source-file": "resource-profiles.json",
        "mapreduce.job.dfs.storage.capacity.kill-limit-exceed": "false",
        "mapreduce.jobhistory.client.thread-count": "10",
        "tfile.fs.input.buffer.size": "262144",
        "mapreduce.client.progressmonitor.pollinterval": "1000",
        "yarn.nodemanager.log-dirs": "${yarn.log.dir}/userlogs",
        "fs.automatic.close": "true",
        "yarn.resourcemanager.delegation-token-renewer.thread-retry-interval": "*********(redacted)",
        "fs.s3a.select.input.csv.quote.character": "\"",
        "yarn.nodemanager.hostname": "0.0.0.0",
        "yarn.nodemanager.runtime.linux.runc.manifest-to-resources-plugin": "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.HdfsManifestToResourcesPlugin",
        "yarn.nodemanager.remote-app-log-dir-include-older": "true",
        "ftp.stream-buffer-size": "4096",
        "yarn.fail-fast": "false",
        "yarn.nodemanager.runtime.linux.runc.layer-mounts-to-keep": "100",
        "yarn.timeline-service.app-aggregation-interval-secs": "15",
        "hadoop.security.group.mapping.ldap.search.filter.user": "(&(objectClass=user)(sAMAccountName={0}))",
        "ipc.[port_number].weighted-cost.lockshared": "10",
        "yarn.nodemanager.container-localizer.log.level": "INFO",
        "mapreduce.job.ubertask.maxmaps": "9",
        "fs.s3a.threads.keepalivetime": "60",
        "mapreduce.jobhistory.webapp.rest-csrf.methods-to-ignore": "GET,OPTIONS,HEAD",
        "mapreduce.task.files.preserve.failedtasks": "false",
        "yarn.app.mapreduce.client.job.retry-interval": "2000",
        "fs.s3a.select.output.csv.quote.escape.character": "\\\\",
        "yarn.timeline-service.client.drain-entities.timeout.ms": "2000",
        "yarn.nodemanager.resource-plugins.fpga.vendor-plugin.class": "org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin",
        "mapreduce.job.encrypted-intermediate-data.buffer.kb": "128",
        "fs.client.resolve.remote.symlinks": "true",
        "fs.s3a.executor.capacity": "16",
        "yarn.timeline-service.entity-group-fs-store.retain-seconds": "604800",
        "yarn.nodemanager.local-dirs": "${hadoop.tmp.dir}/nm-local-dir",
        "yarn.sharedcache.store.in-memory.staleness-period-mins": "10080",
        "fs.adl.impl": "org.apache.hadoop.fs.adl.AdlFileSystem",
        "yarn.resourcemanager.nodemanager.minimum.version": "NONE",
        "yarn.timeline-service.reader.webapp.https.address": "${yarn.timeline-service.webapp.https.address}",
        "yarn.resourcemanager.delegation.token.max-lifetime": "*********(redacted)",
        "hadoop.kerberos.keytab.login.autorenewal.enabled": "false",
        "yarn.resourcemanager.nodemanagers.heartbeat-interval-min-ms": "1000",
        "yarn.timeline-service.entity-group-fs-store.summary-store": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore",
        "mapreduce.reduce.cpu.vcores": "1",
        "yarn.nodemanager.webapp.https.address": "0.0.0.0:8044",
        "hadoop.http.cross-origin.allowed-origins": "*",
        "mapreduce.job.encrypted-intermediate-data": "false",
        "yarn.nodemanager.disk-health-checker.disk-utilization-threshold.enabled": "true",
        "yarn.resourcemanager.metrics.runtime.buckets": "60,300,1440",
        "yarn.timeline-service.generic-application-history.max-applications": "10000",
        "mapreduce.shuffle.connection-keep-alive.enable": "false",
        "yarn.node-labels.configuration-type": "centralized",
        "fs.s3a.path.style.access": "false",
        "yarn.nodemanager.aux-services.mapreduce_shuffle.class": "org.apache.hadoop.mapred.ShuffleHandler",
        "yarn.resourcemanager.application.max-tags": "10",
        "hadoop.domainname.resolver.impl": "org.apache.hadoop.net.DNSDomainNameResolver",
        "mapreduce.jobhistory.webapp.xfs-filter.xframe-options": "SAMEORIGIN",
        "yarn.app.mapreduce.am.staging-dir.erasurecoding.enabled": "false",
        "net.topology.impl": "org.apache.hadoop.net.NetworkTopology",
        "io.map.index.skip": "0",
        "fs.ftp.data.connection.mode": "ACTIVE_LOCAL_DATA_CONNECTION_MODE",
        "yarn.nodemanager.log-aggregation.compression-type": "none",
        "yarn.timeline-service.version": "1.0f",
        "yarn.resourcemanager.system-metrics-publisher.timeline-server-v1.batch-size": "1000",
        "fs.s3a.select.errors.include.sql": "false",
        "fs.s3a.connection.request.timeout": "0",
        "yarn.nodemanager.runtime.linux.docker.host-pid-namespace.allowed": "false",
        "yarn.nodemanager.recovery.dir": "${hadoop.tmp.dir}/yarn-nm-recovery",
        "fs.s3a.max.total.tasks": "32",
        "mapreduce.job.local-fs.single-disk-limit.check.kill-limit-exceed": "true",
        "fs.azure.buffer.dir": "${hadoop.tmp.dir}/abfs",
        "yarn.scheduler.maximum-allocation-vcores": "4",
        "hadoop.http.cross-origin.allowed-headers": "X-Requested-With,Content-Type,Accept,Origin",
        "yarn.ipc.rpc.class": "org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC",
        "mapreduce.reduce.maxattempts": "4",
        "hadoop.security.dns.log-slow-lookups.enabled": "false",
        "mapreduce.job.committer.setup.cleanup.needed": "true",
        "hadoop.security.secure.random.impl": "org.apache.hadoop.crypto.random.OpensslSecureRandom",
        "mapreduce.job.running.reduce.limit": "0",
        "ipc.maximum.response.length": "134217728",
        "yarn.resourcemanager.webapp.rest-csrf.methods-to-ignore": "GET,OPTIONS,HEAD",
        "mapreduce.job.token.tracking.ids.enabled": "*********(redacted)",
        "hadoop.caller.context.max.size": "128",
        "yarn.nodemanager.runtime.linux.docker.delayed-removal.allowed": "false",
        "hadoop.registry.system.acls": "sasl:yarn@, sasl:mapred@, sasl:hdfs@",
        "fs.s3a.fast.upload.buffer": "disk",
        "mapreduce.jobhistory.intermediate-done-dir": "${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate",
        "yarn.app.mapreduce.shuffle.log.separate": "true",
        "yarn.log-aggregation.debug.filesize": "104857600",
        "fs.s3a.readahead.range": "64K",
        "hadoop.http.authentication.simple.anonymous.allowed": "true",
        "fs.s3a.attempts.maximum": "20",
        "yarn.resourcemanager.delegation-token-renewer.thread-timeout": "*********(redacted)",
        "yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size": "10000",
        "yarn.nodemanager.aux-services.manifest.reload-ms": "0",
        "yarn.nodemanager.emit-container-events": "true",
        "yarn.resourcemanager.resource-profiles.enabled": "false",
        "yarn.timeline-service.hbase-schema.prefix": "prod.",
        "fs.azure.authorization": "false",
        "yarn.resourcemanager.decommissioning-nodes-watcher.poll-interval-secs": "20",
        "hadoop.security.group.mapping.ldap.search.group.hierarchy.levels": "0",
        "yarn.resourcemanager.fs.state-store.retry-interval-ms": "1000",
        "yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin.hdfs-hash-file": "/runc-root/image-tag-to-hash",
        "mapreduce.job.speculative.retry-after-speculate": "15000",
        "hadoop.registry.zk.connection.timeout.ms": "15000",
        "yarn.resourcemanager.delegation-token-renewer.thread-count": "*********(redacted)",
        "mapreduce.map.log.level": "INFO",
        "ha.failover-controller.active-standby-elector.zk.op.retries": "3",
        "mapreduce.output.fileoutputformat.compress.type": "RECORD",
        "yarn.resourcemanager.leveldb-state-store.path": "${hadoop.tmp.dir}/yarn/system/rmstore",
        "yarn.timeline-service.webapp.rest-csrf.custom-header": "X-XSRF-Header",
        "mapreduce.ifile.readahead.bytes": "4194304",
        "yarn.sharedcache.app-checker.class": "org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker",
        "yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users": "true",
        "yarn.nodemanager.resource.detect-hardware-capabilities": "false",
        "mapreduce.cluster.acls.enabled": "false",
        "mapreduce.job.speculative.retry-after-no-speculate": "1000",
        "fs.viewfs.overload.scheme.target.abfs.impl": "org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem",
        "file.stream-buffer-size": "4096",
        "yarn.resourcemanager.application-timeouts.monitor.interval-ms": "3000",
        "mapreduce.map.output.compress.codec": "org.apache.hadoop.io.compress.DefaultCodec",
        "mapreduce.map.speculative": "true",
        "yarn.nodemanager.linux-container-executor.cgroups.mount": "false",
        "mapreduce.job.reduce.slowstart.completedmaps": "0.05",
        "yarn.timeline-service.client.internal-timers-ttl-secs": "420",
        "fs.s3a.select.output.csv.quote.character": "\"",
        "hadoop.http.logs.enabled": "true",
        "yarn.nodemanager.logaggregation.threadpool-size-max": "100",
        "fs.AbstractFileSystem.hdfs.impl": "org.apache.hadoop.fs.Hdfs",
        "yarn.nodemanager.disk-health-checker.enable": "true",
        "fs.s3a.select.output.csv.quote.fields": "always",
        "yarn.resourcemanager.delegation-token-renewer.thread-retry-max-attempts": "*********(redacted)",
        "yarn.app.mapreduce.am.container.log.backups": "0",
        "yarn.app.mapreduce.am.log.level": "INFO",
        "yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin": "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.ImageTagToManifestPlugin",
        "io.bytes.per.checksum": "512",
        "yarn.timeline-service.http-authentication.type": "simple",
        "hadoop.security.group.mapping.ldap.search.attr.group.name": "cn",
        "yarn.nodemanager.resource-plugins.fpga.allowed-fpga-devices": "auto",
        "fs.s3a.block.size": "32M",
        "yarn.sharedcache.client-server.address": "0.0.0.0:8045",
        "yarn.resourcemanager.hostname": "0.0.0.0",
        "yarn.resourcemanager.delegation.key.update-interval": "86400000",
        "mapreduce.reduce.shuffle.fetch.retry.enabled": "${yarn.nodemanager.recovery.enabled}",
        "mapreduce.map.memory.mb": "-1",
        "mapreduce.task.skip.start.attempts": "2",
        "ipc.client.tcpnodelay": "true",
        "ipc.client.rpc-timeout.ms": "0",
        "yarn.nodemanager.webapp.rest-csrf.methods-to-ignore": "GET,OPTIONS,HEAD",
        "yarn.router.interceptor.user.threadpool-size": "5",
        "fs.AbstractFileSystem.har.impl": "org.apache.hadoop.fs.HarFs",
        "mapreduce.job.split.metainfo.maxsize": "10000000",
        "yarn.am.liveness-monitor.expiry-interval-ms": "600000",
        "yarn.nodemanager.runtime.linux.runc.hdfs-manifest-to-resources-plugin.stat-cache-timeout-interval-secs": "360",
        "fs.s3a.socket.recv.buffer": "8192",
        "rpc.metrics.timeunit": "MILLISECONDS",
        "yarn.scheduler.configuration.fs.path": "file://${hadoop.tmp.dir}/yarn/system/schedconf",
        "mapreduce.reduce.memory.mb": "-1",
        "ipc.client.low-latency": "false",
        "mapreduce.input.lineinputformat.linespermap": "1",
        "ipc.client.connect.max.retries.on.timeouts": "45",
        "yarn.timeline-service.leveldb-timeline-store.read-cache-size": "104857600",
        "yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs": "*********(redacted)",
        "yarn.timeline-service.entity-group-fs-store.app-cache-size": "10",
        "yarn.resourcemanager.resource-tracker.address": "${yarn.resourcemanager.hostname}:8031",
        "yarn.nodemanager.node-labels.provider.fetch-timeout-ms": "1200000",
        "mapreduce.job.heap.memory-mb.ratio": "0.8",
        "yarn.resourcemanager.leveldb-state-store.compaction-interval-secs": "3600",
        "yarn.resourcemanager.webapp.rest-csrf.custom-header": "X-XSRF-Header",
        "yarn.nodemanager.pluggable-device-framework.enabled": "false",
        "mapreduce.client.output.filter": "FAILED",
        "hadoop.http.filter.initializers": "org.apache.hadoop.http.lib.StaticUserWebFilter",
        "mapreduce.fileoutputcommitter.task.cleanup.enabled": "false",
        "ipc.[port_number].weighted-cost.lockfree": "1",
        "yarn.nodemanager.opportunistic-containers-max-queue-length": "0",
        "yarn.resourcemanager.state-store.max-completed-applications": "${yarn.resourcemanager.max-completed-applications}",
        "mapreduce.job.speculative.minimum-allowed-tasks": "10",
        "fs.s3a.aws.credentials.provider": "\n    org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider,\n    org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider,\n    com.amazonaws.auth.EnvironmentVariableCredentialsProvider,\n    org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider\n  ",
        "yarn.log-aggregation.retain-seconds": "-1",
        "yarn.timeline-service.hostname": "0.0.0.0",
        "file.replication": "1",
        "yarn.nodemanager.container-metrics.unregister-delay-ms": "10000",
        "yarn.nodemanager.container-metrics.period-ms": "-1",
        "yarn.nodemanager.log.retain-seconds": "10800",
        "yarn.timeline-service.entity-group-fs-store.cleaner-interval-seconds": "3600",
        "ipc.[port_number].callqueue.impl": "java.util.concurrent.LinkedBlockingQueue",
        "yarn.resourcemanager.keytab": "/etc/krb5.keytab",
        "hadoop.security.group.mapping.providers.combined": "true",
        "mapreduce.reduce.merge.inmem.threshold": "1000",
        "yarn.timeline-service.recovery.enabled": "false",
        "fs.azure.saskey.usecontainersaskeyforallaccess": "true",
        "yarn.sharedcache.nm.uploader.thread-count": "20",
        "yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs": "3600",
        "mapreduce.shuffle.ssl.enabled": "false",
        "yarn.timeline-service.hbase.coprocessor.app-final-value-retention-milliseconds": "259200000",
        "yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb": "0",
        "mapreduce.jobhistory.max-age-ms": "604800000",
        "hadoop.http.cross-origin.allowed-methods": "GET,POST,HEAD",
        "yarn.resourcemanager.opportunistic-container-allocation.enabled": "false",
        "mapreduce.jobhistory.webapp.address": "0.0.0.0:19888",
        "hadoop.system.tags": "YARN,HDFS,NAMENODE,DATANODE,REQUIRED,SECURITY,KERBEROS,PERFORMANCE,CLIENT\n      ,SERVER,DEBUG,DEPRECATED,COMMON,OPTIONAL",
        "yarn.log-aggregation.file-controller.TFile.class": "org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController",
        "yarn.client.nodemanager-connect.max-wait-ms": "180000",
        "yarn.resourcemanager.webapp.address": "${yarn.resourcemanager.hostname}:8088",
        "mapreduce.jobhistory.recovery.enable": "false",
        "mapreduce.reduce.shuffle.parallelcopies": "5",
        "yarn.app.mapreduce.client.max-retries": "3",
        "hadoop.security.authentication": "simple",
        "fs.s3a.select.input.csv.comment.marker": "#",
        "mapreduce.job.queuename": "default",
        "mapreduce.job.encrypted-intermediate-data-key-size-bits": "128",
        "yarn.nodemanager.webapp.xfs-filter.xframe-options": "SAMEORIGIN",
        "fs.AbstractFileSystem.webhdfs.impl": "org.apache.hadoop.fs.WebHdfs",
        "fs.trash.interval": "0",
        "mapreduce.task.profile.reduce.params": "${mapreduce.task.profile.params}",
        "yarn.app.mapreduce.am.resource.mb": "1536",
        "mapreduce.input.fileinputformat.list-status.num-threads": "1",
        "yarn.nodemanager.container-executor.class": "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor",
        "io.mapfile.bloom.size": "1048576",
        "yarn.timeline-service.ttl-ms": "604800000",
        "yarn.resourcemanager.nm-container-queuing.min-queue-length": "5",
        "yarn.nodemanager.resource.cpu-vcores": "-1",
        "mapreduce.job.reduces": "1",
        "fs.s3a.multipart.size": "64M",
        "yarn.scheduler.minimum-allocation-vcores": "1",
        "mapreduce.job.speculative.speculative-cap-total-tasks": "0.01",
        "hadoop.ssl.client.conf": "ssl-client.xml",
        "fs.s3a.metadatastore.authoritative": "false",
        "ipc.[port_number].weighted-cost.response": "1",
        "ha.health-monitor.sleep-after-disconnect.ms": "1000",
        "yarn.app.mapreduce.shuffle.log.limit.kb": "0",
        "hadoop.security.group.mapping": "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback",
        "yarn.client.application-client-protocol.poll-timeout-ms": "-1",
        "yarn.resourcemanager.application.max-tag.length": "100",
        "hadoop.http.staticuser.user": "dr.who",
        "yarn.nodemanager.linux-container-executor.resources-handler.class": "org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler",
        "mapreduce.reduce.shuffle.memory.limit.percent": "0.25",
        "yarn.resourcemanager.reservation-system.enable": "false",
        "mapreduce.map.output.compress": "false",
        "ha.zookeeper.acl": "world:anyone:rwcda",
        "ipc.server.max.connections": "0",
        "yarn.nodemanager.runtime.linux.docker.default-container-network": "host",
        "yarn.router.webapp.address": "0.0.0.0:8089",
        "yarn.scheduler.maximum-allocation-mb": "8192",
        "yarn.nodemanager.resource-plugins.gpu.docker-plugin.nvidia-docker-v1.endpoint": "http://localhost:3476/v1.0/docker/cli",
        "yarn.app.mapreduce.am.container.log.limit.kb": "0",
        "mapreduce.jobhistory.jhist.format": "binary",
        "mapreduce.task.stuck.timeout-ms": "600000",
        "yarn.resourcemanager.ha.enabled": "false",
        "dfs.client.ignore.namenode.default.kms.uri": "false",
        "mapreduce.task.exit.timeout.check-interval-ms": "20000",
        "mapreduce.jobhistory.intermediate-user-done-dir.permissions": "770",
        "mapreduce.task.exit.timeout": "60000",
        "yarn.resourcemanager.scheduler.monitor.policies": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy",
        "yarn.sharedcache.cleaner.period-mins": "1440",
        "ipc.client.connect.retry.interval": "1000",
        "yarn.timeline-service.http-cross-origin.enabled": "false",
        "fs.wasbs.impl": "org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure",
        "yarn.resourcemanager.nodemanagers.heartbeat-interval-max-ms": "1000",
        "yarn.federation.subcluster-resolver.class": "org.apache.hadoop.yarn.server.federation.resolver.DefaultSubClusterResolverImpl",
        "yarn.resourcemanager.zk-state-store.parent-path": "/rmstore",
        "fs.s3a.select.input.csv.field.delimiter": ",",
        "mapreduce.jobhistory.cleaner.enable": "true",
        "yarn.timeline-service.client.fd-flush-interval-secs": "10",
        "hadoop.security.kms.client.encrypted.key.cache.expiry": "43200000",
        "yarn.resourcemanager.nm-container-queuing.sorting-nodes-interval-ms": "1000",
        "fs.s3a.committer.staging.tmp.path": "tmp/staging",
        "yarn.nodemanager.sleep-delay-before-sigkill.ms": "250",
        "yarn.nodemanager.resource.count-logical-processors-as-cores": "false",
        "hadoop.registry.zk.root": "/registry",
        "mapreduce.client.libjars.wildcard": "true",
        "fs.s3a.committer.staging.unique-filenames": "true",
        "yarn.nodemanager.node-attributes.provider.fetch-timeout-ms": "1200000",
        "yarn.client.nodemanager-client-async.thread-pool-max-size": "500",
        "mapreduce.map.maxattempts": "4",
        "yarn.resourcemanager.nm-container-queuing.min-queue-wait-time-ms": "10",
        "mapreduce.job.end-notification.retry.attempts": "0",
        "adl.feature.ownerandgroup.enableupn": "false",
        "yarn.resourcemanager.zk-max-znode-size.bytes": "1048576",
        "mapreduce.job.reduce.shuffle.consumer.plugin.class": "org.apache.hadoop.mapreduce.task.reduce.Shuffle",
        "yarn.resourcemanager.delayed.delegation-token.removal-interval-ms": "*********(redacted)",
        "yarn.nodemanager.localizer.cache.target-size-mb": "10240",
        "fs.s3a.committer.staging.conflict-mode": "append",
        "fs.s3a.list.version": "2",
        "ftp.client-write-packet-size": "65536",
        "yarn.nodemanager.container-log-monitor.enable": "false",
        "hadoop.security.key.default.cipher": "AES/CTR/NoPadding",
        "mapreduce.job.local-fs.single-disk-limit.check.interval-ms": "5000",
        "net.topology.node.switch.mapping.impl": "org.apache.hadoop.net.ScriptBasedMapping",
        "ipc.[port_number].decay-scheduler.period-ms": "5000",
        "yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin.cache-refresh-interval-secs": "60",
        "map.sort.class": "org.apache.hadoop.util.QuickSort",
        "fs.viewfs.rename.strategy": "SAME_MOUNTPOINT",
        "hadoop.security.kms.client.authentication.retry-count": "1",
        "yarn.nodemanager.runtime.linux.runc.privileged-containers.allowed": "false",
        "ipc.[port_number].weighted-cost.lockexclusive": "100",
        "fs.AbstractFileSystem.adl.impl": "org.apache.hadoop.fs.adl.Adl",
        "yarn.client.failover-retries": "0",
        "fs.s3a.multipart.purge.age": "86400",
        "yarn.nodemanager.amrmproxy.address": "0.0.0.0:8049",
        "ipc.server.listen.queue.size": "256",
        "fs.permissions.umask-mode": "022",
        "fs.s3a.assumed.role.credentials.provider": "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider",
        "yarn.nodemanager.vmem-check-enabled": "true",
        "yarn.nodemanager.health-checker.run-before-startup": "false",
        "mapreduce.job.max.map": "-1",
        "mapreduce.job.ubertask.maxreduces": "1",
        "mapreduce.shuffle.pathcache.max-weight": "10485760",
        "hadoop.security.kms.client.encrypted.key.cache.size": "500",
        "yarn.nodemanager.env-whitelist": "JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ",
        "yarn.registry.class": "org.apache.hadoop.registry.client.impl.FSRegistryOperationsService",
        "mapreduce.jobhistory.admin.acl": "*",
        "yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size": "10",
        "yarn.nodemanager.runtime.linux.runc.hdfs-manifest-to-resources-plugin.stat-cache-size": "500",
        "yarn.timeline-service.webapp.rest-csrf.enabled": "false",
        "yarn.nodemanager.disk-health-checker.min-free-space-per-disk-watermark-high-mb": "0",
        "yarn.nodemanager.numa-awareness.enabled": "false",
        "yarn.nodemanager.recovery.compaction-interval-secs": "3600",
        "yarn.app.mapreduce.client-am.ipc.max-retries": "3",
        "yarn.resourcemanager.system-metrics-publisher.timeline-server-v1.interval-seconds": "60",
        "yarn.federation.registry.base-dir": "yarnfederation/",
        "mapreduce.job.local-fs.single-disk-limit.bytes": "-1",
        "mapreduce.shuffle.pathcache.concurrency-level": "16",
        "hadoop.security.java.secure.random.algorithm": "SHA1PRNG",
        "ha.failover-controller.cli-check.rpc-timeout.ms": "20000",
        "mapreduce.jobhistory.jobname.limit": "50",
        "fs.s3a.select.input.compression": "none",
        "yarn.client.nodemanager-connect.retry-interval-ms": "10000",
        "ipc.[port_number].scheduler.priority.levels": "4",
        "yarn.timeline-service.state-store-class": "org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore",
        "yarn.sharedcache.nested-level": "3",
        "yarn.timeline-service.webapp.rest-csrf.methods-to-ignore": "GET,OPTIONS,HEAD",
        "fs.azure.user.agent.prefix": "unknown",
        "yarn.resourcemanager.zk-delegation-token-node.split-index": "*********(redacted)",
        "yarn.nodemanager.numa-awareness.read-topology": "false",
        "yarn.nodemanager.webapp.address": "${yarn.nodemanager.hostname}:8042",
        "rpc.metrics.quantile.enable": "false",
        "yarn.scheduler.queue-placement-rules": "user-group",
        "hadoop.http.authentication.kerberos.keytab": "${user.home}/hadoop.keytab",
        "yarn.resourcemanager.recovery.enabled": "false",
        "fs.s3a.select.input.csv.header": "none"
    },
    "System Properties": {
        "java.io.tmpdir": "/tmp",
        "line.separator": "\n",
        "path.separator": ":",
        "user.home": "/home/chenhao",
        "file.separator": "/",
        "user.timezone": "PRC",
        "sun.os.patch.level": "unknown",
        "java.vm.specification.vendor": "Oracle Corporation",
        "user.country": "US",
        "awt.toolkit": "sun.awt.X11.XToolkit",
        "os.name": "Linux",
        "sun.management.compiler": "HotSpot 64-Bit Tiered Compilers",
        "sun.cpu.endian": "little",
        "java.specification.version": "11",
        "java.vm.specification.name": "Java Virtual Machine Specification",
        "java.vendor": "Oracle Corporation",
        "java.vm.specification.version": "11",
        "sun.arch.data.model": "64",
        "sun.boot.library.path": "/home/chenhao/libs/jdk11/lib",
        "user.dir": "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/work/driver-20230414161317-0013",
        "java.library.path": "/usr/lib64:/lib64:/public/software/gcc/lib64:/public/software//mpi/openmpi/1.8.7/intel/lib/openmpi:/public/software//mpi/openmpi/1.8.7/intel/lib:/public/software/compiler/composer_xe_2015.0.090/compiler/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/mpirt/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/ipp/../compiler/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/ipp/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/compiler/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/mkl/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/tbb/lib/intel64/gcc4.4:/public/software/compiler/composer_xe_2015.0.090/compiler/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/mpirt/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/ipp/../compiler/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/ipp/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/compiler/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/mkl/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/tbb/lib/intel64/gcc4.4:/public/software/compiler/composer_xe_2015.0.090/compiler/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/mpirt/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/ipp/../compiler/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/ipp/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/compiler/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/mkl/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/tbb/lib/intel64/gcc4.4:/public/software/g09/bsd:/public/software/g09/local:/public/software/g09/extras:/public/software/g09:/opt/gridview//pbs//dispatcher//lib:/public/software/compiler/composer_xe_2015.0.090/compiler/lib/intel64:/public/software/compiler/composer_xe_2015.0.090/mkl/lib/intel64::/usr/local/lib64:/usr/local/lib:/public/software/g09:/public/software/gv/lib:/public/software/MaterialsStudio8.0/lib:/public/software/MaterialsStudio8.0/lib/32:/public/software/gcc/mpc/lib:/opt/hadoop/hadoop-2.7.6-5nodes/lib/native:/usr/local/lib/libdisni/lib:/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib",
        "sun.cpu.isalist": "",
        "os.arch": "amd64",
        "java.vm.version": "11.0.16.1+1-LTS-1",
        "jetty.git.hash": "6b67c5719d1f4371b33655ff2d047d24e171e49a",
        "java.runtime.version": "11.0.16.1+1-LTS-1",
        "java.vm.info": "mixed mode",
        "java.runtime.name": "Java(TM) SE Runtime Environment",
        "java.version.date": "2022-08-18",
        "java.class.version": "55.0",
        "java.specification.name": "Java Platform API Specification",
        "file.encoding": "UTF-8",
        "java.specification.vendor": "Oracle Corporation",
        "sun.java.launcher": "SUN_STANDARD",
        "java.vm.compressedOopsMode": "32-bit",
        "os.version": "2.6.32-220.el6.x86_64",
        "sun.jnu.encoding": "UTF-8",
        "user.language": "en",
        "java.vendor.version": "18.9",
        "java.vendor.url": "https://openjdk.java.net/",
        "java.awt.printerjob": "sun.print.PSPrinterJob",
        "java.awt.graphicsenv": "sun.awt.X11GraphicsEnvironment",
        "java.vm.vendor": "Oracle Corporation",
        "jdk.debug": "release",
        "java.vendor.url.bug": "https://bugreport.java.com/bugreport/",
        "user.name": "chenhao",
        "java.vm.name": "Java HotSpot(TM) 64-Bit Server VM",
        "sun.java.command": "org.apache.spark.deploy.worker.DriverWrapper spark://Worker@11.11.0.85:56250 /home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/work/driver-20230414161317-0013/SparkTemplate-1.3-jar-with-dependencies.jar name.spade5.Main",
        "java.home": "/home/chenhao/libs/jdk11",
        "java.version": "11.0.16.1",
        "sun.io.unicode.encoding": "UnicodeLittle"
    },
    "Classpath Entries": {
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/javolution-5.5.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-serde-2.3.9.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/datanucleus-core-4.1.17.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-storageclass-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spire-util_2.13-0.17.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-beeline-2.3.9.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/scala-reflect-2.13.8.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/javassist-3.25.0-GA.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-transport-native-kqueue-4.1.74.Final-osx-x86_64.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-llap-common-2.3.9.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-yarn_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/libthrift-0.12.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/cats-kernel_2.13-2.1.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spire_2.13-0.17.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-service-rpc-3.1.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/parquet-encoding-1.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/arpack_combined_all-0.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jackson-core-2.13.4.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/json4s-scalap_2.13-3.7.0-M11.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-autoscaling-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jdo-api-3.0.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/minlog-1.3.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/metrics-graphite-4.2.7.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-discovery-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-codec-1.15.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/httpcore-4.4.14.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/json-1.8.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/protobuf-java-2.5.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/guava-14.0.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spire-macros_2.13-0.17.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/HikariCP-2.5.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/rocksdbjni-6.20.3.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/chill-java-0.10.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-resolver-4.1.74.Final.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jakarta.servlet-api-4.0.3.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hadoop-client-runtime-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jna-5.9.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-logging-1.1.3.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/chill_2.13-0.10.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-math3-3.6.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-tags_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-cli-1.5.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/curator-recipes-2.13.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jersey-client-2.36.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/parquet-format-structures-1.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/json4s-ast_2.13-3.7.0-M11.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/osgi-resource-locator-1.0.3.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/compress-lzf-1.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/curator-framework-2.13.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/conf/": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/datanucleus-rdbms-4.1.19.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/oro-2.0.8.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/xz-1.9.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-cli-2.3.9.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-shims-0.23-2.3.9.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-jdbc-2.3.9.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/leveldbjni-all-1.8.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/scala-parser-combinators_2.13-1.1.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/zstd-jni-1.5.2-1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-metastore-2.3.9.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-graphx_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/threeten-extra-1.5.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/JTransforms-3.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/log4j-core-2.17.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/flatbuffers-java-1.12.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-tags_2.13-3.3.2-tests.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/istack-commons-runtime-3.0.8.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jackson-mapper-asl-1.9.13.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jersey-common-2.36.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-network-shuffle_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-coordination-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/annotations-17.0.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/avro-ipc-1.11.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/parquet-jackson-1.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/univocity-parsers-2.9.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/log4j-api-2.17.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-transport-native-epoll-4.1.74.Final-linux-x86_64.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jakarta.validation-api-2.0.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jakarta.xml.bind-api-2.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/slf4j-api-1.7.32.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jakarta.annotation-api-1.3.5.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/json4s-jackson_2.13-3.7.0-M11.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jpam-1.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/lz4-java-1.8.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-collections4-4.4.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/gson-2.2.4.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-buffer-4.1.74.Final.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-tcnative-classes-2.0.48.Final.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-extensions-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/orc-core-1.7.8.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-transport-4.1.74.Final.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/scala-collection-compat_2.13-2.1.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-io-2.11.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/metrics-core-4.2.7.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/zjsonpatch-0.3.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/paranamer-2.8.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/lapack-2.2.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/tink-1.6.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hadoop-client-api-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hk2-locator-2.6.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/okio-1.14.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/bonecp-0.8.0.RELEASE.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/snappy-java-1.1.8.4.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jersey-hk2-2.36.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spire-platform_2.13-0.17.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-mllib-local_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/JLargeArrays-1.5.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-unsafe_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/core-1.1.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jackson-module-scala_2.13-2.13.4.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-transport-native-epoll-4.1.74.Final-linux-aarch_64.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/breeze-macros_2.13-1.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/httpclient-4.5.13.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jta-1.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/derby-10.14.2.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-sql_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jakarta.inject-2.6.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/scala-compiler-2.13.8.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-vector-code-gen-2.3.9.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-metrics-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/curator-client-2.13.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-shims-common-2.3.9.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/zookeeper-3.6.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/antlr4-runtime-4.8.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jersey-container-servlet-2.36.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/algebra_2.13-2.0.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-compiler-3.0.16.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/datanucleus-api-jdo-4.2.4.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/stream-2.9.6.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/janino-3.0.16.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/shapeless_2.13-2.3.7.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-transport-classes-epoll-4.1.74.Final.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/zookeeper-jute-3.6.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/stax-api-1.0.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/pickle-1.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-dbcp-1.4.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jline-3.21.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/arrow-vector-7.0.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-pool-1.5.4.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/scala-library-2.13.8.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jackson-dataformat-yaml-2.13.4.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-repl_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/metrics-json-4.2.7.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/shims-0.9.25.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/json4s-core_2.13-3.7.0-M11.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-shims-scheduler-2.3.9.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-common-2.3.9.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-launcher_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/opencsv-2.3.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-lang-2.6.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/aopalliance-repackaged-2.6.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-text-1.10.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-shims-2.3.9.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hadoop-yarn-server-web-proxy-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/transaction-api-1.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/okhttp-3.12.12.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-mllib_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hk2-utils-2.6.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jcl-over-slf4j-1.7.32.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-node-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/arrow-memory-core-7.0.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jackson-annotations-2.13.4.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/ivy-2.5.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jakarta.ws.rs-api-2.1.6.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hadoop-shaded-guava-1.1.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/arrow-format-7.0.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/audience-annotations-0.5.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-networking-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jline-2.14.6.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-events-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-exec-2.3.9-core.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/avro-mapred-1.11.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/metrics-jvm-4.2.7.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-kubernetes_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-transport-native-kqueue-4.1.74.Final-osx-aarch_64.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/metrics-jmx-4.2.7.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/javax.jdo-3.2.0-m3.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-collections-3.2.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-hive_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/arrow-memory-netty-7.0.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jul-to-slf4j-1.7.32.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jaxb-runtime-2.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-all-4.1.74.Final.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-lang3-3.12.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/avro-1.11.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-sketch_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jsr305-3.0.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-client-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/breeze_2.13-1.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-flowcontrol-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-network-common_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/scala-parallel-collections_2.13-1.0.3.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/objenesis-3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-admissionregistration-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-apps-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-streaming_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/logging-interceptor-3.12.12.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/orc-mapreduce-1.7.8.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-transport-classes-kqueue-4.1.74.Final.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/antlr-runtime-3.5.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-kvstore_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-common-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-core_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/scala-xml_2.13-1.2.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-common-4.1.74.Final.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-policy-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-handler-4.1.74.Final.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-apiextensions-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hk2-api-2.6.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/velocity-1.5.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/hive-storage-api-2.7.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/parquet-common-1.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-catalyst_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/activation-1.1.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/xbean-asm9-shaded-4.20.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-rbac-5.12.2.jar": "System Classpath",
        "spark://node85:46111/jars/SparkTemplate-1.3-jar-with-dependencies.jar": "Added By User",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jersey-container-servlet-core-2.36.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kryo-shaded-4.0.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jersey-server-2.36.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/blas-2.2.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/libfb303-0.9.3.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/joda-time-2.10.13.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-scheduling-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jackson-core-asl-1.9.13.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/generex-1.0.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/RoaringBitmap-0.9.25.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jodd-core-3.5.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/aircompressor-0.21.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-batch-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/parquet-hadoop-1.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-hive-thriftserver_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/log4j-slf4j-impl-2.17.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/py4j-0.10.9.5.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/ST4-4.0.4.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-compress-1.21.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/automaton-1.11-8.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-core-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/commons-crypto-1.1.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/orc-shims-1.7.8.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-codec-4.1.74.Final.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/netty-transport-native-unix-common-4.1.74.Final.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/super-csv-2.2.0.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/log4j-1.2-api-2.17.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/parquet-column-1.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/spark-mesos_2.13-3.3.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jackson-databind-2.13.4.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/kubernetes-model-certificates-5.12.2.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/jackson-datatype-jsr310-2.13.4.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/arpack-2.2.1.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/snakeyaml-1.31.jar": "System Classpath",
        "/home/chenhao/libs/spark-3.3.2-bin-hadoop3-scala2.13/jars/mesos-1.4.3-shaded-protobuf.jar": "System Classpath"
    }
}
{
    "Event": "SparkListenerApplicationStart",
    "App Name": "WordCount",
    "App ID": "app-20230414161322-0013",
    "Timestamp": 1681460001778,
    "User": "chenhao",
    "Driver Logs": {
        "stderr": "http://11.11.0.85:8081/logPage/?driverId=driver-20230414161317-0013&logType=stderr",
        "stdout": "http://11.11.0.85:8081/logPage/?driverId=driver-20230414161317-0013&logType=stdout"
    }
}
{
    "Event": "SparkListenerJobStart",
    "Job ID": 0,
    "Submission Time": 1681460004838,
    "Stage Infos": [
        {
            "Stage ID": 0,
            "Stage Attempt ID": 0,
            "Stage Name": "repartition at DStream.scala:578",
            "Number of Tasks": 3,
            "RDD Info": [
                {
                    "RDD ID": 1,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"6\",\"name\":\"repartition\",\"parent\":{\"id\":\"1_1681460004000\",\"name\":\"repartition\\n@ 16:13:24\"}}",
                    "Callsite": "repartition at DStream.scala:578",
                    "Parent IDs": [
                        0
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 3,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 0,
                    "Name": "KafkaRDD",
                    "Scope": "{\"id\":\"0_1681460004000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 16:13:24\"}",
                    "Callsite": "createDirectStream at main.scala:31",
                    "Parent IDs": [],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 3,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [],
            "Details": "org.apache.spark.rdd.RDD.repartition(RDD.scala:475)\norg.apache.spark.streaming.dstream.DStream.$anonfun$repartition$2(DStream.scala:578)\norg.apache.spark.streaming.dstream.DStream.$anonfun$transform$2(DStream.scala:668)\norg.apache.spark.streaming.dstream.DStream.$anonfun$transform$4(DStream.scala:682)\norg.apache.spark.streaming.dstream.TransformedDStream.compute(TransformedDStream.scala:46)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.TransformedDStream.createRDDWithLocalProperties(TransformedDStream.scala:65)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\nscala.Option.orElse(Option.scala:477)\norg.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\norg.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\nscala.Option.orElse(Option.scala:477)",
            "Accumulables": [],
            "Resource Profile Id": 0
        },
        {
            "Stage ID": 1,
            "Stage Attempt ID": 0,
            "Stage Name": "map at main.scala:36",
            "Number of Tasks": 30,
            "RDD Info": [
                {
                    "RDD ID": 6,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"3_1681460004000\",\"name\":\"map @ 16:13:24\"}",
                    "Callsite": "map at main.scala:36",
                    "Parent IDs": [
                        5
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 30,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 5,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"2_1681460004000\",\"name\":\"flatMap @ 16:13:24\"}",
                    "Callsite": "flatMap at main.scala:33",
                    "Parent IDs": [
                        4
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 30,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 3,
                    "Name": "CoalescedRDD",
                    "Scope": "{\"id\":\"6\",\"name\":\"repartition\",\"parent\":{\"id\":\"1_1681460004000\",\"name\":\"repartition\\n@ 16:13:24\"}}",
                    "Callsite": "repartition at DStream.scala:578",
                    "Parent IDs": [
                        2
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 30,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 2,
                    "Name": "ShuffledRDD",
                    "Scope": "{\"id\":\"6\",\"name\":\"repartition\",\"parent\":{\"id\":\"1_1681460004000\",\"name\":\"repartition\\n@ 16:13:24\"}}",
                    "Callsite": "repartition at DStream.scala:578",
                    "Parent IDs": [
                        1
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 30,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 4,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"6\",\"name\":\"repartition\",\"parent\":{\"id\":\"1_1681460004000\",\"name\":\"repartition\\n@ 16:13:24\"}}",
                    "Callsite": "repartition at DStream.scala:578",
                    "Parent IDs": [
                        3
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 30,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [
                0
            ],
            "Details": "org.apache.spark.streaming.dstream.DStream.map(DStream.scala:548)\nname.spade5.Main$.main(main.scala:36)\nname.spade5.Main.main(main.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:566)\norg.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:63)\norg.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)",
            "Accumulables": [],
            "Resource Profile Id": 0
        },
        {
            "Stage ID": 2,
            "Stage Attempt ID": 0,
            "Stage Name": "saveAsTextFiles at main.scala:36",
            "Number of Tasks": 2,
            "RDD Info": [
                {
                    "RDD ID": 8,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"5_1681460004000\",\"name\":\"saveAsTextFiles\\n@ 16:13:24\"}",
                    "Callsite": "saveAsTextFiles at main.scala:36",
                    "Parent IDs": [
                        7
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 2,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 7,
                    "Name": "ShuffledRDD",
                    "Scope": "{\"id\":\"4_1681460004000\",\"name\":\"reduceByKey\\n@ 16:13:24\"}",
                    "Callsite": "reduceByKey at main.scala:36",
                    "Parent IDs": [
                        6
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 2,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [
                1
            ],
            "Details": "org.apache.spark.streaming.dstream.DStream.saveAsTextFiles(DStream.scala:924)\nname.spade5.Main$.main(main.scala:36)\nname.spade5.Main.main(main.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:566)\norg.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:63)\norg.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)",
            "Accumulables": [],
            "Resource Profile Id": 0
        }
    ],
    "Stage IDs": [
        0,
        1,
        2
    ],
    "Properties": {
        "callSite.long": "org.apache.spark.streaming.dstream.DStream.saveAsTextFiles(DStream.scala:924)\nname.spade5.Main$.main(main.scala:36)\nname.spade5.Main.main(main.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:566)\norg.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:63)\norg.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)",
        "spark.rdd.scope": "{\"id\":\"5_1681460004000\",\"name\":\"saveAsTextFiles\\n@ 16:13:24\"}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1681460004000\">[output operation 0, batch time 16:13:24]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "callSite.short": "saveAsTextFiles at main.scala:36",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1681460004000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 0,
        "Stage Attempt ID": 0,
        "Stage Name": "repartition at DStream.scala:578",
        "Number of Tasks": 3,
        "RDD Info": [
            {
                "RDD ID": 1,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"6\",\"name\":\"repartition\",\"parent\":{\"id\":\"1_1681460004000\",\"name\":\"repartition\\n@ 16:13:24\"}}",
                "Callsite": "repartition at DStream.scala:578",
                "Parent IDs": [
                    0
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 3,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 0,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1681460004000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 16:13:24\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 3,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.repartition(RDD.scala:475)\norg.apache.spark.streaming.dstream.DStream.$anonfun$repartition$2(DStream.scala:578)\norg.apache.spark.streaming.dstream.DStream.$anonfun$transform$2(DStream.scala:668)\norg.apache.spark.streaming.dstream.DStream.$anonfun$transform$4(DStream.scala:682)\norg.apache.spark.streaming.dstream.TransformedDStream.compute(TransformedDStream.scala:46)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.TransformedDStream.createRDDWithLocalProperties(TransformedDStream.scala:65)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\nscala.Option.orElse(Option.scala:477)\norg.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\norg.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\nscala.Option.orElse(Option.scala:477)",
        "Submission Time": 1681460004877,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "callSite.long": "org.apache.spark.streaming.dstream.DStream.saveAsTextFiles(DStream.scala:924)\nname.spade5.Main$.main(main.scala:36)\nname.spade5.Main.main(main.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:566)\norg.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:63)\norg.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)",
        "spark.rdd.scope": "{\"id\":\"5_1681460004000\",\"name\":\"saveAsTextFiles\\n@ 16:13:24\"}",
        "spark.job.interruptOnCancel": "false",
        "resource.executor.cores": "1",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1681460004000\">[output operation 0, batch time 16:13:24]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "callSite.short": "saveAsTextFiles at main.scala:36",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1681460004000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerExecutorAdded",
    "Timestamp": 1681460005681,
    "Executor ID": "0",
    "Executor Info": {
        "Host": "11.11.0.86",
        "Total Cores": 1,
        "Log Urls": {
            "stderr": "http://11.11.0.86:8081/logPage/?appId=app-20230414161322-0013&executorId=0&logType=stderr",
            "stdout": "http://11.11.0.86:8081/logPage/?appId=app-20230414161322-0013&executorId=0&logType=stdout"
        },
        "Attributes": {},
        "Resources": {},
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerBlockManagerAdded",
    "Block Manager ID": {
        "Executor ID": "0",
        "Host": "11.11.0.86",
        "Port": 35466
    },
    "Maximum Memory": 455501414,
    "Timestamp": 1681460005781,
    "Maximum Onheap Memory": 455501414,
    "Maximum Offheap Memory": 0
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 0,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 0,
        "Index": 0,
        "Attempt": 0,
        "Partition ID": 0,
        "Launch Time": 1681460007900,
        "Executor ID": "0",
        "Host": "11.11.0.86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerExecutorAdded",
    "Timestamp": 1681460008761,
    "Executor ID": "1",
    "Executor Info": {
        "Host": "11.11.0.87",
        "Total Cores": 1,
        "Log Urls": {
            "stderr": "http://11.11.0.87:8081/logPage/?appId=app-20230414161322-0013&executorId=1&logType=stderr",
            "stdout": "http://11.11.0.87:8081/logPage/?appId=app-20230414161322-0013&executorId=1&logType=stdout"
        },
        "Attributes": {},
        "Resources": {},
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerExecutorAdded",
    "Timestamp": 1681460008789,
    "Executor ID": "2",
    "Executor Info": {
        "Host": "11.11.0.88",
        "Total Cores": 1,
        "Log Urls": {
            "stderr": "http://11.11.0.88:8081/logPage/?appId=app-20230414161322-0013&executorId=2&logType=stderr",
            "stdout": "http://11.11.0.88:8081/logPage/?appId=app-20230414161322-0013&executorId=2&logType=stdout"
        },
        "Attributes": {},
        "Resources": {},
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerBlockManagerAdded",
    "Block Manager ID": {
        "Executor ID": "1",
        "Host": "11.11.0.87",
        "Port": 44993
    },
    "Maximum Memory": 455501414,
    "Timestamp": 1681460008939,
    "Maximum Onheap Memory": 455501414,
    "Maximum Offheap Memory": 0
}
{
    "Event": "SparkListenerBlockManagerAdded",
    "Block Manager ID": {
        "Executor ID": "2",
        "Host": "11.11.0.88",
        "Port": 41680
    },
    "Maximum Memory": 455501414,
    "Timestamp": 1681460008971,
    "Maximum Onheap Memory": 455501414,
    "Maximum Offheap Memory": 0
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 0,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 1,
        "Index": 1,
        "Attempt": 0,
        "Partition ID": 1,
        "Launch Time": 1681460009419,
        "Executor ID": "0",
        "Host": "11.11.0.86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 0,
        "Stage Attempt ID": 0,
        "Stage Name": "repartition at DStream.scala:578",
        "Number of Tasks": 3,
        "RDD Info": [
            {
                "RDD ID": 1,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"6\",\"name\":\"repartition\",\"parent\":{\"id\":\"1_1681460004000\",\"name\":\"repartition\\n@ 16:13:24\"}}",
                "Callsite": "repartition at DStream.scala:578",
                "Parent IDs": [
                    0
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 3,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 0,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1681460004000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 16:13:24\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 3,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.repartition(RDD.scala:475)\norg.apache.spark.streaming.dstream.DStream.$anonfun$repartition$2(DStream.scala:578)\norg.apache.spark.streaming.dstream.DStream.$anonfun$transform$2(DStream.scala:668)\norg.apache.spark.streaming.dstream.DStream.$anonfun$transform$4(DStream.scala:682)\norg.apache.spark.streaming.dstream.TransformedDStream.compute(TransformedDStream.scala:46)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.TransformedDStream.createRDDWithLocalProperties(TransformedDStream.scala:65)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\nscala.Option.orElse(Option.scala:477)\norg.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\norg.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\nscala.Option.orElse(Option.scala:477)",
        "Submission Time": 1681460004877,
        "Completion Time": 1681460009449,
        "Failure Reason": "Job aborted due to stage failure: task 0.0 in stage 0.0 (TID 0) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord\nSerialization stack:\n\t- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = tweets, partition = 1, leaderEpoch = 0, offset = 0, CreateTime = 1681459447374, serialized key size = -1, serialized value size = 249, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = 550550191565045761,SeekingAlpha,1420096369,Bankers Face BlackBerry Dilemma As EZ Pass Program Expires http://seekingalpha.com/article/2791665-bankers-face-blackberry-dilemma-as-ez-pass-program-expires?source=feed_f… $AAPL #APPLE $SSNLF $BBRY,0,3,0))",
        "Accumulables": [],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerJobEnd",
    "Job ID": 0,
    "Completion Time": 1681460009454,
    "Job Result": {
        "Result": "JobFailed",
        "Exception": {
            "Message": "Job aborted due to stage failure: task 0.0 in stage 0.0 (TID 0) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord\nSerialization stack:\n\t- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = tweets, partition = 1, leaderEpoch = 0, offset = 0, CreateTime = 1681459447374, serialized key size = -1, serialized value size = 249, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = 550550191565045761,SeekingAlpha,1420096369,Bankers Face BlackBerry Dilemma As EZ Pass Program Expires http://seekingalpha.com/article/2791665-bankers-face-blackberry-dilemma-as-ez-pass-program-expires?source=feed_f… $AAPL #APPLE $SSNLF $BBRY,0,3,0))",
            "Stack Trace": [
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGScheduler",
                    "Method Name": "failJobAndIndependentStages",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 2672
                },
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGScheduler",
                    "Method Name": "$anonfun$abortStage$2",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 2608
                },
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGScheduler",
                    "Method Name": "$anonfun$abortStage$2$adapted",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 2607
                },
                {
                    "Declaring Class": "scala.collection.immutable.List",
                    "Method Name": "foreach",
                    "File Name": "List.scala",
                    "Line Number": 333
                },
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGScheduler",
                    "Method Name": "abortStage",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 2607
                },
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGScheduler",
                    "Method Name": "$anonfun$handleTaskSetFailed$1",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 1182
                },
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGScheduler",
                    "Method Name": "$anonfun$handleTaskSetFailed$1$adapted",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 1182
                },
                {
                    "Declaring Class": "scala.Option",
                    "Method Name": "foreach",
                    "File Name": "Option.scala",
                    "Line Number": 437
                },
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGScheduler",
                    "Method Name": "handleTaskSetFailed",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 1182
                },
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop",
                    "Method Name": "doOnReceive",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 2860
                },
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop",
                    "Method Name": "onReceive",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 2802
                },
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop",
                    "Method Name": "onReceive",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 2791
                },
                {
                    "Declaring Class": "org.apache.spark.util.EventLoop$$anon$1",
                    "Method Name": "run",
                    "File Name": "EventLoop.scala",
                    "Line Number": 49
                },
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGScheduler",
                    "Method Name": "runJob",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 952
                },
                {
                    "Declaring Class": "org.apache.spark.SparkContext",
                    "Method Name": "runJob",
                    "File Name": "SparkContext.scala",
                    "Line Number": 2238
                },
                {
                    "Declaring Class": "org.apache.spark.SparkContext",
                    "Method Name": "runJob",
                    "File Name": "SparkContext.scala",
                    "Line Number": 2259
                },
                {
                    "Declaring Class": "org.apache.spark.SparkContext",
                    "Method Name": "runJob",
                    "File Name": "SparkContext.scala",
                    "Line Number": 2291
                },
                {
                    "Declaring Class": "org.apache.spark.internal.io.SparkHadoopWriter$",
                    "Method Name": "write",
                    "File Name": "SparkHadoopWriter.scala",
                    "Line Number": 83
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.PairRDDFunctions",
                    "Method Name": "$anonfun$saveAsHadoopDataset$1",
                    "File Name": "PairRDDFunctions.scala",
                    "Line Number": 1091
                },
                {
                    "Declaring Class": "scala.runtime.java8.JFunction0$mcV$sp",
                    "Method Name": "apply",
                    "File Name": "JFunction0$mcV$sp.scala",
                    "Line Number": 18
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDDOperationScope$",
                    "Method Name": "withScope",
                    "File Name": "RDDOperationScope.scala",
                    "Line Number": 151
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDDOperationScope$",
                    "Method Name": "withScope",
                    "File Name": "RDDOperationScope.scala",
                    "Line Number": 112
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDD",
                    "Method Name": "withScope",
                    "File Name": "RDD.scala",
                    "Line Number": 406
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.PairRDDFunctions",
                    "Method Name": "saveAsHadoopDataset",
                    "File Name": "PairRDDFunctions.scala",
                    "Line Number": 1089
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.PairRDDFunctions",
                    "Method Name": "$anonfun$saveAsHadoopFile$4",
                    "File Name": "PairRDDFunctions.scala",
                    "Line Number": 1062
                },
                {
                    "Declaring Class": "scala.runtime.java8.JFunction0$mcV$sp",
                    "Method Name": "apply",
                    "File Name": "JFunction0$mcV$sp.scala",
                    "Line Number": 18
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDDOperationScope$",
                    "Method Name": "withScope",
                    "File Name": "RDDOperationScope.scala",
                    "Line Number": 151
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDDOperationScope$",
                    "Method Name": "withScope",
                    "File Name": "RDDOperationScope.scala",
                    "Line Number": 112
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDD",
                    "Method Name": "withScope",
                    "File Name": "RDD.scala",
                    "Line Number": 406
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.PairRDDFunctions",
                    "Method Name": "saveAsHadoopFile",
                    "File Name": "PairRDDFunctions.scala",
                    "Line Number": 1027
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.PairRDDFunctions",
                    "Method Name": "$anonfun$saveAsHadoopFile$3",
                    "File Name": "PairRDDFunctions.scala",
                    "Line Number": 1009
                },
                {
                    "Declaring Class": "scala.runtime.java8.JFunction0$mcV$sp",
                    "Method Name": "apply",
                    "File Name": "JFunction0$mcV$sp.scala",
                    "Line Number": 18
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDDOperationScope$",
                    "Method Name": "withScope",
                    "File Name": "RDDOperationScope.scala",
                    "Line Number": 151
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDDOperationScope$",
                    "Method Name": "withScope",
                    "File Name": "RDDOperationScope.scala",
                    "Line Number": 112
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDD",
                    "Method Name": "withScope",
                    "File Name": "RDD.scala",
                    "Line Number": 406
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.PairRDDFunctions",
                    "Method Name": "saveAsHadoopFile",
                    "File Name": "PairRDDFunctions.scala",
                    "Line Number": 1008
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.PairRDDFunctions",
                    "Method Name": "$anonfun$saveAsHadoopFile$2",
                    "File Name": "PairRDDFunctions.scala",
                    "Line Number": 965
                },
                {
                    "Declaring Class": "scala.runtime.java8.JFunction0$mcV$sp",
                    "Method Name": "apply",
                    "File Name": "JFunction0$mcV$sp.scala",
                    "Line Number": 18
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDDOperationScope$",
                    "Method Name": "withScope",
                    "File Name": "RDDOperationScope.scala",
                    "Line Number": 151
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDDOperationScope$",
                    "Method Name": "withScope",
                    "File Name": "RDDOperationScope.scala",
                    "Line Number": 112
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDD",
                    "Method Name": "withScope",
                    "File Name": "RDD.scala",
                    "Line Number": 406
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.PairRDDFunctions",
                    "Method Name": "saveAsHadoopFile",
                    "File Name": "PairRDDFunctions.scala",
                    "Line Number": 963
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDD",
                    "Method Name": "$anonfun$saveAsTextFile$2",
                    "File Name": "RDD.scala",
                    "Line Number": 1599
                },
                {
                    "Declaring Class": "scala.runtime.java8.JFunction0$mcV$sp",
                    "Method Name": "apply",
                    "File Name": "JFunction0$mcV$sp.scala",
                    "Line Number": 18
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDDOperationScope$",
                    "Method Name": "withScope",
                    "File Name": "RDDOperationScope.scala",
                    "Line Number": 151
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDDOperationScope$",
                    "Method Name": "withScope",
                    "File Name": "RDDOperationScope.scala",
                    "Line Number": 112
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDD",
                    "Method Name": "withScope",
                    "File Name": "RDD.scala",
                    "Line Number": 406
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDD",
                    "Method Name": "saveAsTextFile",
                    "File Name": "RDD.scala",
                    "Line Number": 1599
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDD",
                    "Method Name": "$anonfun$saveAsTextFile$1",
                    "File Name": "RDD.scala",
                    "Line Number": 1585
                },
                {
                    "Declaring Class": "scala.runtime.java8.JFunction0$mcV$sp",
                    "Method Name": "apply",
                    "File Name": "JFunction0$mcV$sp.scala",
                    "Line Number": 18
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDDOperationScope$",
                    "Method Name": "withScope",
                    "File Name": "RDDOperationScope.scala",
                    "Line Number": 151
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDDOperationScope$",
                    "Method Name": "withScope",
                    "File Name": "RDDOperationScope.scala",
                    "Line Number": 112
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDD",
                    "Method Name": "withScope",
                    "File Name": "RDD.scala",
                    "Line Number": 406
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDD",
                    "Method Name": "saveAsTextFile",
                    "File Name": "RDD.scala",
                    "Line Number": 1585
                },
                {
                    "Declaring Class": "org.apache.spark.streaming.dstream.DStream",
                    "Method Name": "$anonfun$saveAsTextFiles$2",
                    "File Name": "DStream.scala",
                    "Line Number": 927
                },
                {
                    "Declaring Class": "org.apache.spark.streaming.dstream.DStream",
                    "Method Name": "$anonfun$saveAsTextFiles$2$adapted",
                    "File Name": "DStream.scala",
                    "Line Number": 925
                },
                {
                    "Declaring Class": "org.apache.spark.streaming.dstream.ForEachDStream",
                    "Method Name": "$anonfun$generateJob$2",
                    "File Name": "ForEachDStream.scala",
                    "Line Number": 51
                },
                {
                    "Declaring Class": "scala.runtime.java8.JFunction0$mcV$sp",
                    "Method Name": "apply",
                    "File Name": "JFunction0$mcV$sp.scala",
                    "Line Number": 18
                },
                {
                    "Declaring Class": "org.apache.spark.streaming.dstream.DStream",
                    "Method Name": "createRDDWithLocalProperties",
                    "File Name": "DStream.scala",
                    "Line Number": 417
                },
                {
                    "Declaring Class": "org.apache.spark.streaming.dstream.ForEachDStream",
                    "Method Name": "$anonfun$generateJob$1",
                    "File Name": "ForEachDStream.scala",
                    "Line Number": 51
                },
                {
                    "Declaring Class": "scala.runtime.java8.JFunction0$mcV$sp",
                    "Method Name": "apply",
                    "File Name": "JFunction0$mcV$sp.scala",
                    "Line Number": 18
                },
                {
                    "Declaring Class": "scala.util.Try$",
                    "Method Name": "apply",
                    "File Name": "Try.scala",
                    "Line Number": 210
                },
                {
                    "Declaring Class": "org.apache.spark.streaming.scheduler.Job",
                    "Method Name": "run",
                    "File Name": "Job.scala",
                    "Line Number": 39
                },
                {
                    "Declaring Class": "org.apache.spark.streaming.scheduler.JobScheduler$JobHandler",
                    "Method Name": "$anonfun$run$1",
                    "File Name": "JobScheduler.scala",
                    "Line Number": 256
                },
                {
                    "Declaring Class": "scala.runtime.java8.JFunction0$mcV$sp",
                    "Method Name": "apply",
                    "File Name": "JFunction0$mcV$sp.scala",
                    "Line Number": 18
                },
                {
                    "Declaring Class": "scala.util.DynamicVariable",
                    "Method Name": "withValue",
                    "File Name": "DynamicVariable.scala",
                    "Line Number": 59
                },
                {
                    "Declaring Class": "org.apache.spark.streaming.scheduler.JobScheduler$JobHandler",
                    "Method Name": "run",
                    "File Name": "JobScheduler.scala",
                    "Line Number": 256
                },
                {
                    "Declaring Class": "java.util.concurrent.ThreadPoolExecutor",
                    "Method Name": "runWorker",
                    "File Name": "ThreadPoolExecutor.java",
                    "Line Number": 1128
                },
                {
                    "Declaring Class": "java.util.concurrent.ThreadPoolExecutor$Worker",
                    "Method Name": "run",
                    "File Name": "ThreadPoolExecutor.java",
                    "Line Number": 628
                },
                {
                    "Declaring Class": "java.lang.Thread",
                    "Method Name": "run",
                    "File Name": "Thread.java",
                    "Line Number": 834
                }
            ]
        }
    }
}
{
    "Event": "SparkListenerJobStart",
    "Job ID": 1,
    "Submission Time": 1681460009581,
    "Stage Infos": [
        {
            "Stage ID": 3,
            "Stage Attempt ID": 0,
            "Stage Name": "repartition at DStream.scala:578",
            "Number of Tasks": 3,
            "RDD Info": [
                {
                    "RDD ID": 10,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"7\",\"name\":\"repartition\",\"parent\":{\"id\":\"1_1681460005000\",\"name\":\"repartition\\n@ 16:13:25\"}}",
                    "Callsite": "repartition at DStream.scala:578",
                    "Parent IDs": [
                        9
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 3,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 9,
                    "Name": "KafkaRDD",
                    "Scope": "{\"id\":\"0_1681460005000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 16:13:25\"}",
                    "Callsite": "createDirectStream at main.scala:31",
                    "Parent IDs": [],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 3,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [],
            "Details": "org.apache.spark.rdd.RDD.repartition(RDD.scala:475)\norg.apache.spark.streaming.dstream.DStream.$anonfun$repartition$2(DStream.scala:578)\norg.apache.spark.streaming.dstream.DStream.$anonfun$transform$2(DStream.scala:668)\norg.apache.spark.streaming.dstream.DStream.$anonfun$transform$4(DStream.scala:682)\norg.apache.spark.streaming.dstream.TransformedDStream.compute(TransformedDStream.scala:46)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.TransformedDStream.createRDDWithLocalProperties(TransformedDStream.scala:65)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\nscala.Option.orElse(Option.scala:477)\norg.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\norg.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\nscala.Option.orElse(Option.scala:477)",
            "Accumulables": [],
            "Resource Profile Id": 0
        },
        {
            "Stage ID": 4,
            "Stage Attempt ID": 0,
            "Stage Name": "map at main.scala:36",
            "Number of Tasks": 30,
            "RDD Info": [
                {
                    "RDD ID": 15,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"3_1681460005000\",\"name\":\"map @ 16:13:25\"}",
                    "Callsite": "map at main.scala:36",
                    "Parent IDs": [
                        14
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 30,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 11,
                    "Name": "ShuffledRDD",
                    "Scope": "{\"id\":\"7\",\"name\":\"repartition\",\"parent\":{\"id\":\"1_1681460005000\",\"name\":\"repartition\\n@ 16:13:25\"}}",
                    "Callsite": "repartition at DStream.scala:578",
                    "Parent IDs": [
                        10
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 30,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 13,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"7\",\"name\":\"repartition\",\"parent\":{\"id\":\"1_1681460005000\",\"name\":\"repartition\\n@ 16:13:25\"}}",
                    "Callsite": "repartition at DStream.scala:578",
                    "Parent IDs": [
                        12
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 30,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 14,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"2_1681460005000\",\"name\":\"flatMap @ 16:13:25\"}",
                    "Callsite": "flatMap at main.scala:33",
                    "Parent IDs": [
                        13
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 30,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 12,
                    "Name": "CoalescedRDD",
                    "Scope": "{\"id\":\"7\",\"name\":\"repartition\",\"parent\":{\"id\":\"1_1681460005000\",\"name\":\"repartition\\n@ 16:13:25\"}}",
                    "Callsite": "repartition at DStream.scala:578",
                    "Parent IDs": [
                        11
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 30,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [
                3
            ],
            "Details": "org.apache.spark.streaming.dstream.DStream.map(DStream.scala:548)\nname.spade5.Main$.main(main.scala:36)\nname.spade5.Main.main(main.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:566)\norg.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:63)\norg.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)",
            "Accumulables": [],
            "Resource Profile Id": 0
        },
        {
            "Stage ID": 5,
            "Stage Attempt ID": 0,
            "Stage Name": "saveAsTextFiles at main.scala:36",
            "Number of Tasks": 2,
            "RDD Info": [
                {
                    "RDD ID": 49,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"5_1681460005000\",\"name\":\"saveAsTextFiles\\n@ 16:13:25\"}",
                    "Callsite": "saveAsTextFiles at main.scala:36",
                    "Parent IDs": [
                        16
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 2,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 16,
                    "Name": "ShuffledRDD",
                    "Scope": "{\"id\":\"4_1681460005000\",\"name\":\"reduceByKey\\n@ 16:13:25\"}",
                    "Callsite": "reduceByKey at main.scala:36",
                    "Parent IDs": [
                        15
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 2,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [
                4
            ],
            "Details": "org.apache.spark.streaming.dstream.DStream.saveAsTextFiles(DStream.scala:924)\nname.spade5.Main$.main(main.scala:36)\nname.spade5.Main.main(main.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:566)\norg.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:63)\norg.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)",
            "Accumulables": [],
            "Resource Profile Id": 0
        }
    ],
    "Stage IDs": [
        3,
        4,
        5
    ],
    "Properties": {
        "callSite.long": "org.apache.spark.streaming.dstream.DStream.saveAsTextFiles(DStream.scala:924)\nname.spade5.Main$.main(main.scala:36)\nname.spade5.Main.main(main.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:566)\norg.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:63)\norg.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)",
        "spark.rdd.scope": "{\"id\":\"5_1681460005000\",\"name\":\"saveAsTextFiles\\n@ 16:13:25\"}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1681460005000\">[output operation 0, batch time 16:13:25]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "callSite.short": "saveAsTextFiles at main.scala:36",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1681460005000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 3,
        "Stage Attempt ID": 0,
        "Stage Name": "repartition at DStream.scala:578",
        "Number of Tasks": 3,
        "RDD Info": [
            {
                "RDD ID": 10,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"7\",\"name\":\"repartition\",\"parent\":{\"id\":\"1_1681460005000\",\"name\":\"repartition\\n@ 16:13:25\"}}",
                "Callsite": "repartition at DStream.scala:578",
                "Parent IDs": [
                    9
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 3,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 9,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1681460005000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 16:13:25\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 3,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.repartition(RDD.scala:475)\norg.apache.spark.streaming.dstream.DStream.$anonfun$repartition$2(DStream.scala:578)\norg.apache.spark.streaming.dstream.DStream.$anonfun$transform$2(DStream.scala:668)\norg.apache.spark.streaming.dstream.DStream.$anonfun$transform$4(DStream.scala:682)\norg.apache.spark.streaming.dstream.TransformedDStream.compute(TransformedDStream.scala:46)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.TransformedDStream.createRDDWithLocalProperties(TransformedDStream.scala:65)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\nscala.Option.orElse(Option.scala:477)\norg.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\norg.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\nscala.Option.orElse(Option.scala:477)",
        "Submission Time": 1681460009590,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "callSite.long": "org.apache.spark.streaming.dstream.DStream.saveAsTextFiles(DStream.scala:924)\nname.spade5.Main$.main(main.scala:36)\nname.spade5.Main.main(main.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:566)\norg.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:63)\norg.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)",
        "spark.rdd.scope": "{\"id\":\"5_1681460005000\",\"name\":\"saveAsTextFiles\\n@ 16:13:25\"}",
        "spark.job.interruptOnCancel": "false",
        "resource.executor.cores": "1",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1681460005000\">[output operation 0, batch time 16:13:25]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "callSite.short": "saveAsTextFiles at main.scala:36",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1681460005000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 3,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 2,
        "Index": 2,
        "Attempt": 0,
        "Partition ID": 2,
        "Launch Time": 1681460010382,
        "Executor ID": "0",
        "Host": "11.11.0.86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerTaskEnd",
    "Stage ID": 0,
    "Stage Attempt ID": 0,
    "Task Type": "ShuffleMapTask",
    "Task End Reason": {
        "Reason": "TaskKilled",
        "Kill Reason": "Stage cancelled",
        "Accumulator Updates": [
            {
                "ID": 2,
                "Update": "933",
                "Internal": false,
                "Count Failed Values": true
            },
            {
                "ID": 4,
                "Update": "0",
                "Internal": false,
                "Count Failed Values": true
            },
            {
                "ID": 20,
                "Update": "4684945",
                "Internal": false,
                "Count Failed Values": true
            }
        ]
    },
    "Task Info": {
        "Task ID": 1,
        "Index": 1,
        "Attempt": 0,
        "Partition ID": 1,
        "Launch Time": 1681460009419,
        "Executor ID": "0",
        "Host": "11.11.0.86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 1681460010385,
        "Failed": false,
        "Killed": true,
        "Accumulables": []
    },
    "Task Executor Metrics": {
        "JVMHeapMemory": 0,
        "JVMOffHeapMemory": 0,
        "OnHeapExecutionMemory": 0,
        "OffHeapExecutionMemory": 0,
        "OnHeapStorageMemory": 0,
        "OffHeapStorageMemory": 0,
        "OnHeapUnifiedMemory": 0,
        "OffHeapUnifiedMemory": 0,
        "DirectPoolMemory": 0,
        "MappedPoolMemory": 0,
        "ProcessTreeJVMVMemory": 0,
        "ProcessTreeJVMRSSMemory": 0,
        "ProcessTreePythonVMemory": 0,
        "ProcessTreePythonRSSMemory": 0,
        "ProcessTreeOtherVMemory": 0,
        "ProcessTreeOtherRSSMemory": 0,
        "MinorGCCount": 0,
        "MinorGCTime": 0,
        "MajorGCCount": 0,
        "MajorGCTime": 0,
        "TotalGCTime": 0
    },
    "Task Metrics": {
        "Executor Deserialize Time": 0,
        "Executor Deserialize CPU Time": 0,
        "Executor Run Time": 933,
        "Executor CPU Time": 0,
        "Peak Execution Memory": 0,
        "Result Size": 0,
        "JVM GC Time": 0,
        "Result Serialization Time": 0,
        "Memory Bytes Spilled": 0,
        "Disk Bytes Spilled": 0,
        "Shuffle Read Metrics": {
            "Remote Blocks Fetched": 0,
            "Local Blocks Fetched": 0,
            "Fetch Wait Time": 0,
            "Remote Bytes Read": 0,
            "Remote Bytes Read To Disk": 0,
            "Local Bytes Read": 0,
            "Total Records Read": 0
        },
        "Shuffle Write Metrics": {
            "Shuffle Bytes Written": 0,
            "Shuffle Write Time": 4684945,
            "Shuffle Records Written": 0
        },
        "Input Metrics": {
            "Bytes Read": 0,
            "Records Read": 0
        },
        "Output Metrics": {
            "Bytes Written": 0,
            "Records Written": 0
        },
        "Updated Blocks": []
    }
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 3,
        "Stage Attempt ID": 0,
        "Stage Name": "repartition at DStream.scala:578",
        "Number of Tasks": 3,
        "RDD Info": [
            {
                "RDD ID": 10,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"7\",\"name\":\"repartition\",\"parent\":{\"id\":\"1_1681460005000\",\"name\":\"repartition\\n@ 16:13:25\"}}",
                "Callsite": "repartition at DStream.scala:578",
                "Parent IDs": [
                    9
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 3,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 9,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1681460005000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 16:13:25\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 3,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.repartition(RDD.scala:475)\norg.apache.spark.streaming.dstream.DStream.$anonfun$repartition$2(DStream.scala:578)\norg.apache.spark.streaming.dstream.DStream.$anonfun$transform$2(DStream.scala:668)\norg.apache.spark.streaming.dstream.DStream.$anonfun$transform$4(DStream.scala:682)\norg.apache.spark.streaming.dstream.TransformedDStream.compute(TransformedDStream.scala:46)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.TransformedDStream.createRDDWithLocalProperties(TransformedDStream.scala:65)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\nscala.Option.orElse(Option.scala:477)\norg.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\norg.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\nscala.Option.orElse(Option.scala:477)",
        "Submission Time": 1681460009590,
        "Completion Time": 1681460011406,
        "Failure Reason": "Job aborted due to stage failure: task 2.0 in stage 3.0 (TID 2) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord\nSerialization stack:\n\t- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = tweets, partition = 0, leaderEpoch = 0, offset = 30000, CreateTime = 1681459447914, serialized key size = -1, serialized value size = 152, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = 567109830125830144,stockwire24,1424044494,\"Microsoft Corporation (MSFT); Wells Fargo & Co (WFC), American Express ... $MSFT http://bit.ly/1yIjnLN\",0,0,0))",
        "Accumulables": [],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerJobEnd",
    "Job ID": 1,
    "Completion Time": 1681460011407,
    "Job Result": {
        "Result": "JobFailed",
        "Exception": {
            "Message": "Job aborted due to stage failure: task 2.0 in stage 3.0 (TID 2) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord\nSerialization stack:\n\t- object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = tweets, partition = 0, leaderEpoch = 0, offset = 30000, CreateTime = 1681459447914, serialized key size = -1, serialized value size = 152, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = 567109830125830144,stockwire24,1424044494,\"Microsoft Corporation (MSFT); Wells Fargo & Co (WFC), American Express ... $MSFT http://bit.ly/1yIjnLN\",0,0,0))",
            "Stack Trace": [
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGScheduler",
                    "Method Name": "failJobAndIndependentStages",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 2672
                },
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGScheduler",
                    "Method Name": "$anonfun$abortStage$2",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 2608
                },
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGScheduler",
                    "Method Name": "$anonfun$abortStage$2$adapted",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 2607
                },
                {
                    "Declaring Class": "scala.collection.immutable.List",
                    "Method Name": "foreach",
                    "File Name": "List.scala",
                    "Line Number": 333
                },
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGScheduler",
                    "Method Name": "abortStage",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 2607
                },
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGScheduler",
                    "Method Name": "$anonfun$handleTaskSetFailed$1",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 1182
                },
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGScheduler",
                    "Method Name": "$anonfun$handleTaskSetFailed$1$adapted",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 1182
                },
                {
                    "Declaring Class": "scala.Option",
                    "Method Name": "foreach",
                    "File Name": "Option.scala",
                    "Line Number": 437
                },
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGScheduler",
                    "Method Name": "handleTaskSetFailed",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 1182
                },
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop",
                    "Method Name": "doOnReceive",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 2860
                },
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop",
                    "Method Name": "onReceive",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 2802
                },
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop",
                    "Method Name": "onReceive",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 2791
                },
                {
                    "Declaring Class": "org.apache.spark.util.EventLoop$$anon$1",
                    "Method Name": "run",
                    "File Name": "EventLoop.scala",
                    "Line Number": 49
                },
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGScheduler",
                    "Method Name": "runJob",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 952
                },
                {
                    "Declaring Class": "org.apache.spark.SparkContext",
                    "Method Name": "runJob",
                    "File Name": "SparkContext.scala",
                    "Line Number": 2238
                },
                {
                    "Declaring Class": "org.apache.spark.SparkContext",
                    "Method Name": "runJob",
                    "File Name": "SparkContext.scala",
                    "Line Number": 2259
                },
                {
                    "Declaring Class": "org.apache.spark.SparkContext",
                    "Method Name": "runJob",
                    "File Name": "SparkContext.scala",
                    "Line Number": 2291
                },
                {
                    "Declaring Class": "org.apache.spark.internal.io.SparkHadoopWriter$",
                    "Method Name": "write",
                    "File Name": "SparkHadoopWriter.scala",
                    "Line Number": 83
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.PairRDDFunctions",
                    "Method Name": "$anonfun$saveAsHadoopDataset$1",
                    "File Name": "PairRDDFunctions.scala",
                    "Line Number": 1091
                },
                {
                    "Declaring Class": "scala.runtime.java8.JFunction0$mcV$sp",
                    "Method Name": "apply",
                    "File Name": "JFunction0$mcV$sp.scala",
                    "Line Number": 18
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDDOperationScope$",
                    "Method Name": "withScope",
                    "File Name": "RDDOperationScope.scala",
                    "Line Number": 151
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDDOperationScope$",
                    "Method Name": "withScope",
                    "File Name": "RDDOperationScope.scala",
                    "Line Number": 112
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDD",
                    "Method Name": "withScope",
                    "File Name": "RDD.scala",
                    "Line Number": 406
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.PairRDDFunctions",
                    "Method Name": "saveAsHadoopDataset",
                    "File Name": "PairRDDFunctions.scala",
                    "Line Number": 1089
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.PairRDDFunctions",
                    "Method Name": "$anonfun$saveAsHadoopFile$4",
                    "File Name": "PairRDDFunctions.scala",
                    "Line Number": 1062
                },
                {
                    "Declaring Class": "scala.runtime.java8.JFunction0$mcV$sp",
                    "Method Name": "apply",
                    "File Name": "JFunction0$mcV$sp.scala",
                    "Line Number": 18
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDDOperationScope$",
                    "Method Name": "withScope",
                    "File Name": "RDDOperationScope.scala",
                    "Line Number": 151
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDDOperationScope$",
                    "Method Name": "withScope",
                    "File Name": "RDDOperationScope.scala",
                    "Line Number": 112
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDD",
                    "Method Name": "withScope",
                    "File Name": "RDD.scala",
                    "Line Number": 406
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.PairRDDFunctions",
                    "Method Name": "saveAsHadoopFile",
                    "File Name": "PairRDDFunctions.scala",
                    "Line Number": 1027
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.PairRDDFunctions",
                    "Method Name": "$anonfun$saveAsHadoopFile$3",
                    "File Name": "PairRDDFunctions.scala",
                    "Line Number": 1009
                },
                {
                    "Declaring Class": "scala.runtime.java8.JFunction0$mcV$sp",
                    "Method Name": "apply",
                    "File Name": "JFunction0$mcV$sp.scala",
                    "Line Number": 18
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDDOperationScope$",
                    "Method Name": "withScope",
                    "File Name": "RDDOperationScope.scala",
                    "Line Number": 151
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDDOperationScope$",
                    "Method Name": "withScope",
                    "File Name": "RDDOperationScope.scala",
                    "Line Number": 112
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDD",
                    "Method Name": "withScope",
                    "File Name": "RDD.scala",
                    "Line Number": 406
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.PairRDDFunctions",
                    "Method Name": "saveAsHadoopFile",
                    "File Name": "PairRDDFunctions.scala",
                    "Line Number": 1008
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.PairRDDFunctions",
                    "Method Name": "$anonfun$saveAsHadoopFile$2",
                    "File Name": "PairRDDFunctions.scala",
                    "Line Number": 965
                },
                {
                    "Declaring Class": "scala.runtime.java8.JFunction0$mcV$sp",
                    "Method Name": "apply",
                    "File Name": "JFunction0$mcV$sp.scala",
                    "Line Number": 18
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDDOperationScope$",
                    "Method Name": "withScope",
                    "File Name": "RDDOperationScope.scala",
                    "Line Number": 151
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDDOperationScope$",
                    "Method Name": "withScope",
                    "File Name": "RDDOperationScope.scala",
                    "Line Number": 112
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDD",
                    "Method Name": "withScope",
                    "File Name": "RDD.scala",
                    "Line Number": 406
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.PairRDDFunctions",
                    "Method Name": "saveAsHadoopFile",
                    "File Name": "PairRDDFunctions.scala",
                    "Line Number": 963
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDD",
                    "Method Name": "$anonfun$saveAsTextFile$2",
                    "File Name": "RDD.scala",
                    "Line Number": 1599
                },
                {
                    "Declaring Class": "scala.runtime.java8.JFunction0$mcV$sp",
                    "Method Name": "apply",
                    "File Name": "JFunction0$mcV$sp.scala",
                    "Line Number": 18
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDDOperationScope$",
                    "Method Name": "withScope",
                    "File Name": "RDDOperationScope.scala",
                    "Line Number": 151
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDDOperationScope$",
                    "Method Name": "withScope",
                    "File Name": "RDDOperationScope.scala",
                    "Line Number": 112
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDD",
                    "Method Name": "withScope",
                    "File Name": "RDD.scala",
                    "Line Number": 406
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDD",
                    "Method Name": "saveAsTextFile",
                    "File Name": "RDD.scala",
                    "Line Number": 1599
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDD",
                    "Method Name": "$anonfun$saveAsTextFile$1",
                    "File Name": "RDD.scala",
                    "Line Number": 1585
                },
                {
                    "Declaring Class": "scala.runtime.java8.JFunction0$mcV$sp",
                    "Method Name": "apply",
                    "File Name": "JFunction0$mcV$sp.scala",
                    "Line Number": 18
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDDOperationScope$",
                    "Method Name": "withScope",
                    "File Name": "RDDOperationScope.scala",
                    "Line Number": 151
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDDOperationScope$",
                    "Method Name": "withScope",
                    "File Name": "RDDOperationScope.scala",
                    "Line Number": 112
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDD",
                    "Method Name": "withScope",
                    "File Name": "RDD.scala",
                    "Line Number": 406
                },
                {
                    "Declaring Class": "org.apache.spark.rdd.RDD",
                    "Method Name": "saveAsTextFile",
                    "File Name": "RDD.scala",
                    "Line Number": 1585
                },
                {
                    "Declaring Class": "org.apache.spark.streaming.dstream.DStream",
                    "Method Name": "$anonfun$saveAsTextFiles$2",
                    "File Name": "DStream.scala",
                    "Line Number": 927
                },
                {
                    "Declaring Class": "org.apache.spark.streaming.dstream.DStream",
                    "Method Name": "$anonfun$saveAsTextFiles$2$adapted",
                    "File Name": "DStream.scala",
                    "Line Number": 925
                },
                {
                    "Declaring Class": "org.apache.spark.streaming.dstream.ForEachDStream",
                    "Method Name": "$anonfun$generateJob$2",
                    "File Name": "ForEachDStream.scala",
                    "Line Number": 51
                },
                {
                    "Declaring Class": "scala.runtime.java8.JFunction0$mcV$sp",
                    "Method Name": "apply",
                    "File Name": "JFunction0$mcV$sp.scala",
                    "Line Number": 18
                },
                {
                    "Declaring Class": "org.apache.spark.streaming.dstream.DStream",
                    "Method Name": "createRDDWithLocalProperties",
                    "File Name": "DStream.scala",
                    "Line Number": 417
                },
                {
                    "Declaring Class": "org.apache.spark.streaming.dstream.ForEachDStream",
                    "Method Name": "$anonfun$generateJob$1",
                    "File Name": "ForEachDStream.scala",
                    "Line Number": 51
                },
                {
                    "Declaring Class": "scala.runtime.java8.JFunction0$mcV$sp",
                    "Method Name": "apply",
                    "File Name": "JFunction0$mcV$sp.scala",
                    "Line Number": 18
                },
                {
                    "Declaring Class": "scala.util.Try$",
                    "Method Name": "apply",
                    "File Name": "Try.scala",
                    "Line Number": 210
                },
                {
                    "Declaring Class": "org.apache.spark.streaming.scheduler.Job",
                    "Method Name": "run",
                    "File Name": "Job.scala",
                    "Line Number": 39
                },
                {
                    "Declaring Class": "org.apache.spark.streaming.scheduler.JobScheduler$JobHandler",
                    "Method Name": "$anonfun$run$1",
                    "File Name": "JobScheduler.scala",
                    "Line Number": 256
                },
                {
                    "Declaring Class": "scala.runtime.java8.JFunction0$mcV$sp",
                    "Method Name": "apply",
                    "File Name": "JFunction0$mcV$sp.scala",
                    "Line Number": 18
                },
                {
                    "Declaring Class": "scala.util.DynamicVariable",
                    "Method Name": "withValue",
                    "File Name": "DynamicVariable.scala",
                    "Line Number": 59
                },
                {
                    "Declaring Class": "org.apache.spark.streaming.scheduler.JobScheduler$JobHandler",
                    "Method Name": "run",
                    "File Name": "JobScheduler.scala",
                    "Line Number": 256
                },
                {
                    "Declaring Class": "java.util.concurrent.ThreadPoolExecutor",
                    "Method Name": "runWorker",
                    "File Name": "ThreadPoolExecutor.java",
                    "Line Number": 1128
                },
                {
                    "Declaring Class": "java.util.concurrent.ThreadPoolExecutor$Worker",
                    "Method Name": "run",
                    "File Name": "ThreadPoolExecutor.java",
                    "Line Number": 628
                },
                {
                    "Declaring Class": "java.lang.Thread",
                    "Method Name": "run",
                    "File Name": "Thread.java",
                    "Line Number": 834
                }
            ]
        }
    }
}
{
    "Event": "SparkListenerJobStart",
    "Job ID": 2,
    "Submission Time": 1681460011502,
    "Stage Infos": [
        {
            "Stage ID": 6,
            "Stage Attempt ID": 0,
            "Stage Name": "repartition at DStream.scala:578",
            "Number of Tasks": 3,
            "RDD Info": [
                {
                    "RDD ID": 18,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"8\",\"name\":\"repartition\",\"parent\":{\"id\":\"1_1681460006000\",\"name\":\"repartition\\n@ 16:13:26\"}}",
                    "Callsite": "repartition at DStream.scala:578",
                    "Parent IDs": [
                        17
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 3,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 17,
                    "Name": "KafkaRDD",
                    "Scope": "{\"id\":\"0_1681460006000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 16:13:26\"}",
                    "Callsite": "createDirectStream at main.scala:31",
                    "Parent IDs": [],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "DETERMINATE",
                    "Number of Partitions": 3,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [],
            "Details": "org.apache.spark.rdd.RDD.repartition(RDD.scala:475)\norg.apache.spark.streaming.dstream.DStream.$anonfun$repartition$2(DStream.scala:578)\norg.apache.spark.streaming.dstream.DStream.$anonfun$transform$2(DStream.scala:668)\norg.apache.spark.streaming.dstream.DStream.$anonfun$transform$4(DStream.scala:682)\norg.apache.spark.streaming.dstream.TransformedDStream.compute(TransformedDStream.scala:46)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.TransformedDStream.createRDDWithLocalProperties(TransformedDStream.scala:65)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\nscala.Option.orElse(Option.scala:477)\norg.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\norg.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\nscala.Option.orElse(Option.scala:477)",
            "Accumulables": [],
            "Resource Profile Id": 0
        },
        {
            "Stage ID": 7,
            "Stage Attempt ID": 0,
            "Stage Name": "map at main.scala:36",
            "Number of Tasks": 30,
            "RDD Info": [
                {
                    "RDD ID": 23,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"3_1681460006000\",\"name\":\"map @ 16:13:26\"}",
                    "Callsite": "map at main.scala:36",
                    "Parent IDs": [
                        22
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 30,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 21,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"8\",\"name\":\"repartition\",\"parent\":{\"id\":\"1_1681460006000\",\"name\":\"repartition\\n@ 16:13:26\"}}",
                    "Callsite": "repartition at DStream.scala:578",
                    "Parent IDs": [
                        20
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 30,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 22,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"2_1681460006000\",\"name\":\"flatMap @ 16:13:26\"}",
                    "Callsite": "flatMap at main.scala:33",
                    "Parent IDs": [
                        21
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 30,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 19,
                    "Name": "ShuffledRDD",
                    "Scope": "{\"id\":\"8\",\"name\":\"repartition\",\"parent\":{\"id\":\"1_1681460006000\",\"name\":\"repartition\\n@ 16:13:26\"}}",
                    "Callsite": "repartition at DStream.scala:578",
                    "Parent IDs": [
                        18
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 30,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 20,
                    "Name": "CoalescedRDD",
                    "Scope": "{\"id\":\"8\",\"name\":\"repartition\",\"parent\":{\"id\":\"1_1681460006000\",\"name\":\"repartition\\n@ 16:13:26\"}}",
                    "Callsite": "repartition at DStream.scala:578",
                    "Parent IDs": [
                        19
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 30,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [
                6
            ],
            "Details": "org.apache.spark.streaming.dstream.DStream.map(DStream.scala:548)\nname.spade5.Main$.main(main.scala:36)\nname.spade5.Main.main(main.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:566)\norg.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:63)\norg.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)",
            "Accumulables": [],
            "Resource Profile Id": 0
        },
        {
            "Stage ID": 8,
            "Stage Attempt ID": 0,
            "Stage Name": "saveAsTextFiles at main.scala:36",
            "Number of Tasks": 2,
            "RDD Info": [
                {
                    "RDD ID": 50,
                    "Name": "MapPartitionsRDD",
                    "Scope": "{\"id\":\"5_1681460006000\",\"name\":\"saveAsTextFiles\\n@ 16:13:26\"}",
                    "Callsite": "saveAsTextFiles at main.scala:36",
                    "Parent IDs": [
                        24
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 2,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                },
                {
                    "RDD ID": 24,
                    "Name": "ShuffledRDD",
                    "Scope": "{\"id\":\"4_1681460006000\",\"name\":\"reduceByKey\\n@ 16:13:26\"}",
                    "Callsite": "reduceByKey at main.scala:36",
                    "Parent IDs": [
                        23
                    ],
                    "Storage Level": {
                        "Use Disk": false,
                        "Use Memory": false,
                        "Deserialized": false,
                        "Replication": 1
                    },
                    "Barrier": false,
                    "DeterministicLevel": "UNORDERED",
                    "Number of Partitions": 2,
                    "Number of Cached Partitions": 0,
                    "Memory Size": 0,
                    "Disk Size": 0
                }
            ],
            "Parent IDs": [
                7
            ],
            "Details": "org.apache.spark.streaming.dstream.DStream.saveAsTextFiles(DStream.scala:924)\nname.spade5.Main$.main(main.scala:36)\nname.spade5.Main.main(main.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:566)\norg.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:63)\norg.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)",
            "Accumulables": [],
            "Resource Profile Id": 0
        }
    ],
    "Stage IDs": [
        6,
        7,
        8
    ],
    "Properties": {
        "callSite.long": "org.apache.spark.streaming.dstream.DStream.saveAsTextFiles(DStream.scala:924)\nname.spade5.Main$.main(main.scala:36)\nname.spade5.Main.main(main.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:566)\norg.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:63)\norg.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)",
        "spark.rdd.scope": "{\"id\":\"5_1681460006000\",\"name\":\"saveAsTextFiles\\n@ 16:13:26\"}",
        "spark.job.interruptOnCancel": "false",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1681460006000\">[output operation 0, batch time 16:13:26]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "callSite.short": "saveAsTextFiles at main.scala:36",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1681460006000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerStageSubmitted",
    "Stage Info": {
        "Stage ID": 6,
        "Stage Attempt ID": 0,
        "Stage Name": "repartition at DStream.scala:578",
        "Number of Tasks": 3,
        "RDD Info": [
            {
                "RDD ID": 18,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"8\",\"name\":\"repartition\",\"parent\":{\"id\":\"1_1681460006000\",\"name\":\"repartition\\n@ 16:13:26\"}}",
                "Callsite": "repartition at DStream.scala:578",
                "Parent IDs": [
                    17
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 3,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 17,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1681460006000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 16:13:26\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 3,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.repartition(RDD.scala:475)\norg.apache.spark.streaming.dstream.DStream.$anonfun$repartition$2(DStream.scala:578)\norg.apache.spark.streaming.dstream.DStream.$anonfun$transform$2(DStream.scala:668)\norg.apache.spark.streaming.dstream.DStream.$anonfun$transform$4(DStream.scala:682)\norg.apache.spark.streaming.dstream.TransformedDStream.compute(TransformedDStream.scala:46)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.TransformedDStream.createRDDWithLocalProperties(TransformedDStream.scala:65)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\nscala.Option.orElse(Option.scala:477)\norg.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\norg.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\nscala.Option.orElse(Option.scala:477)",
        "Submission Time": 1681460011506,
        "Accumulables": [],
        "Resource Profile Id": 0
    },
    "Properties": {
        "callSite.long": "org.apache.spark.streaming.dstream.DStream.saveAsTextFiles(DStream.scala:924)\nname.spade5.Main$.main(main.scala:36)\nname.spade5.Main.main(main.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:566)\norg.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:63)\norg.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)",
        "spark.rdd.scope": "{\"id\":\"5_1681460006000\",\"name\":\"saveAsTextFiles\\n@ 16:13:26\"}",
        "spark.job.interruptOnCancel": "false",
        "resource.executor.cores": "1",
        "spark.job.description": "Streaming job from <a href=\"/streaming/batch/?id=1681460006000\">[output operation 0, batch time 16:13:26]</a>",
        "spark.checkpoint.checkpointAllMarkedAncestors": "true",
        "callSite.short": "saveAsTextFiles at main.scala:36",
        "spark.streaming.internal.outputOpId": "0",
        "spark.streaming.internal.batchTime": "1681460006000",
        "spark.rdd.scope.noOverride": "true"
    }
}
{
    "Event": "SparkListenerTaskStart",
    "Stage ID": 6,
    "Stage Attempt ID": 0,
    "Task Info": {
        "Task ID": 3,
        "Index": 2,
        "Attempt": 0,
        "Partition ID": 2,
        "Launch Time": 1681460011515,
        "Executor ID": "0",
        "Host": "11.11.0.86",
        "Locality": "PROCESS_LOCAL",
        "Speculative": false,
        "Getting Result Time": 0,
        "Finish Time": 0,
        "Failed": false,
        "Killed": false,
        "Accumulables": []
    }
}
{
    "Event": "SparkListenerApplicationEnd",
    "Timestamp": 1681460011556
}
{
    "Event": "SparkListenerStageCompleted",
    "Stage Info": {
        "Stage ID": 6,
        "Stage Attempt ID": 0,
        "Stage Name": "repartition at DStream.scala:578",
        "Number of Tasks": 3,
        "RDD Info": [
            {
                "RDD ID": 18,
                "Name": "MapPartitionsRDD",
                "Scope": "{\"id\":\"8\",\"name\":\"repartition\",\"parent\":{\"id\":\"1_1681460006000\",\"name\":\"repartition\\n@ 16:13:26\"}}",
                "Callsite": "repartition at DStream.scala:578",
                "Parent IDs": [
                    17
                ],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 3,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            },
            {
                "RDD ID": 17,
                "Name": "KafkaRDD",
                "Scope": "{\"id\":\"0_1681460006000\",\"name\":\"kafka 0.10 direct stream [0]\\n@ 16:13:26\"}",
                "Callsite": "createDirectStream at main.scala:31",
                "Parent IDs": [],
                "Storage Level": {
                    "Use Disk": false,
                    "Use Memory": false,
                    "Deserialized": false,
                    "Replication": 1
                },
                "Barrier": false,
                "DeterministicLevel": "DETERMINATE",
                "Number of Partitions": 3,
                "Number of Cached Partitions": 0,
                "Memory Size": 0,
                "Disk Size": 0
            }
        ],
        "Parent IDs": [],
        "Details": "org.apache.spark.rdd.RDD.repartition(RDD.scala:475)\norg.apache.spark.streaming.dstream.DStream.$anonfun$repartition$2(DStream.scala:578)\norg.apache.spark.streaming.dstream.DStream.$anonfun$transform$2(DStream.scala:668)\norg.apache.spark.streaming.dstream.DStream.$anonfun$transform$4(DStream.scala:682)\norg.apache.spark.streaming.dstream.TransformedDStream.compute(TransformedDStream.scala:46)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.TransformedDStream.createRDDWithLocalProperties(TransformedDStream.scala:65)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\nscala.Option.orElse(Option.scala:477)\norg.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\norg.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\nscala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\norg.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\norg.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\nscala.Option.orElse(Option.scala:477)",
        "Submission Time": 1681460011506,
        "Completion Time": 1681460011573,
        "Failure Reason": "Stage cancelled because SparkContext was shut down",
        "Accumulables": [],
        "Resource Profile Id": 0
    }
}
{
    "Event": "SparkListenerJobEnd",
    "Job ID": 2,
    "Completion Time": 1681460011574,
    "Job Result": {
        "Result": "JobFailed",
        "Exception": {
            "Message": "Job 2 cancelled because SparkContext was shut down",
            "Stack Trace": [
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGScheduler",
                    "Method Name": "$anonfun$cleanUpAfterSchedulerStop$1",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 1188
                },
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGScheduler",
                    "Method Name": "$anonfun$cleanUpAfterSchedulerStop$1$adapted",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 1186
                },
                {
                    "Declaring Class": "scala.collection.mutable.HashSet$Node",
                    "Method Name": "foreach",
                    "File Name": "HashSet.scala",
                    "Line Number": 435
                },
                {
                    "Declaring Class": "scala.collection.mutable.HashSet",
                    "Method Name": "foreach",
                    "File Name": "HashSet.scala",
                    "Line Number": 361
                },
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGScheduler",
                    "Method Name": "cleanUpAfterSchedulerStop",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 1186
                },
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop",
                    "Method Name": "onStop",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 2887
                },
                {
                    "Declaring Class": "org.apache.spark.util.EventLoop",
                    "Method Name": "stop",
                    "File Name": "EventLoop.scala",
                    "Line Number": 84
                },
                {
                    "Declaring Class": "org.apache.spark.scheduler.DAGScheduler",
                    "Method Name": "stop",
                    "File Name": "DAGScheduler.scala",
                    "Line Number": 2784
                },
                {
                    "Declaring Class": "org.apache.spark.SparkContext",
                    "Method Name": "$anonfun$stop$11",
                    "File Name": "SparkContext.scala",
                    "Line Number": 2105
                },
                {
                    "Declaring Class": "org.apache.spark.util.Utils$",
                    "Method Name": "tryLogNonFatalError",
                    "File Name": "Utils.scala",
                    "Line Number": 1484
                },
                {
                    "Declaring Class": "org.apache.spark.SparkContext",
                    "Method Name": "stop",
                    "File Name": "SparkContext.scala",
                    "Line Number": 2105
                },
                {
                    "Declaring Class": "org.apache.spark.SparkContext",
                    "Method Name": "$anonfun$new$35",
                    "File Name": "SparkContext.scala",
                    "Line Number": 670
                },
                {
                    "Declaring Class": "org.apache.spark.util.SparkShutdownHook",
                    "Method Name": "run",
                    "File Name": "ShutdownHookManager.scala",
                    "Line Number": 214
                },
                {
                    "Declaring Class": "org.apache.spark.util.SparkShutdownHookManager",
                    "Method Name": "$anonfun$runAll$2",
                    "File Name": "ShutdownHookManager.scala",
                    "Line Number": 188
                },
                {
                    "Declaring Class": "scala.runtime.java8.JFunction0$mcV$sp",
                    "Method Name": "apply",
                    "File Name": "JFunction0$mcV$sp.scala",
                    "Line Number": 18
                },
                {
                    "Declaring Class": "org.apache.spark.util.Utils$",
                    "Method Name": "logUncaughtExceptions",
                    "File Name": "Utils.scala",
                    "Line Number": 2066
                },
                {
                    "Declaring Class": "org.apache.spark.util.SparkShutdownHookManager",
                    "Method Name": "$anonfun$runAll$1",
                    "File Name": "ShutdownHookManager.scala",
                    "Line Number": 188
                },
                {
                    "Declaring Class": "scala.runtime.java8.JFunction0$mcV$sp",
                    "Method Name": "apply",
                    "File Name": "JFunction0$mcV$sp.scala",
                    "Line Number": 18
                },
                {
                    "Declaring Class": "scala.util.Try$",
                    "Method Name": "apply",
                    "File Name": "Try.scala",
                    "Line Number": 210
                },
                {
                    "Declaring Class": "org.apache.spark.util.SparkShutdownHookManager",
                    "Method Name": "runAll",
                    "File Name": "ShutdownHookManager.scala",
                    "Line Number": 188
                },
                {
                    "Declaring Class": "org.apache.spark.util.SparkShutdownHookManager$$anon$2",
                    "Method Name": "run",
                    "File Name": "ShutdownHookManager.scala",
                    "Line Number": 178
                },
                {
                    "Declaring Class": "java.util.concurrent.Executors$RunnableAdapter",
                    "Method Name": "call",
                    "File Name": "Executors.java",
                    "Line Number": 515
                },
                {
                    "Declaring Class": "java.util.concurrent.FutureTask",
                    "Method Name": "run",
                    "File Name": "FutureTask.java",
                    "Line Number": 264
                },
                {
                    "Declaring Class": "java.util.concurrent.ThreadPoolExecutor",
                    "Method Name": "runWorker",
                    "File Name": "ThreadPoolExecutor.java",
                    "Line Number": 1128
                },
                {
                    "Declaring Class": "java.util.concurrent.ThreadPoolExecutor$Worker",
                    "Method Name": "run",
                    "File Name": "ThreadPoolExecutor.java",
                    "Line Number": 628
                },
                {
                    "Declaring Class": "java.lang.Thread",
                    "Method Name": "run",
                    "File Name": "Thread.java",
                    "Line Number": 834
                }
            ]
        }
    }
}